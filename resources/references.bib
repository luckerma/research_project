@inproceedings{Kristan21,
  abstract  = {The Visual Object Tracking challenge VOT2021 is the ninth annual tracker benchmarking activity organized by the VOT initiative. Results of 71 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in recent years. The VOT2021 challenge was composed of four sub-challenges focusing on different tracking domains: (i) VOT-ST2021 challenge focused on short-term tracking in RGB, (ii) VOT-RT2021 challenge focused on "real-time" short-term tracking in RGB, (iii) VOT-LT2021 focused on long-term tracking, namely coping with target disappearance and reappearance and (iv) VOT-RGBD2021 challenge focused on long-term tracking in RGB and depth imagery. The VOT-ST2021 dataset was refreshed, while VOT-RGBD2021 introduces a training dataset and sequestered dataset for winner identification. The source code for most of the trackers, the datasets, the evaluation kit and the results along with the source code for most trackers are publicly available at the challenge website1.},
  author    = {Kristan, Matej and Matas, Jiří and Leonardis, Aleš and Felsberg, Michael and Pflugfelder, Roman and Kämäräinen, Joni-Kristian and Chang, Hyung Jin and Danelljan, Martin and Zajc, Luka Čehovin and Lukežič, Alan and Drbohlav, Ondrej and Käpylä, Jani and Häger, Gustav and Yan, Song and Yang, Jinyu and Zhang, Zhongqun and Fernández, Gustavo and Abdelpakey, Mohamed and Bhat, Goutam and Cerkezi, Llukman and Cevikalp, Hakan and Chen, Shengyong and Chen, Xin and Cheng, Miao and Cheng, Ziyi and Chiu, Yu-Chen and Cirakman, Ozgun and Cui, Yutao and Dai, Kenan and Dasari, Mohana Murali and Deng, Qili and Dong, Xingping and Du, Daniel K. and Dunnhofer, Matteo and Feng, Zhen-Hua and Feng, Zhiyong and Fu, Zhihong and Ge, Shiming and Gorthi, Rama Krishna and Gu, Yuzhang and Gunsel, Bilge and Guo, Qing and Gurkan, Filiz and Han, Wencheng and Huang, Yanyan and Lawin, Felix Järemo and Jhang, Shang-Jhih and Ji, Rongrong and Jiang, Cheng and Jiang, Yingjie and Juefei-Xu, Felix and Jun, Yin and Ke, Xiao and Khan, Fahad Shahbaz and Hak Kim, Byeong and Kittler, Josef and Lan, Xiangyuan and Lee, Jun Ha and Leibe, Bastian and Li, Hui and Li, Jianhua and Li, Xianxian and Li, Yuezhou and Liu, Bo and Liu, Chang and Liu, Jingen and Liu, Li and Liu, Qingjie and Lu, Huchuan and Lu, Wei and Luiten, Jonathon and Ma, Jie and Ma, Ziang and Martinel, Niki and Mayer, Christoph and Memarmoghadam, Alireza and Micheloni, Christian and Niu, Yuzhen and Paudel, Danda and Peng, Houwen and Qiu, Shoumeng and Rajiv, Aravindh and Rana, Muhammad and Robinson, Andreas and Saribas, Hasan and Shao, Ling and Shehata, Mohamed and Shen, Furao and Shen, Jianbing and Simonato, Kristian and Song, Xiaoning and Tang, Zhangyong and Timofte, Radu and Torr, Philip and Tsai, Chi-Yi and Uzun, Bedirhan and Van Gool, Luc and Voigtlaender, Paul and Wang, Dong and Wang, Guangting and Wang, Liangliang and Wang, Lijun and Wang, Limin and Wang, Linyuan and Wang, Yong and Wang, Yunhong and Wu, Chenyan and Wu, Gangshan and Wu, Xiao-Jun and Xie, Fei and Xu, Tianyang and Xu, Xiang and Xue, Wanli and Yan, Bin and Yang, Wankou and Yang, Xiaoyun and Ye, Yu and Yin, Jun and Zhang, Chengwei and Zhang, Chunhui and Zhang, Haitao and Zhang, Kaihua and Zhang, Kangkai and Zhang, Xiaohan and Zhang, Xiaolin and Zhang, Xinyu and Zhang, Zhibin and Zhao, Shaochuan and Zhen, Ming and Zhong, Bineng and Zhu, Jiawen and Zhu, Xue-Feng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
  date      = {2021-10},
  doi       = {10.1109/ICCVW54120.2021.00305},
  issn      = {2473-9944},
  pages     = {2711--2738},
  title     = {The Ninth Visual Object Tracking VOT2021 Challenge Results},
}

@inproceedings{Kristan22,
  abstract  = {The Visual Object Tracking challenge VOT2022 is the tenth annual tracker benchmarking activity organized by the VOT initiative. Results of 93 entries are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in recent years. The VOT2022 challenge was composed of seven sub-challenges focusing on different tracking domains: (i) VOT-STs2022 challenge focused on short-term tracking in RGB by segmentation, (ii) VOT-STb2022 challenge focused on short-term tracking in RGB by bounding boxes, (iii) VOT-RTs2022 challenge focused on ``real-time'' short-term tracking in RGB by segmentation, (iv) VOT-RTb2022 challenge focused on ``real-time'' short-term tracking in RGB by bounding boxes, (v) VOT-LT2022 focused on long-term tracking, namely coping with target disappearance and reappearance, (vi) VOT-RGBD2022 challenge focused on short-term tracking in RGB and depth imagery, and (vii) VOT-D2022 challenge focused on short-term tracking in depth-only imagery. New datasets were introduced in VOT-LT2022 and VOT-RGBD2022, VOT-ST2022 dataset was refreshed, and a training dataset was introduced for VOT-LT2022. The source code for most of the trackers, the datasets, the evaluation kit and the results are publicly available at the challenge website (http://votchallenge.net).},
  author    = {Kristan, Matej and Leonardis, Aleš and Matas, Jiří and Felsberg, Michael and Pflugfelder, Roman and Kämäräinen, Joni-Kristian and Chang, Hyung Jin and Danelljan, Martin and Zajc, Luka Čehovin and Lukežič, Alan and Drbohlav, Ondrej and Björklund, Johanna and Zhang, Yushan and Zhang, Zhongqun and Yan, Song and Yang, Wenyan and Cai, Dingding and Mayer, Christoph and Fernández, Gustavo and Ben, Kang and Bhat, Goutam and Chang, Hong and Chen, Guangqi and Chen, Jiaye and Chen, Shengyong and Chen, Xilin and Chen, Xin and Chen, Xiuyi and Chen, Yiwei and Chen, Yu-Hsi and Chen, Zhixing and Cheng, Yangming and Ciaramella, Angelo and Cui, Yutao and Džubur, Benjamin and Dasari, Mohana Murali and Deng, Qili and Dhar, Debajyoti and Di, Shangzhe and Nardo, Emanuel Di and Du, Daniel K. and Dunnhofer, Matteo and Fan, Heng and Feng, Zhenhua and Fu, Zhihong and Gao, Shang and Gorthi, Rama Krishna and Granger, Eric and Gu, Q. H. and Gupta, Himanshu and He, Jianfeng and He, Keji and Huang, Yan and Jangid, Deepak and Ji, Rongrong and Jiang, Cheng and Jiang, Yingjie and Lawin, Felix Järemo and Kang, Ze and Kiran, Madhu and Kittler, Josef and Lai, Simiao and Lan, Xiangyuan and Lee, Dongwook and Lee, Hyunjeong and Lee, Seohyung and Li, Hui and Li, Ming and Li, Wangkai and Li, Xi and Li, Xianxian and Li, Xiao and Li, Zhe and Lin, Liting and Ling, Haibin and Liu, Bo and Liu, Chang and Liu, Si and Lu, Huchuan and Cruz, Rafael M. O. and Ma, Bingpeng and Ma, Chao and Ma, Jie and Ma, Yinchao and Martinel, Niki and Memarmoghadam, Alireza and Micheloni, Christian and Moallem, Payman and Nguyen-Meidine, Le Thanh and Pan, Siyang and Park, ChangBeom and Paudel, Danda and Paul, Matthieu and Peng, Houwen and Robinson, Andreas and Rout, Litu and Shan, Shiguang and Simonato, Kristian and Song, Tianhui and Song, Xiaoning and Sun, Chao and Sun, Jingna and Tang, Zhangyong and Timofte, Radu and Tsai, Chi-Yi and Gool, Luc Van and Verma, Om Prakash and Wang, Dong and Wang, Fei and Wang, Liang and Wang, Liangliang and Wang, Lijun and Wang, Limin and Wang, Qiang and Wu, Gangshan and Wu, Jinlin and Wu, Xiaojun and Xie, Fei and Xu, Tianyang and Xu, Wei and Xu, Yong and Xu, Yuanyou and Xue, Wanli and Xun, Zizheng and Yan, Bin and Yang, Dawei and Yang, Jinyu and Yang, Wankou and Yang, Xiaoyun and Yang, Yi and Yang, Yichun and Yang, Zongxin and Ye, Botao and Yu, Fisher and Yu, Hongyuan and Yu, Jiaqian and Yu, Qianjin and Yu, Weichen and Ze, Kang and Zhai, Jiang and Zhang, Chengwei and Zhang, Chunhu and Zhang, Kaihua and Zhang, Tianzhu and Zhang, Wenkang and Zhang, Zhibin and Zhang, Zhipeng and Zhao, Jie and Zhao, Shaochuan and Zheng, Feng and Zheng, Haixia and Zheng, Min and Zhong, Bineng and Zhu, Jiawen and Zhu, Xuefeng and Zhuang, Yueting},
  editor    = {Karlinsky, Leonid and Michaeli, Tomer and Nishino, Ko},
  location  = {Cham},
  publisher = {Springer Nature Switzerland},
  booktitle = {Computer Vision -- ECCV 2022 Workshops},
  date      = {2023},
  isbn      = {978-3-031-25085-9},
  pages     = {431--460},
  title     = {The Tenth Visual Object Tracking VOT2022 Challenge Results},
}

@article{Chen14,
  abstract     = {In this paper, we introduce a novel algorithm to solve the problem of object tracking across multiple non-overlapping cameras by learning inter-camera transfer models. The transfer models are divided into two parts according to different kinds of cues, i.e. spatio-temporal cues and appearance cues. To learn spatio-temporal transfer models across cameras, an unsupervised topology recovering approach based on N-neighbor accumulated cross-correlations is proposed, which estimates the topology of a non-overlapping multi-camera network. Different from previous methods, the proposed topology recovering method can deal with large amounts of data without considering the size of time window. To learn inter-camera appearance transfer models, a color transfer method is used to model the changes of color characteristics across cameras, which has an advantage of low requirements to training samples, making update efficient when illumination conditions change. The experiments are performed on different datasets. Experimental results demonstrate the effectiveness of the proposed algorithm.},
  author       = {Chen, Xiaotang and Huang, Kaiqi and Tan, Tieniu},
  url          = {https://www.sciencedirect.com/science/article/pii/S003132031300263X},
  date         = {2014},
  doi          = {https://doi.org/10.1016/j.patcog.2013.06.011},
  issn         = {0031-3203},
  journaltitle = {Pattern Recognition},
  keywords     = {Object tracking,Transfer models,Color transfer,Camera network,Non-overlapping views},
  note         = {Handwriting Recognition and other PR Applications},
  number       = {3},
  pages        = {1126--1137},
  title        = {Object tracking across non-overlapping views by learning inter-camera transfer models},
  volume       = {47},
}

@article{Zhang15,
  abstract     = {In this paper we propose a framework for tracking multiple interacting targets in a wide-area camera network consisting of both overlapping and non-overlapping cameras. Our method is motivated from observations that both individuals and groups of targets interact with each other in natural scenes. We associate each raw target trajectory (i.e., a tracklet) with a group state, which indicates if the trajectory belongs to an individual or a group. Structural Support Vector Machine (SSVM) is applied to the group states to decide if merge or split events occur in the scene. Information fusion between multiple overlapping cameras is handled using a homography-based voting scheme. The problem of tracking multiple interacting targets is then converted to a network flow problem, for which the solution can be obtained by the K-shortest paths algorithm. We demonstrate the effectiveness of the proposed algorithm on the challenging VideoWeb dataset in which a large amount of multi-person interaction activities are present. Comparative analysis with state-of-the-art methods is also shown.},
  author       = {Zhang, Shu and Zhu, Yingying and Roy-Chowdhury, Amit},
  url          = {https://www.sciencedirect.com/science/article/pii/S1077314215000168},
  date         = {2015},
  doi          = {https://doi.org/10.1016/j.cviu.2015.01.002},
  issn         = {1077-3142},
  journaltitle = {Computer Vision and Image Understanding},
  keywords     = {Multi-camera tracking,Multi-target tracking,Interacting targets,Wide-area camera network,Network flow},
  note         = {Image Understanding for Real-world Distributed Video Networks},
  pages        = {64--73},
  title        = {Tracking multiple interacting targets in a camera network},
  volume       = {134},
}

@article{Song08,
  abstract     = {We address the problem of tracking multiple people in a network of nonoverlapping cameras. This introduces certain challenges that are unique to this particular application scenario, in addition to existing challenges in tracking like pose and illumination variations, occlusion, clutter and sensor noise. For this purpose, we propose a novel multi-objective optimization framework by combining short term feature correspondences across the cameras with long-term feature dependency models. The overall solution strategy involves adapting the similarities between features observed at different cameras based on the long-term models and finding the stochastically optimal path for each person. For modeling the long-term interdependence of the features over space and time, we propose a novel method based on discriminant analysis models. The entire process allows us to adaptively evolve the feature correspondences by observing the system performance over a time window, and correct for errors in the similarity estimations. We show results on data collected by two large camera networks. These experiments prove that incorporation of the long-term models enable us to hold tracks of objects over extended periods of time, including situations where there are large ldquoblindrdquo areas. The proposed approach is implemented by distributing the processing over the entire network.},
  author       = {Song, Bi and Roy-Chowdhury, Amit K.},
  date         = {2008-08},
  doi          = {10.1109/JSTSP.2008.925992},
  issn         = {1941-0484},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  number       = {4},
  pages        = {582--596},
  title        = {Robust Tracking in A Camera Network: A Multi-Objective Optimization Framework},
  volume       = {2},
}

@book{Daniilidis10,
  author    = {Kostas Daniilidis, Maragos},
  publisher = {Springer Berlin, Heidelberg},
  date      = {2010-09},
  doi       = {10.1007/978-3-642-15549-9},
  title     = {Computer Vision -- ECCV 2010},
}

@article{Zou19,
  abstract     = {Online multi-object tracking (MOT) has broad applications in time-critical video analysis scenarios such as advanced driver-assistance systems (ADASs) and autonomous driving. In this paper, the proposed system aims at tracking multiple vehicles in the front view of an onboard monocular camera. The vehicle detection probes are customized to generate high precision detection, which plays a basic role in the following tracking-by-detection method. A novel Siamese network with a spatial pyramid pooling (SPP) layer is applied to calculate pairwise appearance similarity. The motion model captured from the refined bounding box provides the relative movements and aspects. The online-learned policy treats each tracking period as a Markov decision process (MDP) to maintain long-term, robust tracking. The proposed method is validated in a moving vehicle with an onboard NVIDIA Jetson TX2 and returns real-time speeds. Compared with other methods on KITTI and self-collected datasets, our method achieves significant performance in terms of the &ldquo;Mostly-tracked&rdquo;, &ldquo;Fragmentation&rdquo;, and &ldquo;ID switch&rdquo; variables.},
  author       = {Zou, Yi and Zhang, Weiwei and Weng, Wendi and Meng, Zhengyun},
  url          = {https://www.mdpi.com/1424-8220/19/6/1309},
  date         = {2019},
  doi          = {10.3390/s19061309},
  issn         = {1424-8220},
  journaltitle = {Sensors},
  number       = {6},
  title        = {Multi-Vehicle Tracking via Real-Time Detection Probes and a Markov Decision Process Policy},
  volume       = {19},
}

@article{Amosa23,
  abstract     = {The nascent applicability of multi-camera tracking (MCT) in numerous real-world applications makes it a significant computer vision problem. While visual tracking of objects, especially in video obtained from single camera setup, has drawn huge research attention, the constant identification and tracking of targets as they transit across multiple cameras remains an open research problem. In addition to the linking of target appearance and trajectory information across frames, effective association of such data across multiple cameras is also very critical in MCT. Occlusion, appearance variability, camera motion, as well as nonrigid object structure and motion are widely recognized constraints and major sources of concerns in MCT. In recent years, several literatures have been contributed suggesting a variety of approaches to addressing various problems in MCT. However, studies that critically review and report the advances and trends of research in MCT are still limited. This current study presents a comprehensive and up-to-date review of visual object tracking in multi-camera settings. In this paper, we analyze and categorize existing works based on six crucial facets: problem formulation, adopted problem solving approach, data association requirements, mutual exclusion constraints, benchmark datasets, and performance metrics. Furthermore, the study summarizes the outcomes of 30 state-of-the-art MCT algorithms on common datasets to allow quantitative comparison and analysis of their experimental results. Finally, we examine recent advances in MCT and suggest some promising future research directions.},
  author       = {Amosa, Temitope Ibrahim and Sebastian, Patrick and Izhar, Lila Iznita and Ibrahim, Oladimeji and Ayinla, Lukman Shehu and Bahashwan, Abdulrahman Abdullah and Bala, Abubakar and Samaila, Yau Alhaji},
  url          = {https://www.sciencedirect.com/science/article/pii/S0925231223006811},
  date         = {2023},
  doi          = {https://doi.org/10.1016/j.neucom.2023.126558},
  issn         = {0925-2312},
  journaltitle = {Neurocomputing},
  keywords     = {Multi-object tracking,Computer vision,Visual tracking,Intelligent video surveillance,Video surveillance,Multi-camera system},
  pages        = {126558},
  title        = {Multi-camera multi-object tracking: A review of current trends and future advances},
  volume       = {552},
}

@inproceedings{Choi12,
  abstract  = {We present a coherent, discriminative framework for simultaneously tracking multiple people and estimating their collective activities. Instead of treating the two problems separately, our model is grounded in the intuition that a strong correlation exists between a person's motion, their activity, and the motion and activities of other nearby people. Instead of directly linking the solutions to these two problems, we introduce a hierarchy of activity types that creates a natural progression that leads from a specific person's motion to the activity of the group as a whole. Our model is capable of jointly tracking multiple people, recognizing individual activities (atomic activities), the interactions between pairs of people (interaction activities), and finally the behavior of groups of people (collective activities). We also propose an algorithm for solving this otherwise intractable joint inference problem by combining belief propagation with a version of the branch and bound algorithm equipped with integer programming. Experimental results on challenging video datasets demonstrate our theoretical claims and indicate that our model achieves the best collective activity classification results to date.},
  author    = {Choi, Wongun and Savarese, Silvio},
  editor    = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  location  = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  booktitle = {Computer Vision -- ECCV 2012},
  date      = {2012},
  isbn      = {978-3-642-33765-9},
  pages     = {215--230},
  title     = {A Unified Framework for Multi-target Tracking and Collective Activity Recognition},
}

@article{Cho19,
  abstract     = {In this study, we propose a unified framework which jointly solves both person re-identification and camera network topology inference problems with minimal prior knowledge about the environments. The proposed framework takes general multi-camera network environments into account and can be applied to online person re-identification in large-scale multi-camera networks. In addition, to show the superiority of the proposed framework, we provide a new person re-identification dataset with full annotations, named SLP, captured in the multi-camera network. Experimental results using our re-identification and public datasets show that the proposed methods are promising for both person re-identification and camera topology inference tasks.},
  author       = {Cho, Yeong-Jun and Kim, Su-A and Park, Jae-Han and Lee, Kyuewang and Yoon, Kuk-Jin},
  url          = {https://www.sciencedirect.com/science/article/pii/S1077314219300037},
  date         = {2019},
  doi          = {https://doi.org/10.1016/j.cviu.2019.01.003},
  issn         = {1077-3142},
  journaltitle = {Computer Vision and Image Understanding},
  pages        = {34--46},
  title        = {Joint person re-identification and camera network topology inference in multiple cameras},
  volume       = {180},
}

@article{Liu17,
  author       = {Liu, Wenqian and Camps, Octavia and Sznaier, Mario},
  date         = {2017},
  doi          = {10.48550/arXiv.1709.07065},
  journaltitle = {arXiv preprint arXiv:1709.07065},
  title        = {Multi-camera multi-object tracking},
}

@inproceedings{Orazio09,
  abstract  = {People Tracking in multiple cameras is of great interest for wide area video surveillance systems. Multi-camera tracking with non-overlapping fields of view (FOV) involves the tracking of people in the blind region and their correspondence matching across cameras. We consider these problems in this paper. We propose a multi camera architecture for wide area surveillance and a real time people tracking algorithm across non overlapping cameras. We compared different methods to evaluate the color Brightness Transfer Function (BTF) between non overlapping cameras. These approaches are based on a testing phase during which the color histogram mapping, between pairs of images of the same object observed in the different field of views, is carried out. The experimental results compare two different transfer functions and demonstrate their limits in people association when a new person enters in one camera FOV.},
  author    = {D'Orazio, T. and Mazzeo, P.L. and Spagnolo, P.},
  booktitle = {2009 Third ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC)},
  date      = {2009-08},
  doi       = {10.1109/ICDSC.2009.5289365},
  pages     = {1--6},
  title     = {Color Brightness Transfer Function evaluation for non overlapping multi camera tracking},
}

@article{He20,
  abstract     = {This paper focuses on the Multi-Target Multi-Camera Tracking task (MTMCT), which aims at tracking multiple targets within a multi-camera network. As the trajectory of each target is inherently split into multiple sub-trajectories (namely local tracklets) in a multi-camera network, a major challenge of MTMCT is how to accurately match the local tracklets generated within each camera across different cameras and generate a complete global trajectory for each target, i.e., the cross-camera tracklet matching problem. We solve the cross-camera tracklet matching problem by TRACklet-to-Target Assignment (TRACTA), and propose the Restricted Non-negative Matrix Factorization (RNMF) algorithm to compute the optimal assignment solution that meets a set of constraints, which should be in force in practice. TRACTA can correct the tracking errors caused by occlusions and missed detections in local tracklets, and produce a complete global trajectory for each target across all the cameras. Moreover, we also develop an analytical way of estimating the total number of targets in the camera network, which plays an important role to compute the tracklet-to-target assignment. Experimental evaluations and ablation studies on four MTMCT benchmark datasets show the superiority of the proposed TRACTA method.},
  author       = {He, Yuhang and Wei, Xing and Hong, Xiaopeng and Shi, Weiwei and Gong, Yihong},
  date         = {2020},
  doi          = {10.1109/TIP.2020.2980070},
  issn         = {1941-0042},
  journaltitle = {IEEE Transactions on Image Processing},
  pages        = {5191--5205},
  title        = {Multi-Target Multi-Camera Tracking by Tracklet-to-Target Assignment},
  volume       = {29},
}

@inproceedings{Quach21,
  abstract  = {Multi-Camera Multiple Object Tracking (MC-MOT) is a significant computer vision problem due to its emerging applicability in several real-world applications. Despite a large number of existing works, solving the data association problem in any MC-MOT pipeline is arguably one of the most challenging tasks. Developing a robust MC-MOT system, however, is still highly challenging due to many practical issues such as inconsistent lighting conditions, varying object movement patterns, or the trajectory occlusions of the objects between the cameras. To address these problems, this work, therefore, proposes a new Dynamic Graph Model with Link Prediction (DyGLIP) approach 1 to solve the data association task. Compared to existing methods, our new model offers several advantages, including better feature representations and the ability to recover from lost tracks during camera transitions. Moreover, our model works gracefully regardless of the overlapping ratios between the cameras. Experimental results show that we out-perform existing MC-MOT algorithms by a large margin on several practical datasets. Notably, our model works favor-ably on online settings but can be extended to an incremental approach for large-scale datasets.},
  author    = {Quach, Kha Gia and Nguyen, Pha and Le, Huu and Truong, Thanh-Dat and Duong, Chi Nhan and Tran, Minh-Triet and Luu, Khoa},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2021-06},
  doi       = {10.1109/CVPR46437.2021.01357},
  issn      = {2575-7075},
  pages     = {13779--13788},
  title     = {DyGLIP: A Dynamic Graph Model with Link Prediction for Accurate Multi-Camera Multiple Object Tracking},
}

@inproceedings{Jiang18,
  abstract  = {Online inter-camera trajectory association is a promising topic in intelligent video surveillance, which concentrates on associating trajectories belong to the same individual across different cameras according to time. It remains challenging due to the inconsistent appearance of a person in different cameras and the lack of spatio-temporal constraints between cameras. Besides, the orientation variations and the partial occlusions significantly increase the difficulty of inter-camera trajectory association. Targeting to solve these problems, this work proposes an orientation-driven person re-identification (ODPR) and an effective camera topology estimation based on appearance features for online inter-camera trajectory association. ODPR explicitly leverages the orientation cues and stable torso features to learn discriminative feature representations for identifying trajectories across cameras, which alleviates the pedestrian orientation variations by the designed orientation-driven loss function and orientation aware weights. The effective camera topology estimation introduces appearance features to generate the correct spatio-temporal constraints for narrowing the retrieval range, which improves the time efficiency and provides the possibility for intelligent inter-camera trajectory association in large-scale surveillance environments. Extensive experimental results demonstrate that our proposed approach significantly outperforms most state-of-the-art methods on the popular person re-identification datasets and the public multi-target, multi-camera tracking benchmark.},
  author    = {Jiang, Na and Bai, SiChen and Xu, Yue and Xing, Chang and Zhou, Zhong and Wu, Wei},
  location  = {Seoul, Republic of Korea},
  publisher = {Association for Computing Machinery},
  url       = {https://doi.org/10.1145/3240508.3240663},
  booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
  date      = {2018},
  doi       = {10.1145/3240508.3240663},
  isbn      = {9781450356657},
  keywords  = {person re-identification,camera topology estimation,inter-camera trajectory association},
  pages     = {1457--1465},
  series    = {MM '18},
  title     = {Online Inter-Camera Trajectory Association Exploiting Person Re-Identification and Camera Topology},
}

@inproceedings{Specker21,
  abstract  = {Multi-camera tracking of vehicles on a city-scale level is a crucial task for efficient traffic monitoring. Most of the errors made by such multi-target multi-camera tracking systems arise due to tracking failures or misleading visual information of detection boxes under occlusion. Therefore, we propose an occlusion-aware approach that leverages temporal information from tracks to improve the single-camera tracking performance by an occlusion handling strategy and additional modules to filter false detections. For the multi-camera tracking, we discard obstacle-occluded detection boxes by a background filtering technique and boxes overlapping with other targets using the available track information to improve the quality of extracted visual features. Furthermore, topological and temporal constraints are incorporated to simplify the re-identification task in the multi-camera clustering. We give detailed insights into our method with ablative experiments and show its competitiveness on the CityFlowV2 dataset, where we achieve promising results ranking 4th in Track 3 of the 2021 AI City Challenge.},
  author    = {Specker, Andreas and Stadler, Daniel and Florin, Lucas and Beyerer, Jürgen},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2021-06},
  doi       = {10.1109/CVPRW53098.2021.00471},
  issn      = {2160-7516},
  pages     = {4168--4177},
  title     = {An Occlusion-aware Multi-target Multi-camera Tracking System},
}

@inproceedings{Bredereck12,
  abstract  = {Multi-object tracking is still a challenging task in computer vision. We propose a robust approach to realize multi-object tracking using multi-camera networks. Detection algorithms are utilized to detect object regions with confidence scores for initialization of individual particle filters. Since data association is the key issue in Tracking-by-Detection mechanism, we present an efficient greedy matching algorithm considering multiple judgments based on likelihood functions. Furthermore, tracking in single cameras is realized by a greedy matching method. Afterwards, 3D geometry positions are obtained from the triangulation relationship between cameras. Corresponding objects are tracked in multiple cameras to take the advantages of multi-camera based tracking. Our algorithm performs online and does not need any information about the scene, no restrictions of enter-and-exit zones, no assumption of areas where objects are moving on and can be extended to any class of object tracking. Experimental results show the benefits of using multiple cameras by the higher accuracy and precision rates.},
  author    = {Bredereck, Michael and Jiang, Xiaoyan and Körner, Marco and Denzler, Joachim},
  booktitle = {2012 Sixth International Conference on Distributed Smart Cameras (ICDSC)},
  date      = {2012-10},
  pages     = {1--6},
  title     = {Data association for multi-object Tracking-by-Detection in multi-camera networks},
}

@inproceedings{Specker22,
  abstract  = {Multi-camera tracking of vehicles on a city-wide level is a core component of modern traffic monitoring systems. For this task, single-camera tracking failures are the most common causes of errors concerning automatic multi-target multi-camera tracking systems. To address these problems, we propose several modules that aim at improving single-camera tracklets, e.g., appearance-based tracklet splitting, single-camera clustering, and track completion. After these track refinement steps, hierarchical clustering is used to associate the enhanced single-camera tracklets. During this stage, we leverage vehicle re-identification features as well as prior knowledge about the scene's topology. Last, the proposed track completion strategy is adopted for the cross-camera association task to obtain the final multi-camera tracks. Our method proves itself competitive: With it, we achieved 4th place in track 1 of the 2022 AI City Challenge.},
  author    = {Specker, Andreas and Florin, Lucas and Cormier, Mickael and Beyerer, Jürgen},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2022-06},
  doi       = {10.1109/CVPRW56347.2022.00361},
  issn      = {2160-7516},
  pages     = {3198--3208},
  title     = {Improving Multi-Target Multi-Camera Tracking by Track Refinement and Completion},
}

@article{Iguernaissi18,
  author       = {Iguernaissi, Rabah and Merad, Djamal and Aziz, Kheireddine and Drap, Pierre},
  publisher    = {Springer US},
  url          = {https://link.springer.com/article/10.1007/s11042-018-6638-5},
  date         = {2018-09},
  journaltitle = {SpringerLink},
  title        = {People tracking in multi-camera systems: A Review - multimedia tools and applications},
}

@article{Yoon18,
  abstract     = {In this study, a multiple hypothesis tracking (MHT) algorithm for multi-target multi-camera tracking (MCT) with disjoint views is proposed. The authors' method forms track-hypothesis trees, and each branch of them represents a multi-camera track of a target that may move within a camera as well as move across cameras. Furthermore, multi-target tracking within a camera is performed simultaneously with the tree formation by manipulating a status of each track hypothesis. Each status represents three different stages of a multi-camera track: tracking, searching, and end-of-track. The tracking status means targets are tracked by a single camera tracker. In the searching status, the disappeared targets are examined if they reappear in other cameras. The end-of-track status does the target exited the camera network due to its lengthy invisibility. These three status assists MHT to form the track-hypothesis trees for multi-camera tracking. Furthermore, a gating technique which eliminates the unlikely observation-to-track association using space-time information has been introduced. In the experiments, the proposed method has been tested using two datasets, DukeMTMC and NLPR\\_MCT, which demonstrates that the method outperforms the state-of-the-art method in terms of improvement of the accuracy. In addition, real-time and online performance of proposed method is also showed in this study.},
  author       = {Yoon, Kwangjin and Song, Young-min and Jeon, Moongu},
  url          = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-ipr.2017.1244},
  date         = {2018},
  doi          = {https://doi.org/10.1049/iet-ipr.2017.1244},
  eprint       = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-ipr.2017.1244},
  journaltitle = {IET Image Processing},
  keywords     = {trees (mathematics),target tracking,object tracking,MHT algorithm,multiple hypothesis tracking algorithm,multi target multi-camera tracking,track-hypothesis trees,DukeMTMC,NLPR_MCT},
  number       = {7},
  pages        = {1175--1184},
  title        = {Multiple hypothesis tracking algorithm for multi-target multi-camera tracking with disjoint views},
  volume       = {12},
}

@inproceedings{Dehghan15,
  abstract  = {In this paper we show that multiple object tracking (MOT) can be formulated in a framework, where the detection and data-association are performed simultaneously. Our method allows us to overcome the confinements of data association based MOT approaches; where the performance is dependent on the object detection results provided at input level. At the core of our method lies structured learning which learns a model for each target and infers the best location of all targets simultaneously in a video clip. The inference of our structured learning is done through a new Target Identity-aware Network Flow (TINF), where each node in the network encodes the probability of each target identity belonging to that node. The proposed Lagrangian relaxation optimization finds the high quality solution to the network. During optimization a soft spatial constraint is enforced between the nodes of the graph which helps reducing the ambiguity caused by nearby targets with similar appearance in crowded scenarios. We show that automatically detecting and tracking targets in a single framework can help resolve the ambiguities due to frequent occlusion and heavy articulation of targets. Our experiments involve challenging yet distinct datasets and show that our method can achieve results better than the state-of-art.},
  author    = {Dehghan, Afshin and Tian, Yicong and Torr, Philip. H. S. and Shah, Mubarak},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2015-06},
  doi       = {10.1109/CVPR.2015.7298718},
  issn      = {1063-6919},
  pages     = {1146--1154},
  title     = {Target Identity-aware Network Flow for online multiple target tracking},
}

@misc{Zhu19,
  author      = {Zhu, Ji and Yang, Hua and Liu, Nian and Kim, Minyoung and Zhang, Wenjun and Yang, Ming-Hsuan},
  date        = {2019},
  eprint      = {1902.00749},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  title       = {Online Multi-Object Tracking with Dual Matching Attention Networks},
}