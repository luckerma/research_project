@INPROCEEDINGS{9607833,
  title={The Ninth Visual Object Tracking VOT2021 Challenge Results}, 
  author={Kristan, Matej and Matas, Jiří and Leonardis, Aleš and Felsberg, Michael and Pflugfelder, Roman and Kämäräinen, Joni-Kristian and Chang, Hyung Jin and Danelljan, Martin and Zajc, Luka Čehovin and Lukežič, Alan and Drbohlav, Ondrej and Käpylä, Jani and Häger, Gustav and Yan, Song and Yang, Jinyu and Zhang, Zhongqun and Fernández, Gustavo and Abdelpakey, Mohamed and Bhat, Goutam and Cerkezi, Llukman and Cevikalp, Hakan and Chen, Shengyong and Chen, Xin and Cheng, Miao and Cheng, Ziyi and Chiu, Yu-Chen and Cirakman, Ozgun and Cui, Yutao and Dai, Kenan and Dasari, Mohana Murali and Deng, Qili and Dong, Xingping and Du, Daniel K. and Dunnhofer, Matteo and Feng, Zhen-Hua and Feng, Zhiyong and Fu, Zhihong and Ge, Shiming and Gorthi, Rama Krishna and Gu, Yuzhang and Gunsel, Bilge and Guo, Qing and Gurkan, Filiz and Han, Wencheng and Huang, Yanyan and Lawin, Felix Järemo and Jhang, Shang-Jhih and Ji, Rongrong and Jiang, Cheng and Jiang, Yingjie and Juefei-Xu, Felix and Jun, Yin and Ke, Xiao and Khan, Fahad Shahbaz and Hak Kim, Byeong and Kittler, Josef and Lan, Xiangyuan and Lee, Jun Ha and Leibe, Bastian and Li, Hui and Li, Jianhua and Li, Xianxian and Li, Yuezhou and Liu, Bo and Liu, Chang and Liu, Jingen and Liu, Li and Liu, Qingjie and Lu, Huchuan and Lu, Wei and Luiten, Jonathon and Ma, Jie and Ma, Ziang and Martinel, Niki and Mayer, Christoph and Memarmoghadam, Alireza and Micheloni, Christian and Niu, Yuzhen and Paudel, Danda and Peng, Houwen and Qiu, Shoumeng and Rajiv, Aravindh and Rana, Muhammad and Robinson, Andreas and Saribas, Hasan and Shao, Ling and Shehata, Mohamed and Shen, Furao and Shen, Jianbing and Simonato, Kristian and Song, Xiaoning and Tang, Zhangyong and Timofte, Radu and Torr, Philip and Tsai, Chi-Yi and Uzun, Bedirhan and Van Gool, Luc and Voigtlaender, Paul and Wang, Dong and Wang, Guangting and Wang, Liangliang and Wang, Lijun and Wang, Limin and Wang, Linyuan and Wang, Yong and Wang, Yunhong and Wu, Chenyan and Wu, Gangshan and Wu, Xiao-Jun and Xie, Fei and Xu, Tianyang and Xu, Xiang and Xue, Wanli and Yan, Bin and Yang, Wankou and Yang, Xiaoyun and Ye, Yu and Yin, Jun and Zhang, Chengwei and Zhang, Chunhui and Zhang, Haitao and Zhang, Kaihua and Zhang, Kangkai and Zhang, Xiaohan and Zhang, Xiaolin and Zhang, Xinyu and Zhang, Zhibin and Zhao, Shaochuan and Zhen, Ming and Zhong, Bineng and Zhu, Jiawen and Zhu, Xue-Feng},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
  year={2021},
  volume={},
  number={},
  pages={2711-2738},
  doi={10.1109/ICCVW54120.2021.00305}
},

@article{CHEN20141126,
title = {Object tracking across non-overlapping views by learning inter-camera transfer models},
journal = {Pattern Recognition},
volume = {47},
number = {3},
pages = {1126-1137},
year = {2014},
note = {Handwriting Recognition and other PR Applications},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2013.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S003132031300263X},
author = {Xiaotang Chen and Kaiqi Huang and Tieniu Tan},
keywords = {Object tracking, Transfer models, Color transfer, Camera network, Non-overlapping views},
abstract = {In this paper, we introduce a novel algorithm to solve the problem of object tracking across multiple non-overlapping cameras by learning inter-camera transfer models. The transfer models are divided into two parts according to different kinds of cues, i.e. spatio-temporal cues and appearance cues. To learn spatio-temporal transfer models across cameras, an unsupervised topology recovering approach based on N-neighbor accumulated cross-correlations is proposed, which estimates the topology of a non-overlapping multi-camera network. Different from previous methods, the proposed topology recovering method can deal with large amounts of data without considering the size of time window. To learn inter-camera appearance transfer models, a color transfer method is used to model the changes of color characteristics across cameras, which has an advantage of low requirements to training samples, making update efficient when illumination conditions change. The experiments are performed on different datasets. Experimental results demonstrate the effectiveness of the proposed algorithm.}
},

@article{ZHANG201564,
title = {Tracking multiple interacting targets in a camera network},
journal = {Computer Vision and Image Understanding},
volume = {134},
pages = {64-73},
year = {2015},
note = {Image Understanding for Real-world Distributed Video Networks},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2015.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1077314215000168},
author = {Shu Zhang and Yingying Zhu and Amit Roy-Chowdhury},
keywords = {Multi-camera tracking, Multi-target tracking, Interacting targets, Wide-area camera network, Network flow},
abstract = {In this paper we propose a framework for tracking multiple interacting targets in a wide-area camera network consisting of both overlapping and non-overlapping cameras. Our method is motivated from observations that both individuals and groups of targets interact with each other in natural scenes. We associate each raw target trajectory (i.e., a tracklet) with a group state, which indicates if the trajectory belongs to an individual or a group. Structural Support Vector Machine (SSVM) is applied to the group states to decide if merge or split events occur in the scene. Information fusion between multiple overlapping cameras is handled using a homography-based voting scheme. The problem of tracking multiple interacting targets is then converted to a network flow problem, for which the solution can be obtained by the K-shortest paths algorithm. We demonstrate the effectiveness of the proposed algorithm on the challenging VideoWeb dataset in which a large amount of multi-person interaction activities are present. Comparative analysis with state-of-the-art methods is also shown.}
},

@article{ZHANG201564,
title = {Tracking multiple interacting targets in a camera network},
journal = {Computer Vision and Image Understanding},
volume = {134},
pages = {64-73},
year = {2015},
note = {Image Understanding for Real-world Distributed Video Networks},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2015.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1077314215000168},
author = {Shu Zhang and Yingying Zhu and Amit Roy-Chowdhury},
keywords = {Multi-camera tracking, Multi-target tracking, Interacting targets, Wide-area camera network, Network flow},
abstract = {In this paper we propose a framework for tracking multiple interacting targets in a wide-area camera network consisting of both overlapping and non-overlapping cameras. Our method is motivated from observations that both individuals and groups of targets interact with each other in natural scenes. We associate each raw target trajectory (i.e., a tracklet) with a group state, which indicates if the trajectory belongs to an individual or a group. Structural Support Vector Machine (SSVM) is applied to the group states to decide if merge or split events occur in the scene. Information fusion between multiple overlapping cameras is handled using a homography-based voting scheme. The problem of tracking multiple interacting targets is then converted to a network flow problem, for which the solution can be obtained by the K-shortest paths algorithm. We demonstrate the effectiveness of the proposed algorithm on the challenging VideoWeb dataset in which a large amount of multi-person interaction activities are present. Comparative analysis with state-of-the-art methods is also shown.}
},

