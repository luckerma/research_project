@article{Amosa23,
  abstract     = {The nascent applicability of multi-camera tracking (MCT) in numerous real-world applications makes it a significant computer vision problem. While visual tracking of objects, especially in video obtained from single camera setup, has drawn huge research attention, the constant identification and tracking of targets as they transit across multiple cameras remains an open research problem. In addition to the linking of target appearance and trajectory information across frames, effective association of such data across multiple cameras is also very critical in MCT. Occlusion, appearance variability, camera motion, as well as nonrigid object structure and motion are widely recognized constraints and major sources of concerns in MCT. In recent years, several literatures have been contributed suggesting a variety of approaches to addressing various problems in MCT. However, studies that critically review and report the advances and trends of research in MCT are still limited. This current study presents a comprehensive and up-to-date review of visual object tracking in multi-camera settings. In this paper, we analyze and categorize existing works based on six crucial facets: problem formulation, adopted problem solving approach, data association requirements, mutual exclusion constraints, benchmark datasets, and performance metrics. Furthermore, the study summarizes the outcomes of 30 state-of-the-art MCT algorithms on common datasets to allow quantitative comparison and analysis of their experimental results. Finally, we examine recent advances in MCT and suggest some promising future research directions.},
  author       = {Amosa, Temitope Ibrahim and Sebastian, Patrick and Izhar, Lila Iznita and Ibrahim, Oladimeji and Ayinla, Lukman Shehu and Bahashwan, Abdulrahman Abdullah and Bala, Abubakar and Samaila, Yau Alhaji},
  date         = {2023},
  doi          = {https://doi.org/10.1016/j.neucom.2023.126558},
  issn         = {0925-2312},
  journaltitle = {Neurocomputing},
  keywords     = {Multi-object tracking,Computer vision,Visual tracking,Intelligent video surveillance,Video surveillance,Multi-camera system},
  pages        = {126558},
  title        = {Multi-camera multi-object tracking: A review of current trends and future advances},
  url          = {https://www.sciencedirect.com/science/article/pii/S0925231223006811},
  volume       = {552},
}

@inproceedings{Bergmann19,
  abstract  = {The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.},
  author    = {Bergmann, Philipp and Meinhardt, Tim and Leal-Taixé, Laura},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  date      = {2019-10},
  doi       = {10.1109/ICCV.2019.00103},
  issn      = {2380-7504},
  keywords  = {},
  number    = {},
  pages     = {941-951},
  title     = {Tracking Without Bells and Whistles},
  volume    = {},
}

@article{Bernardin08,
  author    = {Bernardin, Keni and Stiefelhagen, Rainer},
  doi       = {10.1155/2008/246309},
  issn      = {1687-5176, 1687-5281},
  journal   = {EURASIP journal on image and video processing},
  language  = {english},
  pages     = {Art.Nr.: 246309},
  publisher = {{Hindawi}},
  title     = {Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics},
  volume    = {2},
  year      = {2008},
}

@inproceedings{Bewley16,
  abstract  = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9{\%}. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
  author    = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
  date      = {2016-09},
  doi       = {10.1109/ICIP.2016.7533003},
  issn      = {2381-8549},
  keywords  = {},
  number    = {},
  pages     = {3464-3468},
  title     = {Simple online and realtime tracking},
  volume    = {},
}

@article{Blackman04,
  abstract = {Multiple hypothesis tracking (MHT) is generally accepted as the preferred method for solving the data association problem in modern multiple target tracking (MTT) systems. This paper summarizes the motivations for MHT, the basic principles behind MHT and the alternative implementations in common use. It discusses the manner in which the multiple data association hypotheses formed by MHT can be combined with multiple filter models, such as used by the interacting multiple model (IMM) method. An overview of the studies that show the advantages of MHT over the conventional single hypothesis approach is given. Important current applications and areas of future research and development for MHT are discussed.},
  author   = {Blackman, S.S.},
  date     = {2004-01},
  doi      = {10.1109/MAES.2004.1263228},
  issn     = {1557-959X},
  journal  = {IEEE Aerospace and Electronic Systems Magazine},
  keywords = {},
  number   = {1},
  pages    = {5-18},
  title    = {Multiple hypothesis tracking for multiple target tracking},
  volume   = {19},
}

@article{Bochkovskiy20,
  author     = {Alexey Bochkovskiy and Chien{-}Yao Wang and Hong{-}Yuan Mark Liao},
  date       = {2020},
  eprint     = {2004.10934},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Tue, 28 Apr 2020 16:10:02 +0200},
  title      = {YOLOv4: Optimal Speed and Accuracy of Object Detection},
  url        = {https://arxiv.org/abs/2004.10934},
  volume     = {abs/2004.10934},
}

@inproceedings{Bredereck12,
  abstract  = {Multi-object tracking is still a challenging task in computer vision. We propose a robust approach to realize multi-object tracking using multi-camera networks. Detection algorithms are utilized to detect object regions with confidence scores for initialization of individual particle filters. Since data association is the key issue in Tracking-by-Detection mechanism, we present an efficient greedy matching algorithm considering multiple judgments based on likelihood functions. Furthermore, tracking in single cameras is realized by a greedy matching method. Afterwards, 3D geometry positions are obtained from the triangulation relationship between cameras. Corresponding objects are tracked in multiple cameras to take the advantages of multi-camera based tracking. Our algorithm performs online and does not need any information about the scene, no restrictions of enter-and-exit zones, no assumption of areas where objects are moving on and can be extended to any class of object tracking. Experimental results show the benefits of using multiple cameras by the higher accuracy and precision rates.},
  author    = {Bredereck, Michael and Jiang, Xiaoyan and Körner, Marco and Denzler, Joachim},
  booktitle = {2012 Sixth International Conference on Distributed Smart Cameras (ICDSC)},
  date      = {2012-10},
  pages     = {1--6},
  title     = {Data association for multi-object Tracking-by-Detection in multi-camera networks},
}

@article{Cai99,
  abstract     = {This paper presents a comprehensive framework for tracking coarse human models from sequences of synchronized monocular grayscale images in multiple camera coordinates. It demonstrates the feasibility of an end-to-end person tracking system using a unique combination of motion analysis on 3D geometry in different camera coordinates and other existing techniques in motion detection, segmentation, and pattern recognition. The system starts with tracking from a single camera view. When the system predicts that the active camera will no longer have a good view of the subject of interest, tracking will be switched to another camera which provides a better view and requires the least switching to continue tracking. The nonrigidity of the human body is addressed by matching points of the middle line of the human image, spatially and temporally, using Bayesian classification schemes. Multivariate normal distributions are employed to model class-conditional densities of the features for tracking, such as location, intensity, and geometric features. Limited degrees of occlusion are tolerated within the system. Experimental results using a prototype system are presented and the performance of the algorithm is evaluated to demonstrate its feasibility for real time applications.},
  author       = {Cai, Q. and Aggarwal, J.K.},
  date         = {1999-11},
  doi          = {10.1109/34.809119},
  issn         = {1939-3539},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {1241--1247},
  title        = {Tracking human motion in structured environments using a distributed-camera system},
  volume       = {21},
}

@inproceedings{Chang01,
  abstract  = {We present a multi-camera system based on Bayesian modality fusion to track multiple people in an indoor environment. Bayesian networks are used to combine multiple modalities for matching subjects between consecutive image frames and between multiple camera views. Unlike other occlusion reasoning methods, we use multiple cameras in order to obtain continuous visual information of people in either or both cameras so that they can be tracked through interactions. Results demonstrate that the system can maintain people's identities by using multiple cameras cooperatively.},
  author    = {Chang, T.-H. and Gong, S.},
  booktitle = {Proceedings 2001 IEEE Workshop on Multi-Object Tracking},
  date      = {2001-07},
  doi       = {10.1109/MOT.2001.937977},
  pages     = {19--26},
  title     = {Tracking multiple people with a multi-camera system},
}

@inproceedings{Chavdarova18,
  abstract  = {People detection methods are highly sensitive to occlusions between pedestrians, which are extremely frequent in many situations where cameras have to be mounted at a limited height. The reduction of camera prices allows for the generalization of static multi-camera set-ups. Using joint visual information from multiple synchronized cameras gives the opportunity to improve detection performance. In this paper, we present a new large-scale and high-resolution dataset. It has been captured with seven static cameras in a public open area, and unscripted dense groups of pedestrians standing and walking. Together with the camera frames, we provide an accurate joint (extrinsic and intrinsic) calibration, as well as 7 series of 400 annotated frames for detection at a rate of 2 frames per second. This results in over 40 000 bounding boxes delimiting every person present in the area of interest, for a total of more than 300 individuals. We provide a series of benchmark results using baseline algorithms published over the recent months for multi-view detection with deep neural networks, and trajectory estimation using a non-Markovian model.},
  author    = {Chavdarova, Tatjana and Baqué, Pierre and Bouquet, Stéphane and Maksai, Andrii and Jose, Cijo and Bagautdinov, Timur and Lettry, Louis and Fua, Pascal and Van Gool, Luc and Fleuret, François},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  date      = {2018-06},
  doi       = {10.1109/CVPR.2018.00528},
  issn      = {2575-7075},
  keywords  = {},
  number    = {},
  pages     = {5030-5039},
  title     = {WILDTRACK: A Multi-camera HD Dataset for Dense Unscripted Pedestrian Detection},
  volume    = {},
}

@article{Chen14,
  abstract     = {In this paper, we introduce a novel algorithm to solve the problem of object tracking across multiple non-overlapping cameras by learning inter-camera transfer models. The transfer models are divided into two parts according to different kinds of cues, i.e. spatio-temporal cues and appearance cues. To learn spatio-temporal transfer models across cameras, an unsupervised topology recovering approach based on N-neighbor accumulated cross-correlations is proposed, which estimates the topology of a non-overlapping multi-camera network. Different from previous methods, the proposed topology recovering method can deal with large amounts of data without considering the size of time window. To learn inter-camera appearance transfer models, a color transfer method is used to model the changes of color characteristics across cameras, which has an advantage of low requirements to training samples, making update efficient when illumination conditions change. The experiments are performed on different datasets. Experimental results demonstrate the effectiveness of the proposed algorithm.},
  author       = {Chen, Xiaotang and Huang, Kaiqi and Tan, Tieniu},
  date         = {2014},
  doi          = {https://doi.org/10.1016/j.patcog.2013.06.011},
  issn         = {0031-3203},
  journaltitle = {Pattern Recognition},
  keywords     = {Object tracking,Transfer models,Color transfer,Camera network,Non-overlapping views},
  note         = {Handwriting Recognition and other PR Applications},
  number       = {3},
  pages        = {1126--1137},
  title        = {Object tracking across non-overlapping views by learning inter-camera transfer models},
  url          = {https://www.sciencedirect.com/science/article/pii/S003132031300263X},
  volume       = {47},
}

@article{Chen15,
  abstract     = {Tracking multiple targets in nonoverlapping cameras are challenging since the observations of the same targets are often separated by time and space. There might be significant appearance change of a target across camera views caused by variations in illumination conditions, poses, and camera imaging characteristics. Consequently, the same target may appear very different in two cameras. Therefore, associating tracks in different camera views directly based on their appearance similarity is difficult and prone to error. In most previous methods, the appearance similarity is computed either using color histograms or based on pretrained brightness transfer function that maps color between cameras. In this paper, a novel reference set based appearance model is proposed to improve multitarget tracking in a network of nonoverlapping cameras. Contrary to previous work, a reference set is constructed for a pair of cameras, containing subjects appearing in both camera views. For track association, instead of directly comparing the appearance of two targets in different camera views, they are compared indirectly via the reference set. Besides global color histograms, texture and shape features are extracted at different locations of a target, and AdaBoost is used to learn the discriminative power of each feature. The effectiveness of the proposed method over the state of the art on two challenging real-world multicamera video data sets is demonstrated by thorough experiments.},
  author       = {Chen, Xiaojing and An, Le and Bhanu, Bir},
  date         = {2015-05},
  doi          = {10.1109/JSEN.2015.2392781},
  issn         = {1558-1748},
  journaltitle = {IEEE Sensors Journal},
  number       = {5},
  pages        = {2692--2704},
  title        = {Multitarget Tracking in Nonoverlapping Cameras Using a Reference Set},
  volume       = {15},
}

@article{Chen17a,
  abstract     = {Nonoverlapping multicamera visual object tracking typically consists of two steps: single-camera object tracking (SCT) and inter-camera object tracking (ICT). Most of tracking methods focus on SCT, which happens in the same scene, while for real surveillance scenes, ICT is needed and single-camera tracking methods cannot work effectively. In this paper, we try to improve the overall multicamera object tracking performance by a global graph model with an improved similarity metric. Our method treats the similarities of single-camera tracking and inter-camera tracking differently and obtains the optimization in a global graph model. The results show that our method can work better even in the condition of poor SCT.},
  author       = {Chen, Weihua and Cao, Lijun and Chen, Xiaotang and Huang, Kaiqi},
  date         = {2017-11},
  doi          = {10.1109/TCSVT.2016.2589619},
  issn         = {1558-2205},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  number       = {11},
  pages        = {2367--2381},
  title        = {An Equalized Global Graph Model-Based Approach for Multicamera Object Tracking},
  volume       = {27},
}

@article{Chen17b,
  abstract     = {Tracking multiple targets across nonoverlapping cameras aims at estimating the trajectories of all targets, and maintaining their identity labels consistent while they move from one camera to another. Matching targets from different cameras can be very challenging, as there might be significant appearance variation and the blind area between cameras makes the target’s motion less predictable. Unlike most of the existing methods that only focus on modeling the appearance and spatiotemporal cues for inter-camera tracking, this paper presents a novel online learning approach that considers integrating high-level contextual information into the tracking system. The tracking problem is formulated using an online learned conditional random field (CRF) model that minimizes a global energy cost. Besides low-level information, social grouping behavior is explored in order to maintain targets’ identities as they move across cameras. In the proposed method, pairwise grouping behavior of targets is first learned within each camera. During inter-camera tracking, track associations that maintain single camera grouping consistencies are preferred. In addition, we introduce an iterative algorithm to find a good solution for the CRF model. Comparison experiments on several challenging real-world multicamera video sequences show that the proposed method is effective and outperforms the state-of-the-art approaches.},
  author       = {Chen, Xiaojing and Bhanu, Bir},
  date         = {2017-11},
  doi          = {10.1109/TCSVT.2016.2565978},
  issn         = {1558-2205},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  number       = {11},
  pages        = {2382--2394},
  title        = {Integrating Social Grouping for Multitarget Tracking Across Cameras in a CRF Model},
  volume       = {27},
}

@inproceedings{Chen22,
  abstract  = {Multi-camera Multi-object Tracking (MCMOT) is a challenging problem. Most of the existing methods use a centralized architecture to achieve high tracking accuracy. However, as the number of devices increases, the communication and computing needs of a centralized architecture will grow superlinearly, making it difficult to use in city-scale scenarios. In this paper, we propose a real-time distributed MCMOT system, Uni-ID, which ensures that the communication and computing costs of each device remain almost constant as the number of devices increases. Uni-ID uses the existing single camera detection and tracking architecture and adds a feature extraction network between the detection and tracking phases. By broadcasting the extracted features, the device can reach a consensus on the object so that an object has the same ID in different devices. The system has been tested with real-device recorded data and achieved state-of-the-art performance},
  author    = {Chen, Yuzhong and Ma, Lingfei and Liu, Siyu and Liu, Mengzhen and Wu, Chensizhu and Li, Mo},
  booktitle = {2022 2nd International Conference on Electrical Engineering and Mechatronics Technology (ICEEMT)},
  date      = {2022-07},
  doi       = {10.1109/ICEEMT56362.2022.9862731},
  pages     = {146--149},
  title     = {A Real-time Distributed Multi-camera Multi-object Tracking System},
}

@inproceedings{Cheng23,
  author      = {Cheng, Cheng-Che and Qiu, Min-Xuan and Chiang, Chen-Kuo and Lai, Shang-Hong},
  booktitle   = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  date        = {2023},
  eprint      = {2308.13229},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  pages       = {10051--10060},
  title       = {ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking},
}

@article{Cho19,
  abstract     = {In this study, we propose a unified framework which jointly solves both person re-identification and camera network topology inference problems with minimal prior knowledge about the environments. The proposed framework takes general multi-camera network environments into account and can be applied to online person re-identification in large-scale multi-camera networks. In addition, to show the superiority of the proposed framework, we provide a new person re-identification dataset with full annotations, named SLP, captured in the multi-camera network. Experimental results using our re-identification and public datasets show that the proposed methods are promising for both person re-identification and camera topology inference tasks.},
  author       = {Cho, Yeong-Jun and Kim, Su-A and Park, Jae-Han and Lee, Kyuewang and Yoon, Kuk-Jin},
  date         = {2019},
  doi          = {https://doi.org/10.1016/j.cviu.2019.01.003},
  issn         = {1077-3142},
  journaltitle = {Computer Vision and Image Understanding},
  pages        = {34--46},
  title        = {Joint person re-identification and camera network topology inference in multiple cameras},
  url          = {https://www.sciencedirect.com/science/article/pii/S1077314219300037},
  volume       = {180},
}

@inproceedings{Choi12,
  abstract  = {We present a coherent, discriminative framework for simultaneously tracking multiple people and estimating their collective activities. Instead of treating the two problems separately, our model is grounded in the intuition that a strong correlation exists between a person's motion, their activity, and the motion and activities of other nearby people. Instead of directly linking the solutions to these two problems, we introduce a hierarchy of activity types that creates a natural progression that leads from a specific person's motion to the activity of the group as a whole. Our model is capable of jointly tracking multiple people, recognizing individual activities (atomic activities), the interactions between pairs of people (interaction activities), and finally the behavior of groups of people (collective activities). We also propose an algorithm for solving this otherwise intractable joint inference problem by combining belief propagation with a version of the branch and bound algorithm equipped with integer programming. Experimental results on challenging video datasets demonstrate our theoretical claims and indicate that our model achieves the best collective activity classification results to date.},
  author    = {Choi, Wongun and Savarese, Silvio},
  booktitle = {Computer Vision -- ECCV 2012},
  date      = {2012},
  editor    = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  isbn      = {978-3-642-33765-9},
  location  = {Berlin, Heidelberg},
  pages     = {215--230},
  publisher = {Springer Berlin Heidelberg},
  title     = {A Unified Framework for Multi-target Tracking and Collective Activity Recognition},
}

@inproceedings{Choi15,
  abstract  = {In this paper, we tackle two key aspects of multiple target tracking problem: 1 designing an accurate affinity measure to associate detections and 2 implementing an efficient and accurate (near) online multiple target tracking algorithm. As for the first contribution, we introduce a novel Aggregated Local Flow Descriptor (ALFD) that encodes the relative motion pattern between a pair of temporally distant detections using long term interest point trajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust affinity measure for estimating the likelihood of matching detections regardless of the application scenarios. As for another contribution, we present a Near-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is formulated as a data-association between targets and detections in a temporal window, that is performed repeatedly at every frame. While being efficient, NOMT achieves robustness via integrating multiple cues including ALFD metric, target dynamics, appearance similarity, and long term trajectory regularization into the model. Our ablative analysis verifies the superiority of the ALFD metric over the other conventional affinity metrics. We run a comprehensive experimental evaluation on two challenging tracking datasets, KITTI [16] and MOT [2] datasets. The NOMT method combined with ALFD metric achieves the best accuracy in both datasets with significant margins (about 10{\%} higher MOTA) over the state-of-the-art.},
  author    = {Choi, Wongun},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  date      = {2015-12},
  doi       = {10.1109/ICCV.2015.347},
  issn      = {2380-7504},
  pages     = {3029--3037},
  title     = {Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor},
}

@inproceedings{Choi16,
  abstract  = {Outdoor visual surveillance systems have studied object tracking in non-overlapping multiple cameras. In this paper, we present a data association method for multi-object tracking in a non-overlapping camera network. Proposed method is based on similarity function which consists of a multi-camera topology obtained from local tracking results and matching results from person re-identification. Simulation results indicate that the proposed method effectively deal with object tracking approach in a non-overlapping camera network.},
  author    = {Choi, Hyunguk and Jeon, Moongu},
  booktitle = {2016 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)},
  date      = {2016-10},
  doi       = {10.1109/ICCE-Asia.2016.7804834},
  pages     = {1--4},
  title     = {Data association for non-overlapping multi-camera multi-object tracking based on similarity function},
}

@inproceedings{Chu19,
  abstract  = {Data association-based multiple object tracking (MOT) involves multiple separated modules processed or optimized differently, which results in complex method design and requires non-trivial tuning of parameters. In this paper, we present an end-to-end model, named FAMNet, where Feature extraction, Affinity estimation and Multi-dimensional assignment are refined in a single network. All layers in FAMNet are designed differentiable thus can be optimized jointly to learn the discriminative features and higher-order affinity model for robust MOT, which is supervised by the loss directly from the assignment ground truth. In addition, we integrate single object tracking technique and a dedicated target management scheme into the FAMNet-based tracking system to further recover false negatives and inhibit noisy target candidates generated by the external detector. The proposed method is evaluated on a diverse set of benchmarks including MOT2015, MOT2017, KITTI-Car and UA-DETRAC, and achieves promising performance on all of them in comparison with state-of-the-arts.},
  author    = {Chu, Peng and Ling, Haibin},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  date      = {2019-10},
  doi       = {10.1109/ICCV.2019.00627},
  issn      = {2380-7504},
  pages     = {6171--6180},
  title     = {FAMNet: Joint Learning of Feature, Affinity and Multi-Dimensional Assignment for Online Multiple Object Tracking},
}

@inproceedings{Cipolla18,
  abstract  = {Numerous deep learning applications benefit from multitask learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
  author    = {Cipolla, Roberto and Gal, Yarin and Kendall, Alex},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  date      = {2018-06},
  doi       = {10.1109/CVPR.2018.00781},
  issn      = {2575-7075},
  keywords  = {},
  number    = {},
  pages     = {7482-7491},
  title     = {Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics},
  volume    = {},
}

@inproceedings{Dalal05,
  abstract  = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  author    = {Dalal, N. and Triggs, B.},
  booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  date      = {2005-06},
  doi       = {10.1109/CVPR.2005.177},
  issn      = {1063-6919},
  keywords  = {},
  number    = {},
  pages     = {886-893 vol. 1},
  title     = {Histograms of oriented gradients for human detection},
  volume    = {1},
}

@book{Daniilidis10,
  author    = {Kostas Daniilidis, Maragos},
  date      = {2010-09},
  doi       = {10.1007/978-3-642-15549-9},
  publisher = {Springer Berlin, Heidelberg},
  title     = {Computer Vision -- ECCV 2010},
}

@inproceedings{Das14,
  abstract  = {Most existing person re-identification methods focus on finding similarities between persons between pairs of cameras (camera pairwise re-identification) without explicitly maintaining consistency of the results across the network. This may lead to infeasible associations when results from different camera pairs are combined. In this paper, we propose a network consistent re-identification (NCR) framework, which is formulated as an optimization problem that not only maintains consistency in re-identification results across the network, but also improves the camera pairwise re-identification performance between all the individual camera pairs. This can be solved as a binary integer programing problem, leading to a globally optimal solution. We also extend the proposed approach to the more general case where all persons may not be present in every camera. Using two benchmark datasets, we validate our approach and compare against state-of-the-art methods.},
  author    = {Das, Abir and Chakraborty, Anirban and Roy-Chowdhury, Amit K.},
  booktitle = {Computer Vision -- ECCV 2014},
  date      = {2014},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  isbn      = {978-3-319-10605-2},
  location  = {Cham},
  pages     = {330--345},
  publisher = {Springer International Publishing},
  title     = {Consistent Re-identification in a Camera Network},
}

@inproceedings{Davila23,
  abstract  = {In this paper, we present the Multi-view Extended Videos with Identities (MEVID) dataset for large-scale, video person re-identification (ReID) in the wild. To our knowledge, MEVID represents the most-varied video person ReID dataset, spanning an extensive indoor and outdoor environment across nine unique dates in a 73-day window, various camera viewpoints, and entity clothing changes. Specifically, we label the identities of 158 unique people wearing 598 outfits taken from 8, 092 tracklets, average length of about 590 frames, seen in 33 camera views from the very-large-scale MEVA person activities dataset. While other datasets have more unique identities, MEVID emphasizes a richer set of information about each individual, such as: 4 outfits/identity vs. 2 outfits/identity in CCVID, 33 viewpoints across 17 locations vs. 6 in 5 simulated locations for MTA, and 10 million frames vs. 3 million for LS-VID. Being based on the MEVA video dataset, we also inherit data that is intentionally demographically balanced to the continental United States. To accelerate the annotation process, we developed a semi-automatic annotation framework and GUI that combines state-of-the-art real-time models for object detection, pose estimation, person ReID, and multi-object tracking. We evaluate several state-of-the-art methods on MEVID challenge problems and comprehensively quantify their robustness in terms of changes of outfit, scale, and background location. Our quantitative analysis on the realistic, unique aspects of MEVID shows that there are significant remaining challenges in video person ReID and indicates important directions for future research.},
  author    = {Davila, Daniel and Du, Dawei and Lewis, Bryon and Funk, Christopher and Van Pelt, Joseph and Collins, Roderic and Corona, Kellie and Brown, Matt and McCloskey, Scott and Hoogs, Anthony and Clipp, Brian},
  booktitle = {2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  date      = {2023-01},
  doi       = {10.1109/WACV56688.2023.00168},
  issn      = {2642-9381},
  keywords  = {},
  number    = {},
  pages     = {1634-1643},
  title     = {MEVID: Multi-view Extended Videos with Identities for Video Person Re-Identification},
  volume    = {},
}

@inproceedings{Dehghan15,
  abstract  = {In this paper we show that multiple object tracking (MOT) can be formulated in a framework, where the detection and data-association are performed simultaneously. Our method allows us to overcome the confinements of data association based MOT approaches; where the performance is dependent on the object detection results provided at input level. At the core of our method lies structured learning which learns a model for each target and infers the best location of all targets simultaneously in a video clip. The inference of our structured learning is done through a new Target Identity-aware Network Flow (TINF), where each node in the network encodes the probability of each target identity belonging to that node. The proposed Lagrangian relaxation optimization finds the high quality solution to the network. During optimization a soft spatial constraint is enforced between the nodes of the graph which helps reducing the ambiguity caused by nearby targets with similar appearance in crowded scenarios. We show that automatically detecting and tracking targets in a single framework can help resolve the ambiguities due to frequent occlusion and heavy articulation of targets. Our experiments involve challenging yet distinct datasets and show that our method can achieve results better than the state-of-art.},
  author    = {Dehghan, Afshin and Tian, Yicong and Torr, Philip. H. S. and Shah, Mubarak},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2015-06},
  doi       = {10.1109/CVPR.2015.7298718},
  issn      = {1063-6919},
  pages     = {1146--1154},
  title     = {Target Identity-aware Network Flow for online multiple target tracking},
}

@article{Dendorfer19,
  author     = {Dendorfer, P. and Rezatofighi, H. and Milan, A. and Shi, J. and Cremers, D. and Reid, I. and Roth, S. and Schindler, K. and Leal-Taix\'{e}, L. },
  date       = {2019-06},
  journal    = {arXiv:1906.04567 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  note       = {arXiv: 1906.04567},
  shorttitle = {MOT19},
  title      = {{CVPR19} Tracking and Detection Challenge: {H}ow crowded can it get?},
  url        = {http://arxiv.org/abs/1906.04567},
}

@article{Dendorfer20,
  author     = {Dendorfer, P. and Rezatofighi, H. and Milan, A. and Shi, J. and Cremers, D. and Reid, I. and Roth, S. and Schindler, K. and Leal-Taix\'{e}, L. },
  date       = {2020-03},
  journal    = {arXiv:2003.09003[cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  note       = {arXiv: 2003.09003},
  shorttitle = {MOT20},
  title      = {MOT20: A benchmark for multi object tracking in crowded scenes},
  url        = {http://arxiv.org/abs/1906.04567},
}

@inproceedings{Deng09,
  abstract  = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  date      = {2009-06},
  doi       = {10.1109/CVPR.2009.5206848},
  issn      = {1063-6919},
  keywords  = {},
  number    = {},
  pages     = {248-255},
  title     = {ImageNet: A large-scale hierarchical image database},
  volume    = {},
}

@inproceedings{Ferryman09,
  abstract  = {This paper describes the crowd image analysis challenge that forms part of the PETS 2009 workshop. The aim of this challenge is to use new or existing systems for i) crowd count and density estimation, ii) tracking of individual(s) within a crowd, and iii) detection of separate flows and specific crowd events, in a real-world environment. The dataset scenarios were filmed from multiple cameras and involve multiple actors.},
  author    = {Ferryman, J. and Shahrokni, A.},
  booktitle = {2009 Twelfth IEEE International Workshop on Performance Evaluation of Tracking and Surveillance},
  date      = {2009-12},
  doi       = {10.1109/PETS-WINTER.2009.5399556},
  issn      = {},
  keywords  = {},
  number    = {},
  pages     = {1-6},
  title     = {PETS2009: Dataset and challenge},
  volume    = {},
}

@article{Fleuret08,
  abstract     = {Given two to four synchronized video streams taken at eye level and from different angles, we show that we can effectively combine a generative model with dynamic programming to accurately follow up to six individuals across thousands of frames in spite of significant occlusions and lighting changes. In addition, we also derive metrically accurate trajectories for each one of them.Our contribution is twofold. First, we demonstrate that our generative model can effectively handle occlusions in each time frame independently, even when the only data available comes from the output of a simple background subtraction algorithm and when the number of individuals is unknown a priori. Second, we show that multi-person tracking can be reliably achieved by processing individual trajectories separately over long sequences, provided that a reasonable heuristic is used to rank these individuals and avoid confusing them with one another.},
  author       = {Fleuret, Francois and Berclaz, Jerome and Lengagne, Richard and Fua, Pascal},
  date         = {2008-02},
  doi          = {10.1109/TPAMI.2007.1174},
  issn         = {1939-3539},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {267--282},
  title        = {Multicamera People Tracking with a Probabilistic Occupancy Map},
  volume       = {30},
}

@article{Fortmann83,
  abstract = {The problem of associating data with targets in a cluttered multi-target environment is discussed and applied to passive sonar tracking. The probabilistic data association (PDA) method, which is based on computing the posterior probability of each candidate measurement found in a validation gate, assumes that only one real target is present and all other measurements are Poisson-distributed clutter. In this paper, a new theoretical result is presented: the joint probabilistic data association (JPDA) algorithm, in which joint posterior association probabilities are computed for multiple targets (or multiple discrete interfering sources) in Poisson clutter. The algorithm is applied to a passive sonar tracking problem with multiple sensors and targets, in which a target is not fully observable from a single sensor. Targets are modeled with four geographic states, two or more acoustic states, and realistic (i.e., low) probabilities of detection at each sample time. A simulation result is presented for two heavily interfering targets illustrating the dramatic tracking improvements obtained by estimating the targets' states using joint association probabilities.},
  author   = {Fortmann, T. and Bar-Shalom, Y. and Scheffe, M.},
  date     = {1983-06},
  doi      = {10.1109/JOE.1983.1145560},
  issn     = {1558-1691},
  journal  = {IEEE Journal of Oceanic Engineering},
  keywords = {},
  number   = {3},
  pages    = {173-184},
  title    = {Sonar tracking of multiple targets using joint probabilistic data association},
  volume   = {8},
}

@inproceedings{Girshick14,
  abstract  = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
  author    = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  date      = {2014-06},
  doi       = {10.1109/CVPR.2014.81},
  issn      = {1063-6919},
  keywords  = {},
  number    = {},
  pages     = {580-587},
  title     = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  volume    = {},
}

@inproceedings{Girshick15,
  abstract  = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  author    = {Girshick, Ross},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  date      = {2015-12},
  doi       = {10.1109/ICCV.2015.169},
  issn      = {2380-7504},
  keywords  = {},
  number    = {},
  pages     = {1440-1448},
  title     = {Fast R-CNN},
  volume    = {},
}

@inproceedings{Han23,
  abstract  = {Multi-camera tracking systems are gaining popularity in applications that demand high-quality tracking results, such as frictionless checkout. In cluttered and crowded environments, monocular multi-object tracking (MOT) systems often fail due to occlusions. Multiple highly overlapped cameras are capable of recovering partial 3D information. When used properly, 3D data can significantly alleviate the occlusion issue. However, training a multi-camera tracker demands a large-scale multi-camera tracking dataset with diverse camera settings and backgrounds. These requirements make the collection of multi-camera tracking dataset challenging and expensive. The cost of creating such a dataset has limited the availability and scale of datasets in this domain. Instead, we appeal to an auto-annotation system to reduce the cost, which uses overlapped and calibrated depth and RGB cameras to build a 3D tracker and automatically generates the 3D tracking results. The results are manually checked and corrected to ensure the label quality, which is much cheaper than solely manual annotation. Next, the 3D tracking results are projected to each calibrated RGB camera view to create 2D tracking results. In this way, we collect and annotate a large-scale densely labeled multi-camera tracking dataset from five different environments. We have conducted extensive experiments using two real-time multi-camera trackers and a person re-identification (ReID) model under different settings. This dataset provides a reliable benchmark for multi-camera, multi-object tracking systems in cluttered and crowded environments. We expect this benchmark to encourage more research attempts in this domain. Our dataset will be publicly released upon the acceptance of this work.},
  author    = {Han, Xiaotian and You, Quanzeng and Wang, Chunyu and Zhang, Zhizheng and Chu, Peng and Hu, Houdong and Wang, Jiang and Liu, Zicheng},
  booktitle = {2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  date      = {2023-01},
  doi       = {10.1109/WACV56688.2023.00484},
  issn      = {2642-9381},
  keywords  = {},
  number    = {},
  pages     = {4849-4858},
  title     = {MMPTRACK: Large-scale Densely Annotated Multi-camera Multiple People Tracking Benchmark},
  volume    = {},
}

@inproceedings{He16,
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2016-06},
  doi       = {10.1109/CVPR.2016.90},
  issn      = {1063-6919},
  keywords  = {},
  number    = {},
  pages     = {770-778},
  title     = {Deep Residual Learning for Image Recognition},
  volume    = {},
}

@inproceedings{He19,
  author    = {He, Zhiqun and Lei, Yu and Bai, Shuai and Wu, Wei},
  booktitle = {CVPR Workshops},
  date      = {2019},
  title     = {Multi-Camera Vehicle Tracking with Powerful Visual Features and Spatial-Temporal Cue.},
  volume    = {1},
}

@article{He20a,
  abstract     = {This paper focuses on the Multi-Target Multi-Camera Tracking task (MTMCT), which aims at tracking multiple targets within a multi-camera network. As the trajectory of each target is inherently split into multiple sub-trajectories (namely local tracklets) in a multi-camera network, a major challenge of MTMCT is how to accurately match the local tracklets generated within each camera across different cameras and generate a complete global trajectory for each target, i.e., the cross-camera tracklet matching problem. We solve the cross-camera tracklet matching problem by TRACklet-to-Target Assignment (TRACTA), and propose the Restricted Non-negative Matrix Factorization (RNMF) algorithm to compute the optimal assignment solution that meets a set of constraints, which should be in force in practice. TRACTA can correct the tracking errors caused by occlusions and missed detections in local tracklets, and produce a complete global trajectory for each target across all the cameras. Moreover, we also develop an analytical way of estimating the total number of targets in the camera network, which plays an important role to compute the tracklet-to-target assignment. Experimental evaluations and ablation studies on four MTMCT benchmark datasets show the superiority of the proposed TRACTA method.},
  author       = {He, Yuhang and Wei, Xing and Hong, Xiaopeng and Shi, Weiwei and Gong, Yihong},
  date         = {2020},
  doi          = {10.1109/TIP.2020.2980070},
  issn         = {1941-0042},
  journaltitle = {IEEE Transactions on Image Processing},
  pages        = {5191--5205},
  title        = {Multi-Target Multi-Camera Tracking by Tracklet-to-Target Assignment},
  volume       = {29},
}

@inproceedings{He20b,
  abstract  = {This paper focuses on the Multi-Target Multi-Camera Tracking (MTMCT) task in a city-scale multi-camera network. As the trajectory of each target is naturally split into multiple sub-trajectories (namely local tracklets) in different cameras, the key issue of MTMCT is how to match local tracklets belonging to the same target across different cameras. To this end, we propose an efficient two-step MTMCT approach to robustly track vehicles in a camera network. It first generates all local tracklets and then matches the ones belonging to the same target across different cameras. More specifically, in the local tracklet generation phase, we follow the tracking-by-detection paradigm and link the detections to local tracklets by graph clustering. In the cross-camera tracklet matching phase, we first develop a spatial-temporal attention mechanism to produce robust tracklet representations. We then prune false matching candidates by traffic topology reasoning and match tracklets across cameras using the recently proposed TRACklet-to-Target Assignment (TRACTA) algorithm. The proposed method is evaluated on the City-Scale Multi-Camera Vehicle Tracking task at the 2020 AI City Challenge and achieves the second-best results.},
  author    = {He, Yuhang and Han, Jie and Yu, Wentao and Hong, Xiaopeng and Wei, Xing and Gong, Yihong},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2020-06},
  doi       = {10.1109/CVPRW50498.2020.00296},
  issn      = {2160-7516},
  pages     = {2456--2465},
  title     = {City-Scale Multi-Camera Vehicle Tracking by Semantic Attribute Parsing and Cross-Camera Tracklet Matching},
}

@inproceedings{Heath08,
  abstract  = {We describe and evaluate a vision-based technique for tracking many people with a network of stereo camera sensors. The technique requires lightweight local communication and is suitable for crowded scenes where targets are frequently occluded and where appearance based modeling techniques fail. In our approach, multiple stereo sensors individually estimate the 3D trajectories of salient feature points on moving objects. Sensors communicate a subset of their sparse 3D measurements to other sensors with overlapping views. Each sensor fuses 3D measurements from nearby sensors using a particle filter to robustly track nearby objects. We evaluate the technique using the MOTA-MOTP multi-target tracking performance metrics on real data sets with up to 6 people and on challenging simulations of crowds of up to 25 people with uniform appearance. Our method achieves a tracking precision of 10-30 cm in all cases and good tracking accuracy even in crowded scenes of 15 people.},
  author    = {Heath, Kyle and Guibas, Leonidas},
  booktitle = {2008 Second ACM/IEEE International Conference on Distributed Smart Cameras},
  date      = {2008-09},
  doi       = {10.1109/ICDSC.2008.4635679},
  pages     = {1--9},
  title     = {Multi-person tracking from sparse 3D trajectories in a camera sensor network},
}

@article{Hou19,
  author      = {Hou, Yunzhong and Zheng, Liang and Wang, Zhongdao and Wang, Shengjin},
  date        = {2019},
  eprint      = {1911.12037},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  title       = {Locality Aware Appearance Metric for Multi-Target Multi-Camera Tracking},
}

@inproceedings{Hsu19,
  author    = {Hsu, Hung-Min and Huang, Tsung-Wei and Wang, Gaoang and Cai, Jiarui and Lei, Zhichao and Hwang, Jenq-Neng},
  booktitle = {CVPR workshops},
  date      = {2019},
  pages     = {416--424},
  title     = {Multi-camera tracking of vehicles based on deep features re-id and trajectory-based camera link models.},
}

@article{Hsu21,
  abstract     = {In this paper, we propose a novel framework for multi-target multi-camera tracking (MTMCT) of vehicles based on metadata-aided re-identification (MA-ReID) and the trajectory-based camera link model (TCLM). Given a video sequence and the corresponding frame-by-frame vehicle detections, we first address the isolated tracklets issue from single camera tracking (SCT) by the proposed traffic-aware single-camera tracking (TSCT). Then, after automatically constructing the TCLM, we solve MTMCT by the MA-ReID. The TCLM is generated from camera topological configuration to obtain the spatial and temporal information to improve the performance of MTMCT by reducing the candidate search of ReID. We also use the temporal attention model to create more discriminative embeddings of trajectories from each camera to achieve robust distance measures for vehicle ReID. Moreover, we train a metadata classifier for MTMCT to obtain the metadata feature, which is concatenated with the temporal attention based embeddings. Finally, the TCLM and hierarchical clustering are jointly applied for global ID assignment. The proposed method is evaluated on the CityFlow dataset, achieving IDF1 76.77{\%}, which outperforms the state-of-the-art MTMCT methods.},
  author       = {Hsu, Hung-Min and Cai, Jiarui and Wang, Yizhou and Hwang, Jenq-Neng and Kim, Kwang-Ju},
  date         = {2021},
  doi          = {10.1109/TIP.2021.3078124},
  issn         = {1941-0042},
  journaltitle = {IEEE Transactions on Image Processing},
  pages        = {5198--5210},
  title        = {Multi-Target Multi-Camera Tracking of Vehicles Using Metadata-Aided Re-ID and Trajectory-Based Camera Link Model},
  volume       = {30},
}

@inproceedings{Hsu22,
  abstract  = {Multi-Target Multi-Camera Tracking (MTMCT) of vehicles is a challenging task in smart city related applications. The main challenge of MTMCT is how to accurately match the single-camera trajectories generated from different cameras and establish a complete global cross-camera trajectory for each target, i.e., the multi-camera trajectory matching problem. In this paper, we propose a novel framework to solve this problem using the self-supervised trajectory-based camera link model (CLM) with both appearance and topological features systematically extracted from a graph auto-encoder (GAE) network. Unlike most related works that represent the spatio-temporal relationships of multiple cameras with the laborious human-annotated CLM, we introduce a self-supervised CLM (SCLM) generation method that extracts the crucial multi-camera relationships among the vehicle trajectories passing through different cameras robustly and automatically. Moreover, we apply a GAE to encode topological information and appearance features to generate the topological embeddings. According to our experimental results, the proposed method achieves a new state-of-the-art on both CityFlow 2019 and CityFlow 2020 benchmarks with IDF1 of 77.21{\%} and 55.56{\%}, respectively.},
  author    = {Hsu, Hung-Min and Wang, Yizhou and Cai, Jiarui and Hwang, Jenq-Neng},
  booktitle = {2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)},
  date      = {2022-01},
  doi       = {10.1109/WACVW54805.2022.00055},
  issn      = {2690-621X},
  pages     = {489--499},
  title     = {Multi-Target Multi-Camera Tracking of Vehicles by Graph Auto-Encoder and Self-Supervised Camera Link Model},
}

@inproceedings{Huang23a,
  abstract  = {The goal of the Multi-Target Multi-Camera (MTMC) pedestrian tracking task is to simultaneously track multiple target individuals using multiple cameras. Current methods mostly use exponential moving averages to store features and perform multi-camera person matching using these features. Apparently, it will cause the issue of poor long-term feature storage, in this paper, we propose a new method to address the issue, which often leads to ID-switching when individuals change clothes or when lighting conditions change significantly, and also improve the ID-switching problem that occurs during single-camera tracking. To evaluate our method, we created our owns dataset. The dataset was included approximately 40000 frames from 1080p, 30fps videos, whitch were recorded by these cameras. Experimental results show that our method outperforms existing methods in both single-camera and multi-camera tracking, with single-camera tracking improved by 7.06{\%} and multi-camera tracking improved by 12.51{\%}.},
  author    = {Huang, Ding-Jie and Chou, Po-Yung and Xie, Bo-Zheng and Lin, Cheng-Hung},
  booktitle = {2023 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)},
  date      = {2023-07},
  doi       = {10.1109/ICCE-Taiwan58799.2023.10227006},
  issn      = {2575-8284},
  pages     = {629--630},
  title     = {Multi-Target Multi-Camera Pedestrian Tracking System for Non-Overlapping Cameras},
}

@misc{Huang23b,
  archiveprefix = {arXiv},
  author        = {Hsiang-Wei Huang and Cheng-Yen Yang and Zhongyu Jiang and Pyong-Kun Kim and Kyoungoh Lee and Kwangju Kim and Samartha Ramkumar and Chaitanya Mullapudi and In-Su Jang and Chung-I Huang and Jenq-Neng Hwang},
  eprint        = {2304.09471},
  primaryclass  = {cs.CV},
  title         = {Enhancing Multi-Camera People Tracking with Anchor-Guided Clustering and Spatio-Temporal Consistency ID Re-Assignment},
  year          = {2023},
}

@article{Iguernaissi18,
  author       = {Iguernaissi, Rabah and Merad, Djamal and Aziz, Kheireddine and Drap, Pierre},
  date         = {2018-09},
  journaltitle = {SpringerLink},
  publisher    = {Springer US},
  title        = {People tracking in multi-camera systems: A Review - multimedia tools and applications},
  url          = {https://link.springer.com/article/10.1007/s11042-018-6638-5},
}

@inproceedings{Javed03,
  abstract  = {Conventional tracking approaches assume proximity in space, time and appearance of objects in successive observations. However, observations of objects are often widely separated in time and space when viewed from multiple non-overlapping cameras. To address this problem, we present a novel approach for establishing object correspondence across non-overlapping cameras. Our multicamera tracking algorithm exploits the redundance in paths that people and cars tend to follow, e.g. roads, walk-ways or corridors, by using motion trends and appearance of objects, to establish correspondence. Our system does not require any inter-camera calibration, instead the system learns the camera topology and path probabilities of objects using Parzen windows, during a training phase. Once the training is complete, correspondences are assigned using the maximum a posteriori (MAP) estimation framework. The learned parameters are updated with changing trajectory patterns. Experiments with real world videos are reported, which validate the proposed approach.},
  author    = {Javed and Rasheed and Shafique and Shah},
  booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
  date      = {2003-10},
  doi       = {10.1109/ICCV.2003.1238451},
  pages     = {952--957 vol.2},
  title     = {Tracking across multiple cameras with disjoint views},
}

@inproceedings{Jiang18,
  abstract  = {Online inter-camera trajectory association is a promising topic in intelligent video surveillance, which concentrates on associating trajectories belong to the same individual across different cameras according to time. It remains challenging due to the inconsistent appearance of a person in different cameras and the lack of spatio-temporal constraints between cameras. Besides, the orientation variations and the partial occlusions significantly increase the difficulty of inter-camera trajectory association. Targeting to solve these problems, this work proposes an orientation-driven person re-identification (ODPR) and an effective camera topology estimation based on appearance features for online inter-camera trajectory association. ODPR explicitly leverages the orientation cues and stable torso features to learn discriminative feature representations for identifying trajectories across cameras, which alleviates the pedestrian orientation variations by the designed orientation-driven loss function and orientation aware weights. The effective camera topology estimation introduces appearance features to generate the correct spatio-temporal constraints for narrowing the retrieval range, which improves the time efficiency and provides the possibility for intelligent inter-camera trajectory association in large-scale surveillance environments. Extensive experimental results demonstrate that our proposed approach significantly outperforms most state-of-the-art methods on the popular person re-identification datasets and the public multi-target, multi-camera tracking benchmark.},
  author    = {Jiang, Na and Bai, SiChen and Xu, Yue and Xing, Chang and Zhou, Zhong and Wu, Wei},
  booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
  date      = {2018},
  doi       = {10.1145/3240508.3240663},
  isbn      = {9781450356657},
  keywords  = {person re-identification,camera topology estimation,inter-camera trajectory association},
  location  = {Seoul, Republic of Korea},
  pages     = {1457--1465},
  publisher = {Association for Computing Machinery},
  series    = {MM '18},
  title     = {Online Inter-Camera Trajectory Association Exploiting Person Re-Identification and Camera Topology},
  url       = {https://doi.org/10.1145/3240508.3240663},
}

@article{Kalman60,
  author   = {Kalman, Rudolph Emil and Others},
  journal  = {Journal of basic Engineering},
  keywords = {filtering kalman},
  number   = {1},
  pages    = {35--45},
  title    = {A new approach to linear filtering and prediction problems},
  volume   = {82},
  yedatear = {1960},
}

@inproceedings{Khan01,
  author       = {Khan, Sohaib and Javed, Omar and Shah, Mubarak},
  booktitle    = {2nd IEEE Workshop on Performance Evaluation of Tracking and Surveillance},
  date         = {2001},
  organization = {IEEE Computer Society Press Los Alamitos},
  title        = {Tracking in uncalibrated cameras with overlapping field of view},
  volume       = {5},
}

@article{Khan03,
  abstract     = {We address the issue of tracking moving objects in an environment covered by multiple uncalibrated cameras with overlapping fields of view, typical of most surveillance setups. In such a scenario, it is essential to establish correspondence between tracks of the same object, seen in different cameras, to recover complete information about the object. We call this the problem of consistent labeling of objects when seen in multiple cameras. We employ a novel approach of finding the limits of field of view (FOV) of each camera as visible in the other cameras. We show that, if the FOV lines are known, it is possible to disambiguate between multiple possibilities for correspondence. We present a method to automatically recover these lines by observing motion in the environment, Furthermore, once these lines are initialized, the homography between the views can also be recovered. We present results on indoor and outdoor sequences containing persons and vehicles.},
  author       = {Khan, S. and Shah, M.},
  date         = {2003-10},
  doi          = {10.1109/TPAMI.2003.1233912},
  issn         = {1939-3539},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {1355--1360},
  title        = {Consistent labeling of tracked objects in multiple cameras with overlapping fields of view},
  volume       = {25},
}

@inproceedings{Kim06,
  abstract  = {A multi-view multi-hypothesis approach to segmenting and tracking multiple (possibly occluded) persons on a ground plane is proposed. During tracking, several iterations of segmentation are performed using information from human appearance models and ground plane homography. To more precisely locate the ground location of a person, all center vertical axes of the person across views are mapped to the top-view plane and their intersection point on the ground is estimated. To tackle the explosive state space due to multiple targets and views, iterative segmentation-searching is incorporated into a particle filtering framework. By searching for people's ground point locations from segmentations, a set of a few good particles can be identified, resulting in low computational cost. In addition, even if all the particles are away from the true ground point, some of them move towards the true one through the iterated process as long as they are located nearby. We demonstrate the performance of the approach on several video sequences.},
  author    = {Kim, Kyungnam and Davis, Larry S.},
  booktitle = {Computer Vision -- ECCV 2006},
  date      = {2006},
  editor    = {Leonardis, Aleš and Bischof, Horst and Pinz, Axel},
  isbn      = {978-3-540-33837-6},
  location  = {Berlin, Heidelberg},
  pages     = {98--109},
  publisher = {Springer Berlin Heidelberg},
  title     = {Multi-camera Tracking and Segmentation of Occluded People on Ground Plane Using Search-Guided Particle Filtering},
}

@inproceedings{Koehl20,
  abstract  = {Existing multi target multi camera tracking (MTMCT) datasets are small in terms of the number of identities and video length. The creation of new real world datasets is hard as privacy has to be guaranteed and the labeling is tedious. Therefore in the scope of this work a mod for GTA V to record a MTMCT dataset has been developed and used to record a simulated MTMCT dataset called Multi Camera Track Auto (MTA). The MTA dataset contains over 2,800 person identities, 6 cameras and a video length of over 100 minutes per camera. Additionally a MTMCT system has been implemented to provide a baseline for the created dataset. The system’s pipeline consists of stages for person detection, person re-identification, single camera multi target tracking, track distance calculation, and track association. The track distance calculation comprises a weighted aggregation of the following distances: a single camera time constraint, a multi camera time constraint using overlapping camera areas, an appearance feature distance, a homography matching with pairwise camera homographies, and a linear prediction based on the velocity and the time difference of tracks. When using all partial distances, we were able to surpass the results of state-of-the-art single camera trackers by +13{\%} IDF1 score. The MTA dataset, code, and baselines are available at github.com/schuar-iosb/mta-dataset.},
  author    = {Köhl, Philipp and Specker, Andreas and Schumann, Arne and Beyerer, Jürgen},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2020-06},
  doi       = {10.1109/CVPRW50498.2020.00529},
  issn      = {2160-7516},
  pages     = {4489--4498},
  title     = {The MTA Dataset for Multi Target Multi Camera Pedestrian Tracking by Weighted Distance Aggregation},
}

@inproceedings{Komorowski22,
  abstract  = {The paper presents a multi-camera tracking method intended for tracking soccer players in long shot video recordings from multiple calibrated cameras installed around the playing field. The large distance to the camera makes it difficult to visually distinguish individual players, which adversely affects the performance of traditional solutions relying on the appearance of tracked objects. Our method focuses on individual player dynamics and interactions between neighborhood players to improve tracking performance. To overcome the difficulty of reliably merging detections from multiple cameras in the presence of calibration errors, we propose the novel tracking approach, where the tracker operates directly on raw detection heat maps from multiple cameras. Our model is trained on a large synthetic dataset generated using Google Research Football Environment and fine-tuned using real-world data to reduce costs involved with ground truth preparation.},
  author    = {Komorowski, Jacek and Kurzejamski, Grzegorz},
  booktitle = {2022 International Joint Conference on Neural Networks (IJCNN)},
  date      = {2022-07},
  doi       = {10.1109/IJCNN55064.2022.9892562},
  issn      = {2161-4407},
  pages     = {1--8},
  title     = {Graph-Based Multi-Camera Soccer Player Tracker},
}

@inproceedings{Kristan21,
  abstract  = {The Visual Object Tracking challenge VOT2021 is the ninth annual tracker benchmarking activity organized by the VOT initiative. Results of 71 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in recent years. The VOT2021 challenge was composed of four sub-challenges focusing on different tracking domains: (i) VOT-ST2021 challenge focused on short-term tracking in RGB, (ii) VOT-RT2021 challenge focused on "real-time" short-term tracking in RGB, (iii) VOT-LT2021 focused on long-term tracking, namely coping with target disappearance and reappearance and (iv) VOT-RGBD2021 challenge focused on long-term tracking in RGB and depth imagery. The VOT-ST2021 dataset was refreshed, while VOT-RGBD2021 introduces a training dataset and sequestered dataset for winner identification. The source code for most of the trackers, the datasets, the evaluation kit and the results along with the source code for most trackers are publicly available at the challenge website1.},
  author    = {Kristan, Matej and Matas, Jiří and Leonardis, Aleš and Felsberg, Michael and Pflugfelder, Roman and Kämäräinen, Joni-Kristian and Chang, Hyung Jin and Danelljan, Martin and Zajc, Luka Čehovin and Lukežič, Alan and Drbohlav, Ondrej and Käpylä, Jani and Häger, Gustav and Yan, Song and Yang, Jinyu and Zhang, Zhongqun and Fernández, Gustavo and Abdelpakey, Mohamed and Bhat, Goutam and Cerkezi, Llukman and Cevikalp, Hakan and Chen, Shengyong and Chen, Xin and Cheng, Miao and Cheng, Ziyi and Chiu, Yu-Chen and Cirakman, Ozgun and Cui, Yutao and Dai, Kenan and Dasari, Mohana Murali and Deng, Qili and Dong, Xingping and Du, Daniel K. and Dunnhofer, Matteo and Feng, Zhen-Hua and Feng, Zhiyong and Fu, Zhihong and Ge, Shiming and Gorthi, Rama Krishna and Gu, Yuzhang and Gunsel, Bilge and Guo, Qing and Gurkan, Filiz and Han, Wencheng and Huang, Yanyan and Lawin, Felix Järemo and Jhang, Shang-Jhih and Ji, Rongrong and Jiang, Cheng and Jiang, Yingjie and Juefei-Xu, Felix and Jun, Yin and Ke, Xiao and Khan, Fahad Shahbaz and Hak Kim, Byeong and Kittler, Josef and Lan, Xiangyuan and Lee, Jun Ha and Leibe, Bastian and Li, Hui and Li, Jianhua and Li, Xianxian and Li, Yuezhou and Liu, Bo and Liu, Chang and Liu, Jingen and Liu, Li and Liu, Qingjie and Lu, Huchuan and Lu, Wei and Luiten, Jonathon and Ma, Jie and Ma, Ziang and Martinel, Niki and Mayer, Christoph and Memarmoghadam, Alireza and Micheloni, Christian and Niu, Yuzhen and Paudel, Danda and Peng, Houwen and Qiu, Shoumeng and Rajiv, Aravindh and Rana, Muhammad and Robinson, Andreas and Saribas, Hasan and Shao, Ling and Shehata, Mohamed and Shen, Furao and Shen, Jianbing and Simonato, Kristian and Song, Xiaoning and Tang, Zhangyong and Timofte, Radu and Torr, Philip and Tsai, Chi-Yi and Uzun, Bedirhan and Van Gool, Luc and Voigtlaender, Paul and Wang, Dong and Wang, Guangting and Wang, Liangliang and Wang, Lijun and Wang, Limin and Wang, Linyuan and Wang, Yong and Wang, Yunhong and Wu, Chenyan and Wu, Gangshan and Wu, Xiao-Jun and Xie, Fei and Xu, Tianyang and Xu, Xiang and Xue, Wanli and Yan, Bin and Yang, Wankou and Yang, Xiaoyun and Ye, Yu and Yin, Jun and Zhang, Chengwei and Zhang, Chunhui and Zhang, Haitao and Zhang, Kaihua and Zhang, Kangkai and Zhang, Xiaohan and Zhang, Xiaolin and Zhang, Xinyu and Zhang, Zhibin and Zhao, Shaochuan and Zhen, Ming and Zhong, Bineng and Zhu, Jiawen and Zhu, Xue-Feng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
  date      = {2021-10},
  doi       = {10.1109/ICCVW54120.2021.00305},
  issn      = {2473-9944},
  pages     = {2711--2738},
  title     = {The Ninth Visual Object Tracking VOT2021 Challenge Results},
}

@inproceedings{Kristan22,
  abstract  = {The Visual Object Tracking challenge VOT2022 is the tenth annual tracker benchmarking activity organized by the VOT initiative. Results of 93 entries are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in recent years. The VOT2022 challenge was composed of seven sub-challenges focusing on different tracking domains: (i) VOT-STs2022 challenge focused on short-term tracking in RGB by segmentation, (ii) VOT-STb2022 challenge focused on short-term tracking in RGB by bounding boxes, (iii) VOT-RTs2022 challenge focused on ``real-time'' short-term tracking in RGB by segmentation, (iv) VOT-RTb2022 challenge focused on ``real-time'' short-term tracking in RGB by bounding boxes, (v) VOT-LT2022 focused on long-term tracking, namely coping with target disappearance and reappearance, (vi) VOT-RGBD2022 challenge focused on short-term tracking in RGB and depth imagery, and (vii) VOT-D2022 challenge focused on short-term tracking in depth-only imagery. New datasets were introduced in VOT-LT2022 and VOT-RGBD2022, VOT-ST2022 dataset was refreshed, and a training dataset was introduced for VOT-LT2022. The source code for most of the trackers, the datasets, the evaluation kit and the results are publicly available at the challenge website (http://votchallenge.net).},
  author    = {Kristan, Matej and Leonardis, Aleš and Matas, Jiří and Felsberg, Michael and Pflugfelder, Roman and Kämäräinen, Joni-Kristian and Chang, Hyung Jin and Danelljan, Martin and Zajc, Luka Čehovin and Lukežič, Alan and Drbohlav, Ondrej and Björklund, Johanna and Zhang, Yushan and Zhang, Zhongqun and Yan, Song and Yang, Wenyan and Cai, Dingding and Mayer, Christoph and Fernández, Gustavo and Ben, Kang and Bhat, Goutam and Chang, Hong and Chen, Guangqi and Chen, Jiaye and Chen, Shengyong and Chen, Xilin and Chen, Xin and Chen, Xiuyi and Chen, Yiwei and Chen, Yu-Hsi and Chen, Zhixing and Cheng, Yangming and Ciaramella, Angelo and Cui, Yutao and Džubur, Benjamin and Dasari, Mohana Murali and Deng, Qili and Dhar, Debajyoti and Di, Shangzhe and Nardo, Emanuel Di and Du, Daniel K. and Dunnhofer, Matteo and Fan, Heng and Feng, Zhenhua and Fu, Zhihong and Gao, Shang and Gorthi, Rama Krishna and Granger, Eric and Gu, Q. H. and Gupta, Himanshu and He, Jianfeng and He, Keji and Huang, Yan and Jangid, Deepak and Ji, Rongrong and Jiang, Cheng and Jiang, Yingjie and Lawin, Felix Järemo and Kang, Ze and Kiran, Madhu and Kittler, Josef and Lai, Simiao and Lan, Xiangyuan and Lee, Dongwook and Lee, Hyunjeong and Lee, Seohyung and Li, Hui and Li, Ming and Li, Wangkai and Li, Xi and Li, Xianxian and Li, Xiao and Li, Zhe and Lin, Liting and Ling, Haibin and Liu, Bo and Liu, Chang and Liu, Si and Lu, Huchuan and Cruz, Rafael M. O. and Ma, Bingpeng and Ma, Chao and Ma, Jie and Ma, Yinchao and Martinel, Niki and Memarmoghadam, Alireza and Micheloni, Christian and Moallem, Payman and Nguyen-Meidine, Le Thanh and Pan, Siyang and Park, ChangBeom and Paudel, Danda and Paul, Matthieu and Peng, Houwen and Robinson, Andreas and Rout, Litu and Shan, Shiguang and Simonato, Kristian and Song, Tianhui and Song, Xiaoning and Sun, Chao and Sun, Jingna and Tang, Zhangyong and Timofte, Radu and Tsai, Chi-Yi and Gool, Luc Van and Verma, Om Prakash and Wang, Dong and Wang, Fei and Wang, Liang and Wang, Liangliang and Wang, Lijun and Wang, Limin and Wang, Qiang and Wu, Gangshan and Wu, Jinlin and Wu, Xiaojun and Xie, Fei and Xu, Tianyang and Xu, Wei and Xu, Yong and Xu, Yuanyou and Xue, Wanli and Xun, Zizheng and Yan, Bin and Yang, Dawei and Yang, Jinyu and Yang, Wankou and Yang, Xiaoyun and Yang, Yi and Yang, Yichun and Yang, Zongxin and Ye, Botao and Yu, Fisher and Yu, Hongyuan and Yu, Jiaqian and Yu, Qianjin and Yu, Weichen and Ze, Kang and Zhai, Jiang and Zhang, Chengwei and Zhang, Chunhu and Zhang, Kaihua and Zhang, Tianzhu and Zhang, Wenkang and Zhang, Zhibin and Zhang, Zhipeng and Zhao, Jie and Zhao, Shaochuan and Zheng, Feng and Zheng, Haixia and Zheng, Min and Zhong, Bineng and Zhu, Jiawen and Zhu, Xuefeng and Zhuang, Yueting},
  booktitle = {Computer Vision -- ECCV 2022 Workshops},
  date      = {2023},
  editor    = {Karlinsky, Leonid and Michaeli, Tomer and Nishino, Ko},
  isbn      = {978-3-031-25085-9},
  location  = {Cham},
  pages     = {431--460},
  publisher = {Springer Nature Switzerland},
  title     = {The Tenth Visual Object Tracking VOT2022 Challenge Results},
}

@inproceedings{Kristan23,
  author    = {Kristan, Matej and Matas, Ji\v{r}{\'\i} and Danelljan, Martin and Felsberg, Michael and Chang, Hyung Jin and Zajc, Luka \v{C}ehovin and Luke\v{z}i\v{c}, Alan and Drbohlav, Ondrej and Zhang, Zhongqun and Tran, Khanh-Tung and Vu, Xuan-Son and Bj\"orklund, Johanna and Mayer, Christoph and Zhang, Yushan and Ke, Lei and Zhao, Jie and Fern\'andez, Gustavo and Al-Shakarji, Noor and An, Dong and Arens, Michael and Becker, Stefan and Bhat, Goutam and Bullinger, Sebastian and Chan, Antoni B. and Chang, Shijie and Chen, Hanyuan and Chen, Xin and Chen, Yan and Chen, Zhenyu and Cheng, Yangming and Cui, Yutao and Deng, Chunyuan and Dong, Jiahua and Dunnhofer, Matteo and Feng, Wei and Fu, Jianlong and Gao, Jie and Han, Ruize and Hao, Zeqi and He, Jun-Yan and He, Keji and He, Zhenyu and Hu, Xiantao and Huang, Kaer and Huang, Yuqing and Jiang, Yi and Kang, Ben and Lan, Jin-Peng and Lee, Hyungjun and Li, Chenyang and Li, Jiahao and Li, Ning and Li, Wangkai and Li, Xiaodi and Li, Xin and Liu, Pengyu and Liu, Yue and Lu, Huchuan and Luo, Bin and Luo, Ping and Ma, Yinchao and Miao, Deshui and Micheloni, Christian and Palaniappan, Kannappan and Park, Hancheol and Paul, Matthieu and Peng, HouWen and Qian, Zekun and Rahmon, Gani and Scherer-Negenborn, Norbert and Shao, Pengcheng and Shin, Wooksu and Kazemi, Elham Soltani and Song, Tianhui and Stiefelhagen, Rainer and Sun, Rui and Tang, Chuanming and Tang, Zhangyong and Toubal, Imad Eddine and Valmadre, Jack and van de Weijer, Joost and Van Gool, Luc and Vira, Jash and Vujasinovi\'c, St\`ephane and Wan, Cheng and Wan, Jia and Wang, Dong and Wang, Fei and Wang, Feifan and Wang, He and Wang, Limin and Wang, Song and Wang, Yaowei and Wang, Zhepeng and Wu, Gangshan and Wu, Jiannan and Wu, Qiangqiang and Wu, Xiaojun and Xiao, Anqi and Xie, Jinxia and Xu, Chenlong and Xu, Min and Xu, Tianyang and Xu, Yuanyou and Yan, Bin and Yang, Dawei and Yang, Ming-Hsuan and Yang, Tianyu and Yang, Yi and Yang, Zongxin and Yin, Xuanwu and Yu, Fisher and Yu, Hongyuan and Yu, Qianjin and Yu, Weichen and Yuan, YongSheng and Yuan, Zehuan and Zhang, Jianlin and Zhang, Lu and Zhang, Tianzhu and Zhao, Guodongfang and Zhao, Shaochuan and Zheng, Yaozong and Zhong, Bineng and Zhu, Jiawen and Zhu, Xuefeng and Zhuang, Yueting and Zong, ChengAo and Zuo, Kunlong},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
  date      = {2023-10},
  pages     = {1796-1818},
  title     = {The First Visual Object Tracking Segmentation VOTS2023 Challenge Results},
}

@article{Krizhevsky12,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  journal   = {Advances in neural information processing systems},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012},
}

@article{Kuhn55,
  author    = {Kuhn, Harold W},
  date      = {1955},
  journal   = {Naval research logistics quarterly},
  number    = {1-2},
  pages     = {83--97},
  publisher = {Wiley Online Library},
  title     = {The Hungarian method for the assignment problem},
  volume    = {2},
}

@inproceedings{Kuo10,
  abstract  = {We propose a novel system for associating multi-target tracks across multiple non-overlapping cameras by an on-line learned discriminative appearance affinity model. Collecting reliable training samples is a major challenge in on-line learning since supervised correspondence is not available at runtime. To alleviate the inevitable ambiguities in these samples, Multiple Instance Learning (MIL) is applied to learn an appearance affinity model which effectively combines three complementary image descriptors and their corresponding similarity measurements. Based on the spatial-temporal information and the proposed appearance affinity model, we present an improved inter-camera track association framework to solve the ``target handover'' problem across cameras. Our evaluations indicate that our method have higher discrimination between different targets than previous methods.},
  author    = {Kuo, Cheng-Hao and Huang, Chang and Nevatia, Ram},
  booktitle = {Computer Vision -- ECCV 2010},
  date      = {2010},
  editor    = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
  isbn      = {978-3-642-15549-9},
  location  = {Berlin, Heidelberg},
  pages     = {383--396},
  publisher = {Springer Berlin Heidelberg},
  title     = {Inter-camera Association of Multi-target Tracks by On-Line Learned Appearance Affinity Models},
}

@inproceedings{Layne17,
  abstract  = {Video surveillance systems are now widely deployed to improve our lives by enhancing safety, security, health monitoring and business intelligence. This has motivated extensive research into automated video analysis. Nevertheless, there is a gap between the focus of contemporary research, and the needs of end users of video surveillance systems. Many existing benchmarks and methodologies focus on narrowly defined problems in detection, tracking, re-identification or recognition. In contrast, end users face higher-level problems such as long-term monitoring of identities in order to build a picture of a person's activity across the course of a day, producing usage statistics of a particular area of space, and that these capabilities should be robust to challenges such as change of clothing. To achieve this effectively requires less widely studied capabilities such as spatio-temporal reasoning about people identities and locations within a space partially observed by multiple cameras over an extended time period. To bridge this gap between research and required capabilities, we propose a new dataset LIMA that encompasses the challenges of monitoring a typical home / office environment. LIMA contains 4.5 hours of RGB-D video from three cameras monitoring a four room house. To reflect the challenges of a realistic practical application, the dataset includes clothes changes and visitors to ensure the global reasoning is a realistic open-set problem. In addition to raw data, we provide identity annotation for benchmarking, and tracking results from a contemporary RGB-D tracker - thus allowing focus on the higher level monitoring problems.},
  author    = {Layne, Ryan and Hannuna, Sion and Camplani, Massimo and Hall, Jake and Hospedales, Timothy M. and Xiang, Tao and Mirmehdi, Majid and Damen, Dima},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2017-07},
  doi       = {10.1109/CVPRW.2017.189},
  issn      = {2160-7516},
  pages     = {1462--1470},
  title     = {A Dataset for Persistent Multi-target Multi-camera Tracking in RGB-D},
}

@article{Lecun98,
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  author   = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date     = {1998-11},
  doi      = {10.1109/5.726791},
  issn     = {1558-2256},
  journal  = {Proceedings of the IEEE},
  keywords = {},
  number   = {11},
  pages    = {2278-2324},
  title    = {Gradient-based learning applied to document recognition},
  volume   = {86},
}

@article{Lee18,
  abstract     = {Due to the expanding scale of camera networks, Multiple Camera Tracking (MCT) of humans has received increased attention in recent years. In this paper, we present a novel approach to tracking each human within a single camera and across multiple disjoint cameras. Our framework includes a multi-object tracking and segmentation system, a two-phase feature extractor, and an online-learning-based camera link model estimation. For tracking within a single camera, we apply tracking by segmentation and local object detection with multi-kernel feedback to adaptively improve the robustness of the algorithm. In inter-camera tracking, we introduce an effective integration of appearance and context features. Couples are automatically detected, and the couple feature is also integrated with existing features. The proposed algorithm is scalable by a fully unsupervised online-learning framework. In our experiments, the proposed method outperforms all of the state-of-the-art methods in the benchmark National Laboratory of Pattern Recognition (NLPR)_MCT dataset.},
  author       = {Lee, Young-Gun and Tang, Zheng and Hwang, Jenq-Neng},
  date         = {2018-10},
  doi          = {10.1109/TCSVT.2017.2707399},
  issn         = {1558-2205},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  number       = {10},
  pages        = {2870--2883},
  title        = {Online-Learning-Based Human Tracking Across Non-Overlapping Cameras},
  volume       = {28},
}

@inproceedings{Li19,
  abstract  = {Multi-target Multi-camera Tracking (MTMCT) aims to extract the trajectories from videos captured by a set of cameras. Recently, the tracking performance of MTMCT is significantly enhanced with the employment of re-identification (Re-ID) model. However, the appearance feature usually becomes unreliable due to the occlusion and orientation variance of the targets. Directly applying Re-ID model in MTMCT will encounter the problem of identity switches (IDS) and tracklet fragment caused by occlusion. To solve these problems, we propose a novel tracking framework in this paper. In this framework, the occlusion status and orientation information are utilized in Re-ID model with human pose information considered. In addition, the tracklet association using the proposed fused tracking feature is adopted to handle the fragment problem. The proposed tracker achieves 81.3{\%} IDF1 on the multiple-camera hard sequence, which outperforms all other reference methods by a large margin.},
  author    = {Li, Peng and Zhang, Jiabin and Zhu, Zheng and Li, Yanwei and Jiang, Lu and Huang, Guan},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2019-06},
  doi       = {10.1109/CVPRW.2019.00192},
  issn      = {2160-7516},
  pages     = {1506--1516},
  title     = {State-Aware Re-Identification Feature for Multi-Target Multi-Camera Tracking},
}

@article{Li20,
  author     = {Wei Li and Yuanjun Xiong and Shuo Yang and Siqi Deng and Wei Xia},
  eprint     = {2010.16031},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Mon, 15 May 2023 16:24:40 +0200},
  title      = {{SMOT:} Single-Shot Multi Object Tracking},
  url        = {https://arxiv.org/abs/2010.16031},
  volume     = {abs/2010.16031},
  year       = {2020},
}

@inproceedings{Li22a,
  abstract  = {Multi-Target Multi-Camera tracking is a fundamental task for intelligent traffic systems. The track 1 of AI City Challenge 2022 aims at the city-scale multi-camera vehicle tracking task. In this paper we propose an accurate vehicle tracking system composed of 4 parts, including: (1) State-of-the-art detection and re-identification models for vehicle detection and feature extraction. (2) Single camera tracking, where we introduce augmented tracks prediction and multi-level association method on top of tracking-by-detection paradigm.(3) Zone-based singe-camera track-let merging strategy. (4) Multi-camera spatial-temporal matching and clustering strategy. The proposed system achieves promising results and ranks the second place in Track 1 of the AI City Challenge 2022 with a IDF1 score of 0.8437.},
  author    = {Li, Fei and Wang, Zhen and Nie, Ding and Zhang, Shiyi and Jiang, Xingqun and Zhao, Xingxing and Hu, Peng},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2022-06},
  doi       = {10.1109/CVPRW56347.2022.00369},
  issn      = {2160-7516},
  pages     = {3264--3272},
  title     = {Multi-Camera Vehicle Tracking System for AI City Challenge 2022},
}

@inproceedings{Li22b,
  abstract  = {In order to solve the problem of multi-target multi-camera tracking, the method of detection and tracking is used to complete single-camera multi-target tracking, and a person re-identification model using mutual information time weight aggregation is proposed to extract appearance features, and determine whether the camera is in the camera. A new method for cross-camera multi-target tracking is finally performed. We use the deep learning framework Pytorch to build network models and tune model-related parameters and compare experiments with existing models on the dataset. The results show that the model has better tracking performance.},
  author    = {Li, Jiayue and Piao, Yan},
  booktitle = {2022 IEEE 5th International Conference on Electronic Information and Communication Technology (ICEICT)},
  date      = {2022-08},
  doi       = {10.1109/ICEICT55736.2022.9908659},
  pages     = {149--151},
  title     = {Multi-Target Multi-Camera Tracking Based On Mutual Information-Temporal Weight Aggregation Person Re-Identification},
}

@article{Lian23,
  archiveprefix = {arXiv},
  author        = {Qing Lian and Tai Wang and Dahua Lin and Jiangmiao Pang},
  date          = {2023},
  eprint        = {2303.16628},
  primaryclass  = {cs.CV},
  title         = {DORT: Modeling Dynamic Objects in Recurrent for Multi-Camera 3D Object Detection and Tracking},
}

@article{Lin14,
  author     = {Tsung{-}Yi Lin and Michael Maire and Serge J. Belongie and Lubomir D. Bourdev and Ross B. Girshick and James Hays and Pietro Perona and Deva Ramanan and Piotr Doll{\'{a}}r and C. Lawrence Zitnick},
  date       = {2014},
  eprint     = {1405.0312},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Mon, 13 Aug 2018 16:48:13 +0200},
  title      = {Microsoft {COCO:} Common Objects in Context},
  url        = {http://arxiv.org/abs/1405.0312},
  volume     = {abs/1405.0312},
}

@inproceedings{Lin17,
  abstract  = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  author    = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2017-07},
  doi       = {10.1109/CVPR.2017.106},
  issn      = {1063-6919},
  keywords  = {},
  number    = {},
  pages     = {936-944},
  title     = {Feature Pyramid Networks for Object Detection},
  volume    = {},
}

@article{Liu15,
  author     = {Wei Liu and Dragomir Anguelov and Dumitru Erhan and Christian Szegedy and Scott E. Reed and Cheng{-}Yang Fu and Alexander C. Berg},
  date       = {2015},
  eprint     = {1512.02325},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Wed, 12 Feb 2020 08:32:49 +0100},
  title      = {{SSD:} Single Shot MultiBox Detector},
  url        = {http://arxiv.org/abs/1512.02325},
  volume     = {abs/1512.02325},
}

@article{Liu17,
  author       = {Liu, Wenqian and Camps, Octavia and Sznaier, Mario},
  date         = {2017},
  doi          = {10.48550/arXiv.1709.07065},
  journaltitle = {arXiv preprint arXiv:1709.07065},
  title        = {Multi-camera multi-object tracking},
}

@article{Lowe04,
  abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
  author   = {David G. Lowe},
  date     = {2004},
  doi      = {10.1023/B:VISI.0000029664.99615.94},
  issn     = {1573-1405},
  journal  = {International Journal of Computer Vision},
  month    = {11},
  number   = {2},
  pages    = {91-110},
  title    = {Distinctive Image Features from Scale-Invariant Keypoints},
  url      = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
  volume   = {60},
}

@inproceedings{Lui21,
  abstract  = {Multi-Target Multi-Camera Tracking has a wide range of applications and is the basis for many advanced inferences and predictions. This paper describes our solution to the Track 3 multi-camera vehicle tracking task in 2021 AI City Challenge (AICITY21). This paper proposes a multi-target multi-camera vehicle tracking framework guided by the crossroad zones. The framework includes: (1) Use mature detection and vehicle re-identification models to extract targets and appearance features. (2) Use modified JDE-Tracker (without detection module) to track single-camera vehicles and generate single-camera tracklets. (3) According to the characteristics of the crossroad, the Tracklet Filter Strategy and the Direction Based Temporal Mask are proposed. (4) Propose Sub-clustering in Adjacent Cameras for multi-camera tracklets matching. Through the above techniques, our method obtained an IDF1 score of 0.8095, ranking first on the leaderboard1. The code will be released later.},
  author    = {Liu, Chong and Zhang, Yuqi and Luo, Hao and Tang, Jiasheng and Chen, Weihua and Xu, Xianzhe and Wang, Fan and Li, Hao and Shen, Yi-Dong},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2021-06},
  doi       = {10.1109/CVPRW53098.2021.00466},
  issn      = {2160-7516},
  pages     = {4124--4132},
  title     = {City-Scale Multi-Camera Vehicle Tracking Guided by Crossroad Zones},
}

@article{Luiten20,
  author     = {Jonathon Luiten and Aljosa Osep and Patrick Dendorfer and Philip H. S. Torr and Andreas Geiger and Laura Leal{-}Taix{\'{e}} and Bastian Leibe},
  eprint     = {2009.07736},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Fri, 20 Nov 2020 13:22:19 +0100},
  title      = {{HOTA:} {A} Higher Order Metric for Evaluating Multi-Object Tracking},
  url        = {https://arxiv.org/abs/2009.07736},
  volume     = {abs/2009.07736},
  year       = {2020},
}

@article{Meyer17,
  abstract     = {We propose an algorithm for tracking an unknown number of targets based on measurements provided by multiple sensors. Our algorithm achieves low computational complexity and excellent scalability by running belief propagation on a suitably devised factor graph. A redundant formulation of data association uncertainty and the use of “augmented target states” including binary target indicators make it possible to exploit statistical independencies for a drastic reduction of complexity. An increase in the number of targets, sensors, or measurements leads to additional variable nodes in the factor graph but not to higher dimensions of the messages. As a consequence, the complexity of our method scales only quadratically in the number of targets, linearly in the number of sensors, and linearly in the number of measurements per sensor. The performance of the method compares well with that of previously proposed methods, including methods with a less favorable scaling behavior. In particular, our method can outperform multisensor versions of the probability hypothesis density (PHD) filter, the cardinalized PHD filter, and the multi-Bernoulli filter.},
  author       = {Meyer, Florian and Braca, Paolo and Willett, Peter and Hlawatsch, Franz},
  date         = {2017-07},
  doi          = {10.1109/TSP.2017.2688966},
  issn         = {1941-0476},
  journaltitle = {IEEE Transactions on Signal Processing},
  number       = {13},
  pages        = {3478--3493},
  title        = {A Scalable Algorithm for Tracking an Unknown Number of Targets Using Multiple Sensors},
  volume       = {65},
}

@article{Milan16a,
  author     = {Milan, A. and Leal-Taix\'{e}, L. and Reid, I. and Roth, S. and Schindler, K.},
  date       = {2016-03},
  journal    = {arXiv:1603.00831 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  note       = {arXiv: 1603.00831},
  shorttitle = {MOT16},
  title      = {{MOT}16: {A} Benchmark for Multi-Object Tracking},
  url        = {http://arxiv.org/abs/1603.00831},
}

@article{Milan16b,
  author     = {Anton Milan and Seyed Hamid Rezatofighi and Anthony R. Dick and Konrad Schindler and Ian D. Reid},
  eprint     = {1604.03635},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Mon, 13 Aug 2018 16:47:54 +0200},
  title      = {Online Multi-target Tracking using Recurrent Neural Networks},
  url        = {http://arxiv.org/abs/1604.03635},
  volume     = {abs/1604.03635},
  year       = {2016},
}

@inproceedings{Naphade23,
  author    = {Milind Naphade and Shuo Wang and David C. Anastasiu and Zheng Tang and Ming-Ching Chang and Yue Yao and Liang Zheng and Mohammed Shaiqur Rahman and Meenakshi S. Arya and Anuj Sharma and Qi Feng and Vitaly Ablavsky and Stan Sclaroff and Pranamesh Chakraborty and Sanjita Prajapati and Alice Li and Shangru Li and Krishna Kunadharaju and Shenxin Jiang and Rama Chellappa},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  date      = {2023-06},
  title     = {The 7th AI City Challenge},
}

@inproceedings{Nguyen22a,
  abstract  = {Multi-Camera Multi-Object Tracking is currently drawing attention in the computer vision field due to its superior performance in real-world applications such as video surveillance with crowded scenes or in wide spaces. In this work, we propose a mathematically elegant multi-camera multiple object tracking approach based on a spatial-temporal lifted multicut formulation. Our model utilizes state-of-the-art tracklets produced by single-camera trackers as proposals. As these tracklets may contain ID-Switch errors, we refine them through a novel pre-clustering obtained from 3D geometry projections. As a result, we derive a better tracking graph without ID switches and more precise affinity costs for the data association phase. Tracklets are then matched to multi-camera trajectories by solving a global lifted multicut formulation that incorporates short and long-range temporal interactions on tracklets located in the same camera as well as inter-camera ones. Experimental results on the WildTrack dataset yield near-perfect performance, outperforming state-of-the-art trackers on Campus while being on par on the PETS-09 dataset. We will release our implementations at this link https://github.com/nhmduy/LMGP.},
  author    = {Nguyen, Duy M. H. and Henschel, Roberto and Rosenhahn, Bodo and Sonntag, Daniel and Swoboda, Paul},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2022-06},
  doi       = {10.1109/CVPR52688.2022.00866},
  issn      = {2575-7075},
  pages     = {8856--8865},
  title     = {LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking},
}

@article{Nguyen22b,
  author      = {Nguyen, Pha and Quach, Kha Gia and Duong, Chi Nhan and Phung, Son Lam and Le, Ngan and Luu, Khoa},
  date        = {2022},
  eprint      = {2211.09663},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  title       = {Multi-Camera Multi-Object Tracking on the Move via Single-Stage Global Association Approach},
}

@inproceedings{Nummiaro03,
  abstract  = {This paper presents a multi-view tracker, meant to operate in smart rooms that are equipped with multiple cameras. The cameras are assumed to be calibrated. In particular, we demonstrate a virtual classroom application, where the system automatically selects the camera with the 'best' view on the face of a person moving in the room. Real-time object tracking, which is needed to achieve this, is implemented by means of color-based particle filtering. The use of multiple model histograms for the target (human head) results robust tracking, even when the view on the target changes considerably like from the front to the back. Information is shared between the cameras, which adds robustness to the system. Once one camera has lost the target, it can be reinitialized with the help of the epipolar constraints suggested by the others. Experiments in our research environment corroborate the effectiveness of the approach.},
  author    = {Nummiaro, Katja and Koller-Meier, Esther and Svoboda, Tomš and Roth, Daniel and Van Gool, Luc},
  booktitle = {Pattern Recognition},
  date      = {2003},
  editor    = {Michaelis, Bernd and Krell, Gerald},
  isbn      = {978-3-540-45243-0},
  location  = {Berlin, Heidelberg},
  pages     = {591--599},
  publisher = {Springer Berlin Heidelberg},
  title     = {Color-Based Object Tracking in Multi-camera Environments},
}

@inproceedings{Orazio09,
  abstract  = {People Tracking in multiple cameras is of great interest for wide area video surveillance systems. Multi-camera tracking with non-overlapping fields of view (FOV) involves the tracking of people in the blind region and their correspondence matching across cameras. We consider these problems in this paper. We propose a multi camera architecture for wide area surveillance and a real time people tracking algorithm across non overlapping cameras. We compared different methods to evaluate the color Brightness Transfer Function (BTF) between non overlapping cameras. These approaches are based on a testing phase during which the color histogram mapping, between pairs of images of the same object observed in the different field of views, is carried out. The experimental results compare two different transfer functions and demonstrate their limits in people association when a new person enters in one camera FOV.},
  author    = {D'Orazio, T. and Mazzeo, P.L. and Spagnolo, P.},
  booktitle = {2009 Third ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC)},
  date      = {2009-08},
  doi       = {10.1109/ICDSC.2009.5289365},
  pages     = {1--6},
  title     = {Color Brightness Transfer Function evaluation for non overlapping multi camera tracking},
}

@incollection{Pearl88,
  address   = {San Francisco (CA)},
  author    = {Judea Pearl},
  booktitle = {Probabilistic Reasoning in Intelligent Systems},
  date      = {1988},
  doi       = {https://doi.org/10.1016/B978-0-08-051489-5.50007-2},
  editor    = {Judea Pearl},
  isbn      = {978-0-08-051489-5},
  pages     = {1-141},
  publisher = {Morgan Kaufmann},
  title     = {Probabilistic Reasoning in Intelligent Systems (Chapters 1-3)},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780080514895500072},
}

@inproceedings{Pellegrini09,
  abstract  = {Object tracking typically relies on a dynamic model to predict the object's location from its past trajectory. In crowded scenarios a strong dynamic model is particularly important, because more accurate predictions allow for smaller search regions, which greatly simplifies data association. Traditional dynamic models predict the location for each target solely based on its own history, without taking into account the remaining scene objects. Collisions are resolved only when they happen. Such an approach ignores important aspects of human behavior: people are driven by their future destination, take into account their environment, anticipate collisions, and adjust their trajectories at an early stage in order to avoid them. In this work, we introduce a model of dynamic social behavior, inspired by models developed for crowd simulation. The model is trained with videos recorded from birds-eye view at busy locations, and applied as a motion model for multi-people tracking from a vehicle-mounted camera. Experiments on real sequences show that accounting for social interactions and scene knowledge improves tracking performance, especially during occlusions.},
  author    = {Pellegrini, S. and Ess, A. and Schindler, K. and van Gool, L.},
  booktitle = {2009 IEEE 12th International Conference on Computer Vision},
  date      = {2009-09},
  doi       = {10.1109/ICCV.2009.5459260},
  issn      = {2380-7504},
  pages     = {261--268},
  title     = {You'll never walk alone: Modeling social behavior for multi-target tracking},
}

@inproceedings{Peng16,
  abstract  = {Most existing person re-identification (Re-ID) approaches follow a supervised learning framework, in which a large number of labelled matching pairs are required for training. This severely limits their scalability in realworld applications. To overcome this limitation, we develop a novel cross-dataset transfer learning approach to learn a discriminative representation. It is unsupervised in the sense that the target dataset is completely unlabelled. Specifically, we present an multi-task dictionary learning method which is able to learn a dataset-shared but target-data-biased representation. Experimental results on five benchmark datasets demonstrate that the method significantly outperforms the state-of-the-art.},
  author    = {Peng, Peixi and Xiang, Tao and Wang, Yaowei and Pontil, Massimiliano and Gong, Shaogang and Huang, Tiejun and Tian, Yonghong},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2016-06},
  doi       = {10.1109/CVPR.2016.146},
  issn      = {1063-6919},
  pages     = {1306--1315},
  title     = {Unsupervised Cross-Dataset Transfer Learning for Person Re-identification},
}

@inproceedings{Qian20,
  abstract  = {City-scale multi-camera vehicle tracking is an important task in the intelligent city and traffic management. It is quite challenging with large scale variance, frequent occlusion and appearance variance caused by viewing perspective difference. In this paper, we propose ELECTRICITY, an efficient multi-camera vehicle tracking system with aggregation loss and fast multi-target cross-camera tracking strategy. The proposed system contains four main modules. Firstly, we extract tracklets under single camera view through object detection and multi-object tracking modules which shared the detection features. After that, we match the generated tracklets through a multicamera re-identification module. Finally, we eliminate isolated tracklets and synchronize tracking ids according to the re-identification results. The proposed system wins the first place in the City-Scale Multi-Camera Vehicle Tracking of AI City 2020 Challenge (Track 3)1 with a score of 0.4585.},
  author    = {Qian, Yijun and Yu, Lijun and Liu, Wenhe and Hauptmann, Alexander G.},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2020-06},
  doi       = {10.1109/CVPRW50498.2020.00302},
  issn      = {2160-7516},
  pages     = {2511--2519},
  title     = {ELECTRICITY: An Efficient Multi-camera Vehicle Tracking System for Intelligent City},
}

@inproceedings{Quach21,
  abstract  = {Multi-Camera Multiple Object Tracking (MC-MOT) is a significant computer vision problem due to its emerging applicability in several real-world applications. Despite a large number of existing works, solving the data association problem in any MC-MOT pipeline is arguably one of the most challenging tasks. Developing a robust MC-MOT system, however, is still highly challenging due to many practical issues such as inconsistent lighting conditions, varying object movement patterns, or the trajectory occlusions of the objects between the cameras. To address these problems, this work, therefore, proposes a new Dynamic Graph Model with Link Prediction (DyGLIP) approach 1 to solve the data association task. Compared to existing methods, our new model offers several advantages, including better feature representations and the ability to recover from lost tracks during camera transitions. Moreover, our model works gracefully regardless of the overlapping ratios between the cameras. Experimental results show that we out-perform existing MC-MOT algorithms by a large margin on several practical datasets. Notably, our model works favor-ably on online settings but can be extended to an incremental approach for large-scale datasets.},
  author    = {Quach, Kha Gia and Nguyen, Pha and Le, Huu and Truong, Thanh-Dat and Duong, Chi Nhan and Tran, Minh-Triet and Luu, Khoa},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2021-06},
  doi       = {10.1109/CVPR46437.2021.01357},
  issn      = {2575-7075},
  pages     = {13779--13788},
  title     = {DyGLIP: A Dynamic Graph Model with Link Prediction for Accurate Multi-Camera Multiple Object Tracking},
}

@article{Redmon15,
  author     = {Joseph Redmon and Santosh Kumar Divvala and Ross B. Girshick and Ali Farhadi},
  date       = {2015},
  eprint     = {1506.02640},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Mon, 13 Aug 2018 16:48:08 +0200},
  title      = {You Only Look Once: Unified, Real-Time Object Detection},
  url        = {http://arxiv.org/abs/1506.02640},
  volume     = {abs/1506.02640},
}

@article{Reid79,
  abstract = {An algorithm for tracking multiple targets in a cluttered enviroment is developed. The algorithm is capable of initiating tracks, accounting for false or missing reports, and processing sets of dependent reports. As each measurement is received, probabilities are calculated for the hypotheses that the measurement came from previously known targets in a target file, or from a new target, or that the measurement is false. Target states are estimated from each such data-association hypothesis using a Kalman filter. As more measurements are received, the probabilities of joint hypotheses are calculated recursively using all available information such as density of unknown targets, density of false targets, probability of detection, and location uncertainty. This branching technique allows correlation of a measurement with its source based on subsequent, as well as previous, data. To keep the number of hypotheses reasonable, unlikely hypotheses are eliminated and hypotheses with similar target estimates are combined. To minimize computational requirements, the entire set of targets and measurements is divided into clusters that are solved independently. In an illustrative example of aircraft tracking, the algorithm successfully tracks targets over a wide range of conditions.},
  author   = {Reid, D.},
  date     = {1979-12},
  doi      = {10.1109/TAC.1979.1102177},
  issn     = {1558-2523},
  journal  = {IEEE Transactions on Automatic Control},
  keywords = {},
  number   = {6},
  pages    = {843-854},
  title    = {An algorithm for tracking multiple targets},
  volume   = {24},
}

@article{Ren17,
  abstract     = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  author       = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date         = {2017-06},
  doi          = {10.1109/TPAMI.2016.2577031},
  issn         = {1939-3539},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1137--1149},
  title        = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  volume       = {39},
}

@inproceedings{Ristani16,
  author       = {Ristani, Ergys and Solera, Francesco and Zou, Roger S. and Cucchiara, Rita and Tomasi, Carlo},
  booktitle    = {European conference on computer vision},
  date         = {2016},
  eprint       = {1609.01775},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  organization = {Springer},
  pages        = {17--35},
  title        = {Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking},
}

@inproceedings{Ristani18,
  author    = {Ristani, Ergys and Tomasi, Carlo},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  date      = {2018},
  pages     = {6036--6046},
  title     = {Features for multi-target multi-camera tracking and re-identification},
}

@inproceedings{Schroff15,
  abstract  = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63{\%}. On YouTube Faces DB it achieves 95.12{\%}. Our system cuts the error rate in comparison to the best published result [15] by 30{\%} on both datasets.},
  author    = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2015-06},
  doi       = {10.1109/CVPR.2015.7298682},
  issn      = {1063-6919},
  keywords  = {},
  number    = {},
  pages     = {815-823},
  title     = {FaceNet: A unified embedding for face recognition and clustering},
  volume    = {},
}

@article{Song08,
  abstract     = {We address the problem of tracking multiple people in a network of nonoverlapping cameras. This introduces certain challenges that are unique to this particular application scenario, in addition to existing challenges in tracking like pose and illumination variations, occlusion, clutter and sensor noise. For this purpose, we propose a novel multi-objective optimization framework by combining short term feature correspondences across the cameras with long-term feature dependency models. The overall solution strategy involves adapting the similarities between features observed at different cameras based on the long-term models and finding the stochastically optimal path for each person. For modeling the long-term interdependence of the features over space and time, we propose a novel method based on discriminant analysis models. The entire process allows us to adaptively evolve the feature correspondences by observing the system performance over a time window, and correct for errors in the similarity estimations. We show results on data collected by two large camera networks. These experiments prove that incorporation of the long-term models enable us to hold tracks of objects over extended periods of time, including situations where there are large ldquoblindrdquo areas. The proposed approach is implemented by distributing the processing over the entire network.},
  author       = {Song, Bi and Roy-Chowdhury, Amit K.},
  date         = {2008-08},
  doi          = {10.1109/JSTSP.2008.925992},
  issn         = {1941-0484},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  number       = {4},
  pages        = {582--596},
  title        = {Robust Tracking in A Camera Network: A Multi-Objective Optimization Framework},
  volume       = {2},
}

@inproceedings{Specker21,
  abstract  = {Multi-camera tracking of vehicles on a city-scale level is a crucial task for efficient traffic monitoring. Most of the errors made by such multi-target multi-camera tracking systems arise due to tracking failures or misleading visual information of detection boxes under occlusion. Therefore, we propose an occlusion-aware approach that leverages temporal information from tracks to improve the single-camera tracking performance by an occlusion handling strategy and additional modules to filter false detections. For the multi-camera tracking, we discard obstacle-occluded detection boxes by a background filtering technique and boxes overlapping with other targets using the available track information to improve the quality of extracted visual features. Furthermore, topological and temporal constraints are incorporated to simplify the re-identification task in the multi-camera clustering. We give detailed insights into our method with ablative experiments and show its competitiveness on the CityFlowV2 dataset, where we achieve promising results ranking 4th in Track 3 of the 2021 AI City Challenge.},
  author    = {Specker, Andreas and Stadler, Daniel and Florin, Lucas and Beyerer, Jürgen},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2021-06},
  doi       = {10.1109/CVPRW53098.2021.00471},
  issn      = {2160-7516},
  pages     = {4168--4177},
  title     = {An Occlusion-aware Multi-target Multi-camera Tracking System},
}

@inproceedings{Specker22,
  abstract  = {Multi-camera tracking of vehicles on a city-wide level is a core component of modern traffic monitoring systems. For this task, single-camera tracking failures are the most common causes of errors concerning automatic multi-target multi-camera tracking systems. To address these problems, we propose several modules that aim at improving single-camera tracklets, e.g., appearance-based tracklet splitting, single-camera clustering, and track completion. After these track refinement steps, hierarchical clustering is used to associate the enhanced single-camera tracklets. During this stage, we leverage vehicle re-identification features as well as prior knowledge about the scene's topology. Last, the proposed track completion strategy is adopted for the cross-camera association task to obtain the final multi-camera tracks. Our method proves itself competitive: With it, we achieved 4th place in track 1 of the 2022 AI City Challenge.},
  author    = {Specker, Andreas and Florin, Lucas and Cormier, Mickael and Beyerer, Jürgen},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2022-06},
  doi       = {10.1109/CVPRW56347.2022.00361},
  issn      = {2160-7516},
  pages     = {3198--3208},
  title     = {Improving Multi-Target Multi-Camera Tracking by Track Refinement and Completion},
}

@article{Taix15,
  author     = {Leal-Taix\'{e}, L. and Milan, A. and Reid, I. and Roth, S. and Schindler, K.},
  date       = {2015-04},
  journal    = {arXiv:1504.01942 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  note       = {arXiv: 1504.01942},
  shorttitle = {MOTChallenge 2015},
  title      = {{MOTC}hallenge 2015: {T}owards a Benchmark for Multi-Target Tracking},
  url        = {http://arxiv.org/abs/1504.01942},
}

@inproceedings{Tang19,
  abstract  = {Urban traffic optimization using traffic cameras as sensors is driving the need to advance state-of-the-art multi-target multi-camera (MTMC) tracking. This work introduces CityFlow, a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID). We conducted an extensive experimental evaluation of baselines/state-of-the-art approaches in MTMC tracking, multi-target single-camera (MTSC) tracking, object detection, and image-based ReID on this dataset, analyzing the impact of different network architectures, loss functions, spatio-temporal models and their combinations on task effectiveness. An evaluation server is launched with the release of our benchmark at the 2019 AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to compare the performance of their newest techniques. We expect this dataset to catalyze research in this field, propel the state-of-the-art forward, and lead to deployed traffic optimization(s) in the real world.},
  author    = {Tang, Zheng and Naphade, Milind and Liu, Ming-Yu and Yang, Xiaodong and Birchfield, Stan and Wang, Shuo and Kumar, Ratnesh and Anastasiu, David and Hwang, Jenq-Neng},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2019-06},
  doi       = {10.1109/CVPR.2019.00900},
  issn      = {2575-7075},
  pages     = {8789--8798},
  title     = {CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification},
}

@article{Tao16,
  author     = {Ran Tao and Efstratios Gavves and Arnold W. M. Smeulders},
  date       = {2016},
  eprint     = {1605.05863},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Tue, 15 Oct 2019 08:23:29 +0200},
  title      = {Siamese Instance Search for Tracking},
  url        = {http://arxiv.org/abs/1605.05863},
  volume     = {abs/1605.05863},
}

@misc{Teepe23,
  archiveprefix = {arXiv},
  author        = {Torben Teepe and Philipp Wolters and Johannes Gilg and Fabian Herzog and Gerhard Rigoll},
  eprint        = {2310.13350},
  primaryclass  = {cs.CV},
  title         = {EarlyBird: Early-Fusion for Multi-View Tracking in the Bird's Eye View},
  year          = {2023},
}

@article{Tesfaye17,
  author      = {Tesfaye, Yonatan Tariku and Zemene, Eyasu and Prati, Andrea and Pelillo, Marcello and Shah, Mubarak},
  date        = {2017},
  eprint      = {1706.06196},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  title       = {Multi-Target Tracking in Multiple Non-Overlapping Cameras using Constrained Dominant Sets},
}

@article{Tesfaye19,
  abstract     = {In this paper, a unified three-layer hierarchical approach for solving tracking problem in a multiple non-overlapping cameras setting is proposed. Given a video and a set of detections (obtained by any person detector), we first solve within-camera tracking employing the first two layers of our framework and then, in the third layer, we solve across-camera tracking by associating tracks of the same person in all cameras simultaneously. To best serve our purpose, we propose fast-constrained dominant set clustering (FCDSC), a novel method which is several orders of magnitude faster (close to real time) than existing methods. FCDSC is a parameterized family of quadratic programs that generalizes the standard quadratic optimization problem. In our method, we first build a graph where nodes of the graph represent short-tracklets, tracklets and tracks in the first, second and third layer of the framework, respectively. The edge weights reflect the similarity between nodes. FCDSC takes as input a constrained set, a subset of nodes from the graph which need to be included in the extracted cluster. Given a constrained set, FCDSC generates compact clusters by selecting nodes from the graph which are highly similar to each other and with elements in the constrained set. We have tested this approach on a very large and challenging dataset (namely, MOTchallenge DukeMTMC) and show that the proposed framework outperforms the state-of-the-art approaches. Even though the main focus of this paper is on multi-target tracking in non-overlapping cameras, the proposed approach can also be applied to solve video-based person re-identification problem. We show that when the re-identification problem is formulated as a clustering problem, FCDSC can be used in conjunction with state-of-the-art video-based re-identification algorithms, to increase their already good performances. Our experiments demonstrate the general applicability of the proposed framework for multi-target multi-camera tracking and person re-identification tasks.},
  author       = {Tesfaye, Yonatan Tariku and Zemene, Eyasu and Prati, Andrea and Pelillo, Marcello and Shah, Mubarak},
  date         = {2019-09},
  journaltitle = {International Journal of Computer Vision},
  number       = {9},
  pages        = {1303--1320},
  title        = {Multi-target Tracking in Multiple Non-overlapping Cameras Using {Fast-Constrained} Dominant Sets},
  volume       = {127},
}

@thesis{Tian19,
  abstract    = {This work proposes novel approaches for object tracking in challenging scenarios like severe occlusion, deteriorated vision and long range multi-object reidentiﬁcation. All these solutions are only based on image sequence captured by a monocular camera and do not require additional sensors. Experiments on standard benchmarks demonstrate an improved state-of-the-art performance of these approaches. Since all the presented approaches are smartly designed, they can run at a real-time speed.},
  author      = {Tian, Wei},
  date        = {2019},
  doi         = {10.5445/KSP/1000091919},
  institution = {Karlsruher Institut für Technologie (KIT)},
  isbn        = {978-3-7315-0915-8},
  issn        = {1613-4214},
  keywords    = {Verfolgung,Verdeckung,geringe Beleuchtung,mehrere Objeke,monokulare Kamera,Tracking,Occlusion,low illumination,multiple objects,monocular camera},
  language    = {english},
  pagetotal   = {146},
  publisher   = {KIT Scientific Publishing},
  series      = {Schriftenreihe / Institut für Mess- und Regelungstechnik, Karlsruher Institut für Technologie},
  title       = {Novel Aggregated Solutions for Robust Visual Tracking in Trafﬁc Scenarios},
  type        = {phdthesis},
  volume      = {044},
}

@inproceedings{Varga15,
  abstract  = {Combining multiple observation views has proven beneficial for pedestrian tracking. In this paper, we present a methodology for tracking pedestrians in an uncalibrated multi-view camera network. Using a set of color and infrared cameras, we can accurately tracking pedestrians for a general scene configuration. We design an algorithmic framework that can be generalized to an arbitrary number of cameras. A novel pedestrian detection algorithm based on Center-symmetric Local Binary Patterns is integrated into the proposed system. In our experiments the common field of view of two neighboring cameras was about 30{\%}. The system improves upon existing systems in the following ways: (1) The system registers partially overlapping camera-views automatically and does not require any manual input. (2) The system reaches the state-of-the-art performance when the common field of view of any two cameras is low and successfully integrates optical and infrared cameras. Our experiments also demonstrate that the proposed architecture is able to provide robust, real-time input to a video surveillance system. Our system was tested in a multi-view, outdoor environment with uncalibrated cameras.},
  author    = {Varga, Domonkos and Szirányi, Tamás and Kiss, Attila and Spórás, László and Havasi, László},
  booktitle = {2015 IEEE International Conference on Computer Vision Workshop (ICCVW)},
  date      = {2015-12},
  doi       = {10.1109/ICCVW.2015.33},
  pages     = {184--191},
  title     = {A Multi-View Pedestrian Tracking Method in an Uncalibrated Camera Network},
}

@article{Varior16,
  author     = {Rahul Rama Varior and Mrinal Haloi and Gang Wang},
  date       = {2016},
  eprint     = {1607.08378},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Mon, 13 Aug 2018 16:48:57 +0200},
  title      = {Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification},
  url        = {http://arxiv.org/abs/1607.08378},
  volume     = {abs/1607.08378},
}

@article{Voigtlaender19,
  author     = {Paul Voigtlaender and Michael Krause and Aljosa Osep and Jonathon Luiten and Berin Balachandar Gnana Sekar and Andreas Geiger and Bastian Leibe},
  date       = {2019},
  journal    = {arXiv:1902.03604[cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  note       = {arXiv: 1902.03604},
  shorttitle = {MOTS20},
  title      = {MOTS: Multi-Object Tracking and Segmentation},
  url        = {http://arxiv.org/abs/1902.03604},
}

@inproceedings{Wang20a,
  abstract  = {Modern multiple object tracking (MOT) systems usually follow the tracking-by-detection paradigm. It has 1) a detection model for target localization and 2) an appearance embedding model for data association. Having the two models separately executed might lead to efficiency problems, as the running time is simply a sum of the two steps without investigating potential structures that can be shared between them. Existing research efforts on real-time MOT usually focus on the association step, so they are essentially real-time association methods but not real-time MOT system. In this paper, we propose an MOT system that allows target detection and appearance embedding to be learned in a shared model. Specifically, we incorporate the appearance embedding model into a single-shot detector, such that the model can simultaneously output detections and the corresponding embeddings. We further propose a simple and fast association method that works in conjunction with the joint model. In both components the computation cost is significantly reduced compared with former MOT systems, resulting in a neat and fast baseline for future follow-ups on real-time MOT algorithm design. To our knowledge, this work reports the first (near) real-time MOT system, with a running speed of 22 to 40 FPS depending on the input resolution. Meanwhile, its tracking accuracy is comparable to the state-of-the-art trackers embodying separate detection and embedding (SDE) learning ({\$}{\$}64.4{\backslash}{\%}{\$}{\$}64.4{\%}MOTA v.s. {\$}{\$}66.1{\backslash}{\%}{\$}{\$}66.1{\%}MOTA on MOT-16 challenge). Code and models are available at https://github.com/Zhongdao/Towards-Realtime-MOT.},
  author    = {Wang, Zhongdao and Zheng, Liang and Liu, Yixuan and Li, Yali and Wang, Shengjin},
  booktitle = {Computer Vision -- ECCV 2020},
  date      = {2020},
  editor    = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  isbn      = {978-3-030-58621-8},
  location  = {Cham},
  pages     = {107--122},
  publisher = {Springer International Publishing},
  title     = {Towards Real-Time Multi-Object Tracking},
}

@article{Wang20b,
  author     = {Yongxin Wang and Xinshuo Weng and Kris Kitani},
  date       = {2020},
  eprint     = {2006.13164},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Tue, 20 Oct 2020 12:00:14 +0200},
  title      = {Joint Detection and Multi-Object Tracking with Graph Neural Networks},
  url        = {https://arxiv.org/abs/2006.13164},
  volume     = {abs/2006.13164},
}

@inproceedings{Wang21,
  abstract  = {In this paper we study techniques for accurate detection, localization, and tracking of multiple people in an indoor scene covered by multiple top-view fisheye cameras. This is a rarely studied setting within the topic of multi-camera object tracking. The experimental results on test videos exhibit good performance for practical use. We also propose methods to account for occlusion by scene objects at different stages of the algorithm that lead to improved results.},
  author    = {Wang, Tsaipei and Liao, Chih-Hao and Hsieh, Li-Hsuan and Tsui, Arvin Wen and Huang, Hsin-Chien},
  booktitle = {2021 International Conference on Visual Communications and Image Processing (VCIP)},
  date      = {2021-12},
  doi       = {10.1109/VCIP53242.2021.9675451},
  issn      = {2642-9357},
  pages     = {1--5},
  title     = {People Detection and Tracking Using a Fisheye Camera Network},
}

@misc{Wang23,
  archiveprefix = {arXiv},
  author        = {Yu-Hsiang Wang and Jun-Wei Hsieh and Ping-Yang Chen and Ming-Ching Chang and Hung Hin So and Xin Li},
  eprint        = {2211.08824},
  primaryclass  = {cs.CV},
  title         = {SMILEtrack: SiMIlarity LEarning for Occlusion-Aware Multiple Object Tracking},
  year          = {2023},
}

@inproceedings{Wei18,
  abstract  = {Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT171 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.},
  author    = {Wei, Longhui and Zhang, Shiliang and Gao, Wen and Tian, Qi},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  date      = {2018-06},
  doi       = {10.1109/CVPR.2018.00016},
  issn      = {2575-7075},
  keywords  = {},
  number    = {},
  pages     = {79-88},
  title     = {Person Transfer GAN to Bridge Domain Gap for Person Re-identification},
  volume    = {},
}

@inproceedings{Wojke17,
  abstract  = {Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a largescale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45{\%}, achieving overall competitive performance at high frame rates.},
  author    = {Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  booktitle = {2017 IEEE International Conference on Image Processing (ICIP)},
  date      = {2017-09},
  doi       = {10.1109/ICIP.2017.8296962},
  issn      = {2381-8549},
  pages     = {3645--3649},
  title     = {Simple online and realtime tracking with a deep association metric},
}

@inproceedings{Wu06,
  abstract  = {Tracking of humans in videos is important for many applications. A major source of difficulty in performing this task is due to inter-human or scene occlusion. We present an approach based on representing humans as an assembly of four body parts and detection of the body parts in single frames which makes the method insensitive to camera motions. The responses of the body part detectors and a combined human detector provide the "observations" used for tracking. Trajectory initialization and termination are both fully automatic and rely on the confidences computed from the detection responses. An object is tracked by data association if its corresponding detection response can be found; otherwise it is tracked by a meanshift style tracker. Our method can track humans with both inter-object and scene occlusions. The system is evaluated on three sets of videos and compared with previous method.},
  author    = {Bo Wu and Nevatia, R.},
  booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  date      = {2006-06},
  doi       = {10.1109/CVPR.2006.312},
  issn      = {1063-6919},
  keywords  = {},
  number    = {},
  pages     = {951-958},
  title     = {Tracking of Multiple, Partially Occluded Humans based on Static Body Part Detection},
  volume    = {1},
}

@inproceedings{Xing09,
  abstract  = {This paper presents an online detection-based two-stage multi-object tracking method in dense visual surveillances scenarios with a single camera. In the local stage, a particle filter with observer selection that could deal with partial object occlusion is used to generate a set of reliable tracklets. In the global stage, the detection responses are collected from a temporal sliding window to deal with ambiguity caused by full object occlusion to generate a set of potential tracklets. The reliable tracklets generated in the local stage and the potential tracklets generated within the temporal sliding window are associated by Hungarian algorithm on a modified pairwise tracklets association cost matrix to get the global optimal association. This method is applied to the pedestrian class and evaluated on two challenging datasets. The experimental results prove the effectiveness of our method.},
  author    = {Xing, Junliang and Ai, Haizhou and Lao, Shihong},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  date      = {2009-06},
  doi       = {10.1109/CVPR.2009.5206745},
  issn      = {1063-6919},
  pages     = {1200--1207},
  title     = {Multi-object tracking through occlusions by local tracklets filtering and global tracklets association with detection responses},
}

@inproceedings{Yang22,
  abstract  = {Multi-Camera Multi-Target tracking (MCMT) is an essential task in intelligent transportation systems. It is highly challenging due to several problems such as heavy occlusion and appearance variance caused by various camera perspectives and congested vehicles. In this paper, we propose a practical framework for dealing with the city-scale MCMT task, consisting of four modules. The vehicles detection and ReID feature extraction are the first two modules, which locate all vehicles and extract the appearance features for all cameras. The third module is Single-Camera Multi-Target tracking (SCMT), which tracks multiple vehicles to generate candidate trajectories within each camera on the basis of the detected boxes and appearance features. The last module is Inter-Camera Association (ICA), which associates all candidate trajectories between two successive cameras using the K-reciprocal nearest neighbors algorithm, and combines all successively matched trajectories for final results. The ICA module takes the constraints of traveling time, road topology structures, and traffic rules into consideration to reduce the searching space and accelerate the matching speed. Experiments results on the public test set of 2022 AI CITY CHALLENGE Track1 demonstrate the effectiveness of our method, which achieves IDF1 of 84.86{\%}, ranking 1st on the leaderboard.},
  author    = {Yang, Xipeng and Ye, Jin and Lu, Jincheng and Gong, Chenting and Jiang, Minyue and Lin, Xiangru and Zhang, Wei and Tan, Xiao and Li, Yingying and Ye, Xiaoqing and Ding, Errui},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  date      = {2022-06},
  doi       = {10.1109/CVPRW56347.2022.00349},
  issn      = {2160-7516},
  pages     = {3095--3105},
  title     = {Box-Grained Reranking Matching for Multi-Camera Multi-Target Tracking},
}

@misc{Yang23,
  archiveprefix = {arXiv},
  author        = {Fan Yang and Shigeyuki Odashima and Sosuke Yamao and Hiroaki Fujimoto and Shoichi Masui and Shan Jiang},
  eprint        = {2302.03820},
  primaryclass  = {cs.CV},
  title         = {A Unified Multi-view Multi-person Tracking Framework},
  year          = {2023},
}

@article{Yoon18,
  abstract     = {In this study, a multiple hypothesis tracking (MHT) algorithm for multi-target multi-camera tracking (MCT) with disjoint views is proposed. The authors' method forms track-hypothesis trees, and each branch of them represents a multi-camera track of a target that may move within a camera as well as move across cameras. Furthermore, multi-target tracking within a camera is performed simultaneously with the tree formation by manipulating a status of each track hypothesis. Each status represents three different stages of a multi-camera track: tracking, searching, and end-of-track. The tracking status means targets are tracked by a single camera tracker. In the searching status, the disappeared targets are examined if they reappear in other cameras. The end-of-track status does the target exited the camera network due to its lengthy invisibility. These three status assists MHT to form the track-hypothesis trees for multi-camera tracking. Furthermore, a gating technique which eliminates the unlikely observation-to-track association using space-time information has been introduced. In the experiments, the proposed method has been tested using two datasets, DukeMTMC and NLPR\\_MCT, which demonstrates that the method outperforms the state-of-the-art method in terms of improvement of the accuracy. In addition, real-time and online performance of proposed method is also showed in this study.},
  author       = {Yoon, Kwangjin and Song, Young-min and Jeon, Moongu},
  date         = {2018},
  doi          = {https://doi.org/10.1049/iet-ipr.2017.1244},
  eprint       = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-ipr.2017.1244},
  journaltitle = {IET Image Processing},
  keywords     = {trees (mathematics),target tracking,object tracking,MHT algorithm,multiple hypothesis tracking algorithm,multi target multi-camera tracking,track-hypothesis trees,DukeMTMC,NLPR_MCT},
  number       = {7},
  pages        = {1175--1184},
  title        = {Multiple hypothesis tracking algorithm for multi-target multi-camera tracking with disjoint views},
  url          = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-ipr.2017.1244},
  volume       = {12},
}

@article{You21,
  abstract     = {Multi-target multi-camera tracking (MTMCT) targets to generate trajectories of the object that appeared under multiple cameras automatically. MTMCT can be treated as a combination of intra-camera tracking and cross-camera tracking. The existing work only employs the global description to perform the tracklet generating. However, the global description cannot model the local similarity between targets, leading to existing methods not to be robust to occlusion and fast motion. To handle the mentioned problem, we propose an online Optical-based Pose Association (OPA) for multi-target multi-camera tracking. The proposed method utilizes local pose matching to solve the occlusion problem, and applies optical flow to reduce the distance caused by fast motion. For optical-based pose association, we firstly employ OpenPose to generate human pose for each proposal. Then, we utilize the optical flow generated by PWC-Net to adjust the estimated pose for the previous frame. Finally, the modified Object Keypoint Similarity is used to compute the similarity between the pose of the current frame and adjusted pose in the prior frame. Once obtaining the optical-based pose similarity, we combine it with the visual and bounding box spatial similarities to generate the final similarity matrix, and apply the Kuhn-Munkras algorithm for data association. The experiments on the MTMCT and MOT datasets verify the rationality of using human pose information and prove the superiority of the proposed method.},
  author       = {You, Sisi and Yao, Hantao and Xu, Changsheng},
  date         = {2021-08},
  doi          = {10.1109/TCSVT.2020.3036467},
  issn         = {1558-2205},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  number       = {8},
  pages        = {3105--3117},
  title        = {Multi-Target Multi-Camera Tracking With Optical-Based Pose Association},
  volume       = {31},
}

@inproceedings{Yu13,
  abstract  = {A device just like Harry Potter's Marauder's Map, which pinpoints the location of each person-of-interest at all times, provides invaluable information for analysis of surveillance videos. To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem. Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection, face recognition and non-background information to perform tracking. Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints. Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual. Experiments show that our algorithm performs robust localization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment.},
  author    = {Yu, Shoou-I and Yang, Yi and Hauptmann, Alexander},
  booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
  date      = {2013-06},
  doi       = {10.1109/CVPR.2013.476},
  issn      = {1063-6919},
  keywords  = {},
  number    = {},
  pages     = {3714-3720},
  title     = {Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization},
  volume    = {},
}

@article{Yu17,
  author  = {Yu, Wei and Liang, Fan and He, Xiaofei and Hatcher, William and Lu, Chao and Lin, Jie and Yang, Xinyu},
  date    = {2017},
  doi     = {10.1109/ACCESS.2017.2778504},
  journal = {IEEE Access},
  month   = {11},
  pages   = {1-1},
  title   = {A Survey on the Edge Computing for the Internet of Things},
  volume  = {PP},
}

@article{Zadeh21,
  author     = {Seyed Mojtaba Marvasti{-}Zadeh and Li Cheng and Hossein Ghanei{-}Yakhdan and Shohreh Kasaei},
  date       = {2019},
  eprint     = {1912.00535},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Wed, 21 Jul 2021 08:03:41 +0200},
  title      = {Deep Learning for Visual Tracking: {A} Comprehensive Survey},
  url        = {http://arxiv.org/abs/1912.00535},
  volume     = {abs/1912.00535},
}

@article{Zemene16,
  author     = {Eyasu Zemene and Marcello Pelillo},
  eprint     = {1608.00641},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Mon, 13 Aug 2018 16:48:19 +0200},
  title      = {Interactive Image Segmentation Using Constrained Dominant Sets},
  url        = {http://arxiv.org/abs/1608.00641},
  volume     = {abs/1608.00641},
  year       = {2016},
}

@inproceedings{Zhang08,
  abstract  = {We propose a network flow based optimization method for data association needed for multiple object tracking. The maximum-a-posteriori (MAP) data association problem is mapped into a cost-flow network with a non-overlap constraint on trajectories. The optimal data association is found by a min-cost flow algorithm in the network. The network is augmented to include an Explicit Occlusion Model(EOM) to track with long-term inter-object occlusions. A solution to the EOM-based network is found by an iterative approach built upon the original algorithm. Initialization and termination of trajectories and potential false observations are modeled by the formulation intrinsically. The method is efficient and does not require hypotheses pruning. Performance is compared with previous results on two public pedestrian datasets to show its improvement.},
  author    = {Li Zhang and Yuan Li and Nevatia, Ramakant},
  booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
  date      = {2008-06},
  doi       = {10.1109/CVPR.2008.4587584},
  issn      = {1063-6919},
  keywords  = {},
  number    = {},
  pages     = {1-8},
  title     = {Global data association for multi-object tracking using network flows},
  volume    = {},
}

@article{Zhang15a,
  abstract     = {In this paper we propose a framework for tracking multiple interacting targets in a wide-area camera network consisting of both overlapping and non-overlapping cameras. Our method is motivated from observations that both individuals and groups of targets interact with each other in natural scenes. We associate each raw target trajectory (i.e., a tracklet) with a group state, which indicates if the trajectory belongs to an individual or a group. Structural Support Vector Machine (SSVM) is applied to the group states to decide if merge or split events occur in the scene. Information fusion between multiple overlapping cameras is handled using a homography-based voting scheme. The problem of tracking multiple interacting targets is then converted to a network flow problem, for which the solution can be obtained by the K-shortest paths algorithm. We demonstrate the effectiveness of the proposed algorithm on the challenging VideoWeb dataset in which a large amount of multi-person interaction activities are present. Comparative analysis with state-of-the-art methods is also shown.},
  author       = {Zhang, Shu and Zhu, Yingying and Roy-Chowdhury, Amit},
  date         = {2015},
  doi          = {https://doi.org/10.1016/j.cviu.2015.01.002},
  issn         = {1077-3142},
  journaltitle = {Computer Vision and Image Understanding},
  keywords     = {Multi-camera tracking,Multi-target tracking,Interacting targets,Wide-area camera network,Network flow},
  note         = {Image Understanding for Real-world Distributed Video Networks},
  pages        = {64--73},
  title        = {Tracking multiple interacting targets in a camera network},
  url          = {https://www.sciencedirect.com/science/article/pii/S1077314215000168},
  volume       = {134},
}

@inproceedings{Zhang15b,
  abstract  = {In this paper, we propose a novel Non-Overlapping Camera Network Tracking Dataset (CamNeT) for evaluating multi-target tracking algorithms. The dataset is composed of five to eight cameras covering both indoor and outdoor scenes at a university. This dataset consists of six scenarios. Within each scenario are challenges relevant to lighting changes, complex topographies, crowded scenes, and changing grouping dynamics. Persons with predefined trajectories are combined with persons with random trajectories. Ground truth data for predefined trajectories is provided for each camera. Also, a baseline multi-target tracking system is presented. The tracking results using the baseline system are provided, which can be compared with future works. The work provides a comprehensive multicamera dataset for performance evaluation in this challenging application domain, as well as an initial set of results.},
  author    = {Zhang, Shu and Staudt, Elliot and Faltemier, Tim and Roy-Chowdhury, Amit K.},
  booktitle = {2015 IEEE Winter Conference on Applications of Computer Vision},
  date      = {2015-01},
  doi       = {10.1109/WACV.2015.55},
  issn      = {1550-5790},
  pages     = {365--372},
  title     = {A Camera Network Tracking (CamNeT) Dataset and Performance Baseline},
}

@article{Zhang17,
  author      = {Zhang, Zhimeng and Wu, Jianan and Zhang, Xuan and Zhang, Chi},
  date        = {2017},
  eprint      = {1712.09531},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  title       = {Multi-Target, Multi-Camera Tracking by Hierarchical Clustering: Recent Progress on DukeMTMC Project},
}

@article{Zhang21,
  abstract     = {Multi-object tracking (MOT) is an important problem in computer vision which has a wide range of applications. Formulating MOT as multi-task learning of object detection and re-ID in a single network is appealing since it allows joint optimization of the two tasks and enjoys high computation efficiency. However, we find that the two tasks tend to compete with each other which need to be carefully addressed. In particular, previous works usually treat re-ID as a secondary task whose accuracy is heavily affected by the primary detection task. As a result, the network is biased to the primary detection task which is not fair to the re-ID task. To solve the problem, we present a simple yet effective approach termed as FairMOT based on the anchor-free object detection architecture CenterNet. Note that it is not a naive combination of CenterNet and re-ID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies. The resulting approach achieves high accuracy for both detection and tracking. The approach outperforms the state-of-the-art methods by a large margin on several public datasets. The source code and pre-trained models are released at https://github.com/ifzhang/FairMOT.},
  author       = {Zhang, Yifu and Wang, Chunyu and Wang, Xinggang and Zeng, Wenjun and Liu, Wenyu},
  date         = {2021-11-01},
  doi          = {10.1007/s11263-021-01513-4},
  issn         = {1573-1405},
  journaltitle = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3069--3087},
  title        = {FairMOT: On the Fairness of Detection and Re-identification in Multiple Object Tracking},
  url          = {https://doi.org/10.1007/s11263-021-01513-4},
  volume       = {129},
}

@inproceedings{Zheng15,
  abstract  = {This paper contributes a new high quality dataset for person re-identification, named "Market-1501". Generally, current datasets: 1) are limited in scale, 2) consist of hand-drawn bboxes, which are unavailable under realistic settings, 3) have only one ground truth and one query image for each identity (close environment). To tackle these problems, the proposed Market-1501 dataset is featured in three aspects. First, it contains over 32,000 annotated bboxes, plus a distractor set of over 500K images, making it the largest person re-id dataset to date. Second, images in Market-1501 dataset are produced using the Deformable Part Model (DPM) as pedestrian detector. Third, our dataset is collected in an open system, where each identity has multiple images under each camera. As a minor contribution, inspired by recent advances in large-scale image search, this paper proposes an unsupervised Bag-of-Words descriptor. We view person re-identification as a special task of image search. In experiment, we show that the proposed descriptor yields competitive accuracy on VIPeR, CUHK03, and Market-1501 datasets, and is scalable on the large-scale 500k dataset.},
  author    = {Zheng, Liang and Shen, Liyue and Tian, Lu and Wang, Shengjin and Wang, Jingdong and Tian, Qi},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  date      = {2015-12},
  doi       = {10.1109/ICCV.2015.133},
  issn      = {2380-7504},
  keywords  = {},
  number    = {},
  pages     = {1116-1124},
  title     = {Scalable Person Re-identification: A Benchmark},
  volume    = {},
}

@article{Zheng16a,
  author     = {Liang Zheng and Hengheng Zhang and Shaoyan Sun and Manmohan Chandraker and Qi Tian},
  date       = {2016},
  eprint     = {1604.02531},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Mon, 13 Aug 2018 16:46:45 +0200},
  title      = {Person Re-identification in the Wild},
  url        = {http://arxiv.org/abs/1604.02531},
  volume     = {abs/1604.02531},
}

@inproceedings{Zheng16b,
  abstract  = {This paper considers person re-identification (re-id) in videos. We introduce a new video re-id dataset, named Motion Analysis and Re-identification Set (MARS), a video extension of the Market-1501 dataset. To our knowledge, MARS is the largest video re-id dataset to date. Containing 1,261 IDs and around 20,000 tracklets, it provides rich visual information compared to image-based datasets. Meanwhile, MARS reaches a step closer to practice. The tracklets are automatically generated by the Deformable Part Model (DPM) as pedestrian detector and the GMMCP tracker. A number of false detection/tracking results are also included as distractors which would exist predominantly in practical video databases. Extensive evaluation of the state-of-the-art methods including the space-time descriptors and CNN is presented. We show that CNN in classification mode can be trained from scratch using the consecutive bounding boxes of each identity. The learned CNN embedding outperforms other competing methods considerably and has good generalization ability on other video re-id datasets upon fine-tuning.},
  address   = {Cham},
  author    = {Zheng, Liang and Bie, Zhi and Sun, Yifan and Wang, Jingdong and Su, Chi and Wang, Shengjin and Tian, Qi},
  booktitle = {Computer Vision -- ECCV 2016},
  editor    = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  isbn      = {978-3-319-46466-4},
  pages     = {868--884},
  publisher = {Springer International Publishing},
  title     = {MARS: A Video Benchmark for Large-Scale Person Re-Identification},
  year      = {2016},
}

@article{Zheng16c,
  author     = {Liang Zheng and Yi Yang and Alexander G. Hauptmann},
  eprint     = {1610.02984},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Mon, 13 Aug 2018 16:47:30 +0200},
  title      = {Person Re-identification: Past, Present and Future},
  url        = {http://arxiv.org/abs/1610.02984},
  volume     = {abs/1610.02984},
  year       = {2016},
}

@inproceedings{Zhu19,
  author      = {Zhu, Ji and Yang, Hua and Liu, Nian and Kim, Minyoung and Zhang, Wenjun and Yang, Ming-Hsuan},
  booktitle   = {Proceedings of the European conference on computer vision (ECCV)},
  date        = {2019},
  eprint      = {1902.00749},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  pages       = {366--382},
  title       = {Online Multi-Object Tracking with Dual Matching Attention Networks},
}

@article{Zou19,
  abstract     = {Online multi-object tracking (MOT) has broad applications in time-critical video analysis scenarios such as advanced driver-assistance systems (ADASs) and autonomous driving. In this paper, the proposed system aims at tracking multiple vehicles in the front view of an onboard monocular camera. The vehicle detection probes are customized to generate high precision detection, which plays a basic role in the following tracking-by-detection method. A novel Siamese network with a spatial pyramid pooling (SPP) layer is applied to calculate pairwise appearance similarity. The motion model captured from the refined bounding box provides the relative movements and aspects. The online-learned policy treats each tracking period as a Markov decision process (MDP) to maintain long-term, robust tracking. The proposed method is validated in a moving vehicle with an onboard NVIDIA Jetson TX2 and returns real-time speeds. Compared with other methods on KITTI and self-collected datasets, our method achieves significant performance in terms of the &ldquo;Mostly-tracked&rdquo;, &ldquo;Fragmentation&rdquo;, and &ldquo;ID switch&rdquo; variables.},
  author       = {Zou, Yi and Zhang, Weiwei and Weng, Wendi and Meng, Zhengyun},
  date         = {2019},
  doi          = {10.3390/s19061309},
  issn         = {1424-8220},
  journaltitle = {Sensors},
  number       = {6},
  title        = {Multi-Vehicle Tracking via Real-Time Detection Probes and a Markov Decision Process Policy},
  url          = {https://www.mdpi.com/1424-8220/19/6/1309},
  volume       = {19},
}