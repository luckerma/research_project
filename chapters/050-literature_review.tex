\chapter{Literature Review}\label{chap:literature_review}
This chapter reviews the literature on Multi-Target Multi-Camera Tracking (MTMCT) and discusses the trends and advancements as well as the milestones in this field. It will only focus on the latest and state-of-the-art methods and technologies and will not cover the whole history of MTMCT including all the past algorithms and methods. The chapter is structured as follows: Section~\ref{sec:the_beginnings} discusses the beginnings of MTMCT, Section~\ref{sec:milestones} highlights the milestones in MTMCT, Section~\ref{sec:methods} reviews the methods and algorithms used in MTMCT.

\section{The Beginnings}\label{sec:the_beginnings}
Back in \citeyear{Cai99} and \citeyear{Chang01} \textcite{Cai99} and \textcite{Chang01} conducted research in the area of tracking people in an multi-camera system. Also in \citeyear{Khan01}, \textcite{Khan01} proposed a method for tracking people and vehicles with uncalibrated cameras. The system is able to discover spatial relationships between the FOVs of the three cameras used. All three works rely on Bayesian classification and networks~\cite{Pearl88}.

The methods even demonstrated the feasibility of tracking people in real-time, but are in general very limited in their capabilities. For example the work of \citeauthor{Chang01} is limited to people in upright pose. The algorithm proposed by \citeauthor{Cai99} lacks robustness compared to the single-camera tracking and \citeauthor{Khan01} approach does not calibrate the cameras correctly and is highly susceptible to errors caused by occlusion. But in the past two decades the field of tracking in multi-camera systems has evolved significantly.

\section{Milestones}\label{sec:milestones}
This section highlights significant milestones that have shaped the MTMCT research domain, focusing on the five critical areas: detection, feature extraction, data association, tracking, and datasets (challenges).

\subsection{Detection}\label{subsec:milestone_detection}
The foundation for modern object detection methods was laid in \citeyear{Lecun98} by \citeauthor{Lecun98} with the development of Convolutional Neural Networks (CNNs), which are deep learning models specifically designed to process images~\cite{Lecun98}. The advent of deep learning in the past quarter-century has led to a significant improvement in object detection performance.

With the introduction of R-CNN~\cite{Girshick14} in \citeyear{Girshick14}, \citeauthor{Girshick14} demonstrated that deep learning can be used for object detection. The architecture follows a two-stage process: first, it proposes regions of interest using a selective search and then classifies these regions using CNN features. Due to R-CNN proposing the regions of interest independently, it was computationally intensive. Just one year later improvements were made with Fast R-CNN~\cite{Girshick15}, addressed the inefficiencies of its predecessor by introducing a mechanism to share convolutional computations across region proposals and incorporating a Region of Interest (RoI) pooling layer to extract a fixed-size feature vector from the feature map for each proposal. In \citeyear{Ren17} \citeauthor{Ren17} proposed Faster R-CNN~\cite{Ren17}, which integrated a Region Proposal Network (RPN) into the architecture that employs anchors, which are predefined reference boxes of various scales and aspect ratios and used as a basis for proposing potential object locations. This allows the generation of region proposals almost cost-free by sharing the convolutional features with the downstream detection network. This end-to-end trainable model marked a significant leap in efficiency and set a new standard for object detection tasks.

Following the success of R-CNN and its successors, the object detection landscape was further revolutionized by the introduction of You-Only-Look-Once (YOLO)~\cite{Redmon15} and Single Shot MultiBox Detector (SSD)~\cite{Liu15}, which are designed to be even more efficient and suitable for real-time applications.

The YOLO framework, presented by \citeauthor{Redmon15} in \citeyear{Redmon15}, revolutionized real-time object detection by predicting bounding boxes and class probabilities directly from full images in just one evaluation. YOLO processes the entire image in a single forward pass through the network, divides the image into a grid, and predicts bounding boxes and probabilities for each grid cell. The strength of YOLO lies in its speed, making it highly suitable for applications where real-time detection is crucial. Even though the original author has stopped working on YOLO, due to ethical concerns, it is still being improved continuously. The latest official version, YOLOv4, was released in \citeyear{Bochkovskiy20} by \textcite{Bochkovskiy20}. In \citeyear{Jocher23a} Ultralytics released the most recent version YOLOv8~\cite{Jocher23a, Jocher23b}.

\citeauthor{Liu15} proposed SSD in \citeyear{Liu15}, another influential single-shot object detector that balances the trade-off between speed and accuracy. Unlike YOLO, SSD operates on multiple feature maps at different resolutions to effectively handle objects of various sizes. The architecture applies a set of convolutional filters to these feature maps to predict both the bounding box offsets and the class probabilities for a fixed set of default bounding boxes, which are distributed over the image. Detecting and tracking objects across different scales and perspectives makes SSD particularly suitable for MTMCT applications.

\begin{table}[ht]
    \centering
    \caption{Overview Object Detectors}\label{tab:overview_object_detectors}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|l|c|c|c|}
            \hline
            \textbf{Model}            & \textbf{Speed} & \textbf{Accuracy} & \textbf{Computational Requirements} \\
            \hline
            YOLO~\cite{Redmon15}      & Very High      & Moderate          & Low                                 \\
            Faster R-CNN~\cite{Ren17} & Moderate       & High              & High                                \\
            SSD~\cite{Liu15}          & High           & High              & Moderate                            \\
            \hline
        \end{tabular}
    }
\end{table}

Table~\ref{tab:overview_object_detectors} compares the mentioned prominent object detection models used in MTMCT:

\begin{itemize}
    \item \textbf{Speed:} Refers to the time it takes for the detector to process a single frame, usually measured in frames per second (FPS). High speed is crucial for real-time tracking applications, where it is necessary to process video feeds live or near-live.
    \item \textbf{Accuracy:} Measures the ability to correctly identify and locate objects. It is usually quantified by precision and recall rates, or the average precision (AP) over a dataset.
    \item \textbf{Computational Requirements:} Refers to the resources needed to run the detector, typically measured in terms of the number of floating-point operations (FLOPs) or the memory and processing power required. Efficient use of computational resources is essential for deploying MTMCT systems on hardware with limited capabilities.
\end{itemize}

%TODO: Double check citation and references for the following subsections
\subsection{Feature Extraction}\label{subsec:milestone:eature_extraction}
Early feature extraction techniques relied on hand-crafted descriptors such as Scale-Invariant Feature Transform (SIFT)~\cite{Lowe04} and Histogram of Oriented Gradients (HOG)~\cite{Dalal05}, which were pivotal in object recognition and re-identification (re-ID) tasks. With the introduction of deep learning, CNNs have enabled the automatic learning of feature representations, greatly enhancing the robustness and power for re-ID~\cite{Krizhevsky12, He16}.

More recently, Siamese networks have emerged as a popular choice for learning discriminative features in a pairwise manner, proving to be highly effective for re-ID tasks~\cite{Varior16}.

\subsection{Data Association}\label{subsec:milestone_data_association}
Data association in MTMCT involves matching detections of the same object across different frames and camera views, which is essential for maintaining object identity over time. The Hungarian algorithm~\cite{Kuhn55}, also known as the Munkres assignment algorithm, has historically been used for optimal assignment in data association, addressing the problem of associating detections to tracks in a globally optimal way.

The complexity of data association increased with the need to handle multiple objects and cameras, giving rise to the development of Joint Probabilistic Data Association Filters (JPDAF)~\cite{Fortmann83} that consider the probabilities of all potential measurement-to-track assignments.

\citeauthor{Fleuret08} introduced the use of Probabilistic Occupancy Map (POM)~\cite{Fleuret08} to model targets into a POM and combine occupancy probabilities with color and motion attributes in the tracking process in \citeyear{Fleuret08}. The POM is a ground plane that represents the occupancy probability of each cell with this approach tracking of multiple persons in a complex environment is possible. The POM is still used in recent and state-of-the-art approaches.

The advent of graph-based approaches provided a robust framework for data association, viewing the problem as finding the shortest path in a graph where each node represents a detection and edges represent association costs~\cite{Zhang08}. This method became especially useful in managing associations over long periods and occlusions.

Recently, with the surge of deep learning, Neural Networks have been employed to learn the data association task, allowing for an end-to-end approach to tracking by directly learning to associate features extracted from raw pixels namely Recurrent Neural Networks (RNNs)~\cite{Milan16b}. This signifies a shift from traditional methods that require hand-crafted features and heuristics towards data-driven approaches.

The introduction of appearance models using deep learning has significantly improved the association performance in MTMCT by providing discriminative features that can robustly represent an object across different viewpoints and illumination conditions, which are essential for accurate association over multiple cameras~\cite{Schroff15, Zheng16c}.

\subsection{Tracking}\label{subsec:milestone_tracking}
The Kalman Filter~\cite{Kalman60} represents one of the early foundations for object tracking, providing a framework for predicting the future locations of an object.

As tracking scenarios became more complex, approaches like Multiple Hypothesis Tracking (MHT)~\cite{Blackman04} were developed to manage several potential data association hypotheses, especially in crowded scenes~\cite{Reid79}.

Graph-based methods are another cornerstone in tracking, framing the tracking task as an optimization problem where the best path in a graph represents the sequence of object detections over time, where the nodes represent detections and and the edges represent the association costs~\cite{Zhang08}.

\subsection{Datasets and Challenges}\label{subsec:datasets_and_challenges}
Besides the datasets mentioned in section~\ref{sec:datasets}, which are used for object detection in general, there are also datasets which are more tailored towards MTMCT. Typically revolves around tracking specific object classes, predominantly people and vehicles. Datasets, which fits these requirements are listed in table~\ref{tab:overview_datasets}.

\begin{table}[ht]
    \centering
    \caption{Overview of Datasets}\label{tab:overview_datasets}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
            \hline
            \textbf{Dataset}                     & \textbf{Environment} & \textbf{Num. of Scenarios} & \textbf{Num. of Cameras (Overlap)} & \textbf{FPS} & \textbf{IDs} & \textbf{Year} & \textbf{Class}  \\
            \hline
            PETS~\cite{Ferryman09}               & Outdoor              & 3                          & 8 (\cmark)                         & 25           & ---          & 2009          & Person          \\
            MARS\footnotemark[1]~\cite{Zheng16b} & Mixed                & Multiple                   & 6 (\cmark)                         & ---          & 1261         & 2016          & Person          \\
            MOT16~\cite{Milan16a}                & Outdoor              & 14                         & 1                                  & 25-30        & ---          & 2016          & Person, Vehicle \\
            DukeMTMC~\cite{Ristani16}            & Outdoor              & 1                          & 8 (\cmark)                         & 60           & 2834         & 2016          & Person          \\
            WILDTRACK~\cite{Chavdarova18}        & Outdoor              & Multiple                   & 7 (\cmark)                         & 2            & 313          & 2018          & Person          \\
            MSMT17~\cite{Wei18}                  & Mixed                & 12                         & 15 (\cmark)                        & 15           & 4101         & 2018          & Person          \\
            CityFlowV1~\cite{Tang19}             & Outdoor              & 5                          & 40 (\cmark)                        & 10           & 666          & 2019          & Vehicle         \\
            MOT20~\cite{Dendorfer20}             & Outdoor              & 8                          & 1                                  & 25           & ---          & 2020          & Person, Vehicle \\
            CityFlowV2~\cite{Tang19}             & Outdoor              & 6                          & 46 (\cmark)                        & 10           & 880          & 2021          & Vehicle         \\
            MMPTRACK~\cite{Han23}                & Indoor               & 5                          & 23 (\cmark)                        & 15           & ---          & 2023          & Person          \\
            MEVID~\cite{Davila23}                & Mixed                & 17                         & 33 (\cmark)                        & ---          & 158          & 2023          & Person          \\
            \hline
        \end{tabular}
    }
\end{table}
\footnotetext[1]{extension of Market-1501~\cite{Zheng15}}

Table~\ref{tab:overview_datasets} provides a summary of various datasets that have significantly contributed to the MTMCT research domain. Each dataset is categorized based on several distinct criteria to reflect its unique characteristics and relevance:

\begin{itemize}
    \item \textbf{Environment}: Setting of data collection, from controlled indoor environments to dynamic outdoor locations.
    \item \textbf{Num. of Scenarios}: Details the number of distinct scenarios or situations represented in the dataset.
    \item \textbf{Num. of Cameras (Overlap)}: Represents the number of cameras involved and indicates if there is an overlap in their views.
    \item \textbf{FPS}: Specifies the frame rate of the dataset, important for real-time processing considerations.
    \item \textbf{IDs}: Enumerates the unique identities present, which can provide a measure of the complexity of the dataset.
    \item \textbf{Year}: States the year of the release, representing the recentness of the dataset.
    \item \textbf{Class}: Identifies the subjects annotated, such as persons or vehicles.
\end{itemize}

Each dataset listed plays a role in the following sections, the reviewed literature is often evaluated on one or more of these datasets. The datasets are also used to train and test the tracking methods.

In recent years, challenges have been established to encourage research in object detection and tracking, although they have mostly centered on ST-SCT and MT-SCT. Nevertheless, these challenges remain relevant to MTMCT research. The most recent representatives of the primary challenges are:

\begin{itemize}
    \item \textbf{MOT20 Challenge:} Benchmark, which includes crowded environments and variable lighting conditions. Moreover, it provides ground truth data to facilitate evaluation. The MOT datasets are released in conjunction with the MOTChallenge~\cite{Dendorfer20}.
    \item \textbf{2023 AICity Challenge:} Focuses on AI applications in smart cities and includes multi-object tracking for traffic surveillance and anomaly detection as one of its key components. The CityFlow datasets belong to the AICity Challenges.~\cite{Naphade23}
    \item \textbf{VOT2022 Challenge (Visual Object Tracking Challenge):} An annual competition that provides a standardized dataset and evaluation framework for single-object tracking.~\cite{Kristan22}
    \item \textbf{VOTS2023 Challenge (Visual Object Tracking and Segmentation Challenge):} An extension of the VOT Challenge that focuses on multi-object tracking. The challenge, recently published in October 2023, affirms the quickly growing interest in this field.~\cite{Kristan23}
\end{itemize}

\section{Methods}\label{sec:methods}
This section reviews the methods and state-of-the-art algorithms used in MTMCT.

\subsection{Tracking Paradigms}\label{subsec:tracking_paradigms}
In the past years various tracking paradigms have been developed, which are used in MTMCT. The most common paradigms are tracking-by-detection, tracking-by-regression, tracking-by-segmentation, and tracking-by-attention. Each of these paradigms has its own advantages and disadvantages, which are discussed in the following sections.

\subsubsection{Tracking-by-Detection}\label{subsubsec:tracking-by-detection}
The most common approach used by MTMCT systems is to first detect the objects in each frame and then data association is performed to link the detections across frames. This Tracking-by-Detection (TbD) implementation as a multi-shot approach and treats detection and association as separate, sequential tasks, allowing for the use of specialized methods tailored for each step.

One of the pioneering works in this domain is the Simple Online and Realtime Tracking (SORT)~\cite{Bewley16} algorithm proposed by \citeauthor{Bewley16}. SORT employs a combination of Kalman filters for predicting the motion of objects and the Hungarian algorithm for associating detections over time, based on both predicted locations and detected bounding boxes. Its efficiency and speed make it suitable for real-time applications, though it may struggle with identity switches in crowded scenes due to its reliance on motion cues alone.

Building on the foundation laid by SORT, \citeauthor{Wojke17} introduced the DeepSORT~\cite{Wojke17} algorithm, which enhances the tracking performance by incorporating deep learning techniques for appearance features extraction. DeepSORT extends SORT by adding a neural network that generates a high-dimensional vector representation of the appearance of an object, which can be used to compute similarity scores between detections. This addition significantly improves the robustness of the tracker in scenarios where motion predictions are insufficient, such as occlusions or complex, dynamic environments.

Both SORT and DeepSORT have set benchmarks in the field of object tracking, with the latter demonstrating how the integration of motion and appearance information can lead to improved tracking performance.

\begin{itemize}
    \item \textbf{SORT:} Focuses on speed and simplicity by using motion models for prediction and frame-by-frame data association.
    \item \textbf{DeepSORT:} Improves SORT by adding appearance information into the data association step, thus enhancing tracking accuracy, especially in cases where objects interact closely or are temporarily occluded.
\end{itemize}

It is important to mention that both tracking frameworks rely on an external object detector to provide bounding box detections, which can be any of the object detection models discussed in section~\ref{subsec:milestone_detection}. Also SORT as well as DeepSORT are not able to perform inter-camera tracking, this step has to be performed separately.

\subsubsection{Tracking-by-Regression}\label{subsubsec:tracking-by-regression}
Tracking-by-Regression (TbR) involves directly estimating the position of the object in each frame of a video sequence. Unlike the TbD approach, regression-based methods predict the changes in position of the object. Most approaches learn a regression function from the input image features to the location coordinates. The advantage of this method lies in its ability to continuously refine the estimated position, making it well-suited for scenarios with smooth movements or predictable trajectory patterns.

\subsubsection{Tracking-by-Segmentation}\label{subsubsec:tracking-by-segmentation}
Tracking-by-Segmentation (TbS), on the other hand, focuses on delineating the precise shape of the target object in each frame. This method not only tracks the position of the object but also provides its detailed segmentation, capturing its exact outline and shape. It is particularly useful in complex scenes where the object might change shape, size, or orientation. By combining tracking and segmentation, this approach offers more detailed and accurate object tracking, especially in environments where the distinction between foreground and background is critical.

\subsubsection{Tracking-by-Attention}\label{subsubsec:tracking_by_attention}Tracking-by-Attention (TbA) represents a paradigm shift in MTMCT systems by incorporating attention mechanisms that prioritize the most salient features of objects during tracking. The attention paradigm, inspired by the visual ability of humans to focus selectively, has been integrated into tracking frameworks to dynamically emphasize important spatial and temporal features.

\subsection{Single-Shot Approaches}\label{subsec:single-shot_approaches}
In contrast to all the mentioned tracking paradigms, single-shot approaches aim to perform detection and data association simultaneously in a single step. This paradigm, while less common, offers the advantage of speed and simplicity by eliminating the need for separate data association algorithms. Especially in scenarios where computational resources are limited and real-time performance is critical, single-shot approaches can be highly effective.

A notable contribution in this domain is the Single-Shot Multi Object Tracking (SMOT)~\cite{Li20} algorithm proposed by \citeauthor{Li20} in \citeyear{Li20}. SMOT is a tracking framework, which is able to convert any single-shot object detector into a multi-object tracker, which is able to simultaneously generate detection and tracking outputs. It is based on work of \textcite{Bergmann19}, who developed a \textit{Tracktor}, an object detector, which is also able to track objects at the same time. The SMOT framework is able to generate tracklets with a almost constant runtime with respect to number of targets, due to the use of a light-weighted linkage algorithm for online tracklet linking.

In the same year \citeauthor{Wang20a} published the paper \citetitle{Wang20a}, which proposes a single deep-network that Jointly learns the Detection and Embedding (JDE) model. Due to reduction of computational cost, the system is able to achieve (near) real-time performance, while being almost as accurate as the models, which are separately trained for detection and embedding. The architecture is based on the Feature Pyramid Network (FPN)~\cite{Lin17}, which is useful for detecting objects of different sizes. A variation of the triplet loss~\cite{Schroff15} is used to learn the embedding space, which is used for data association. This variation of the triplet loss is defines as follows:

\begin{equation}
    \label{eq:triplet_loss}
    \mathcal{L}_{\text{triplet}}=\sum_i \max \left(0, f^{\top} f_i^{-}-f^{\top} f^{+}\right)
    \quad\text{\cite[Eq.~1]{Wang20a}}
\end{equation}

\begin{itemize}
    \item \(f^{\top}\): Instance in a mini-batch selected as the anchor
    \item \(f^{+}\): Represents a positive instance (same ID as anchor)
    \item \(f^{-}\): Represents a negative instance (different ID as anchor)
\end{itemize}

The triplet loss defined in equation~\ref{eq:triplet_loss} is used to learn an embedding space where instances of the same identity are closely mapped to each other while pushing apart the embeddings of dissimilar identities.

An even more recent framework is the FairMOT~\cite{Zhang21} algorithm proposed by \citeauthor{Zhang21} in \citeyear{Zhang21}. It combines the two tasks of object detection and re-ID while addressing the \textit{unfairness} issue in multi-task learning, which arises because re-ID is often treated as a secondary task in existing frameworks and is not given enough attention. The paper raises three key issues with existing multi-task learning frameworks:

\begin{enumerate}
    \item \textbf{Unfairness Caused by Anchors:} Re-ID task is overlooked in the anchor-based detection framework, where the anchors are only optimized for the detection task.
    \item \textbf{Unfairness Caused by Features:} One-shot trackers share most of their features between the detection and re-ID branches. While detection requires deep features to estimate the object class re-ID requires low-level appearance features to distinguish between different identities, this leads to a conflict between the two tasks.
    \item \textbf{Unfairness Caused by Feature Dimension:} The features dimension of re-ID features is usually much higher than the detection features, but high-dimensional features notably harm the detection performance.
\end{enumerate}

To jointly train the detection and re-ID branches in the FairMOT network the uncertainty loss proposed by \textcite{Cipolla18} is used. The uncertainty loss is defined as follows:

\begin{equation}
    \label{eq:uncertainty_loss}
    L_{\text{total}}=\frac{1}{2}\left(\frac{1}{e^{w_1}} L_{\text{detection}}+\frac{1}{e^{w_2}} L_{\text{identity}}+w_1+w_2\right)
    \quad\text{\cite[Eq.~5]{Zhang21}}
\end{equation}

The uncertainty loss defined in equation~\ref{eq:uncertainty_loss} is used to jointly train the detection and re-ID tasks by assigning different weights to the two tasks to allow a fair learning process. The weights \(w_1\) and \(w_2\) are used to control the balance between the two tasks and are learned during training. \(L_{\text{detection}}\) and \(L_{\text{identity}}\) are the detection and re-ID losses respectively.

By addressing the three key issues with existing multi-task learning frameworks, the FairMOT framework is able to outperform state-of-the-art methods in terms of both tracking accuracy and speed on the MOT17 dataset.

An important notice is that the term \textit{single-shot} used by those frameworks only refers to the detection and intra-camera tracking, the inter-camera (multi-camera) associations still require an additional separate step.

\subsection{Graph Based Approaches}\label{subsec:graph_based_approaches}
Graph-based approaches have been widely used in MTMCT, especially for data association. The problem of data association can be formulated as a graph optimization problem, where each node represents a detection and edges represent the association costs. The goal is to find the shortest path in the graph, which represents the sequence of object detections over time. More recently Graph Neural Networks (GNNs)~\cite{Scarselli09} have been employed to learn the data association task, allowing for an end-to-end approach to tracking.

In \citeyear{Chen17a} \textcite{Chen17a} proposed a pedestrian tracking model, which combines inter- and intra-camera tracking and unifies the two steps into one global graph by considering the initial observations as inputs and directly outputting the final trajectories. Due to the fact that the initial observations contain more information like motion than simple detections, they are more credible for data association. Futhermore, it speeds up computing time, because the number of observations is much smaller than the number of detections. The main focus of this paper is on equalizing the similarity metrics of both tasks to allow unbiased data association. An equalization of metrics is needed, if it is not applied the joint approach would favor objects from the same camera view almost all the time as more similar, because the observations are made under the same circumstances like view angle and illumination. Experimental results show that the proposed joint approach leads to improved performance compared to tackling the association as two independent tasks, especially when the accuracy of intra-camera tracking quality is poor the two step approach is not able to recover at the second step and produces mismatches errors.

Similar to~\cite{Chen17a} \citeauthor{Nguyen22b} present a single-stage approach that combines intra- and inter-camera association by reformulating it as a single-global one-to-many assignment problem. With a focus on dynamic (on-the-move) cameras, the method is used in an autonomous vehicle (AV) environment, which is not the focus of this project, but still an interesting concept and worth mentioning. The proposed method is called Fractional Optimal Transport Assignment (FOTA)~\cite{Nguyen22b} and can be used in both the tracking-by-detection and tracking-by-attention paradigms. The architecture consists of an encoder, two decoders and a box-matching layer. The encoder extracts features of the current and previous frames from the cameras and encodes the feature maps into keys that are used by the decoders to detect and track object boxes. The box-matching layer is then used to match the boxes and provide the final tracking results. The FOTA method results in a reduction of ID switch errors in a large AV dataset compared to state-of-the-art methods.

The Dynamic Graph Model with Link Prediction (DyGLIP)~\cite{Quach21} approach proposed by \citeauthor{Quach21} in \citeyear{Quach21} is a graph model that uses link prediction to solve the data association problem. It works for both overlapping and non-overlapping cameras and is tested on both person and vehicle tracking. The main advantage are better feature representations and the ability to recover from lost tracks during camera transitions. DyGLIP combines link prediction in conjunction with a dynamic graph formulation that takes temporal information of an object into account for the first time in MTMCT. Based on this approach \citeauthor{Cheng23} propose a Reconfigurable Spatial-Temporal Graph Model (ReST)~\cite{Cheng23} in \citeyear{Cheng23}, that handles data association in two steps. First spatial association matches objects across different views at the same frames. Before the second step, a graph reconfiguration module simplifies and reconfigures the graph. Then, temporal association uses information such as speed and time to build a temporal graph and match objects across different frames. Unlike traditional approaches ReST does not rely on single-camera tracking results, because it directly matches objects across camera views in the first step. Another advantage is that two graph models can be trained separately, so there is no need to compromise between the two tasks of intra- and inter-camera data association. The ReST model achieves state-of-the-art performance on the Wildtrack dataset.

The graph based soccer player tracker published by \textcite{Komorowski22} directly uses raw detection heat maps of the feet of the players instead of bounding boxes. The feet of the players are detected by the pre-trained detector FootAndBall~\cite{Komorowski19}, the detection heat maps from all cameras are transformed onto a bird's eye view plane and stacked together to form a multi-channel tensor. This leads to extraction and aggregation being performed within the tracking network itself instead of using a separate preprocessing step like common approaches do, therefore this approach is called \textit{tracking-by-regression}. The tracking network consists out of a Long Short-Term Memory-based (LSTM)~\cite{Gers02} RNN that models the player dynamics and a GNN that is able to learn the interaction between players. The training data is synthetically generated by the Google Research Football Environment (GRF)~\cite{Kurach19} and the final tracker is compared with a baseline approach, base on a particle filter. Even though the proposed tracker is not able to use visual cues like jersey numbers due to a large distance to the camera, it achieves better accuracy and a lower number of ID switches compared to the baseline approach.

%TODO: Write subsection
\subsection{Attention Models and Transformers}\label{subsec:attention_models_and_transformers}
%TODO:~\cite{Zhu19}: Dual Matching Attention Networks
%TODO:~\cite{Sun20}: TransTrack
%TODO:~\cite{Yang21}: AOT
%TODO:~\cite{Hou21}: MVDeTr
%TODO:~\cite{Li21}: Attention
%TODO:~\cite{Meinhardt22}: TrackFormer
%TODO:~\cite{Vaswani17}: Attention is all you need


%TODO: Write subsection
\subsection{Edge Computing}\label{subsec:edge_computing}
The term \textit{edge computing} refers to the concept of processing data near the source of the data, which is in contrast to the traditional approach of processing data in a centralized cloud. The advantages of edge computing are low latency, reduced bandwidth, and improved security. The major disadvantage is the limited computational resources of edge devices in this case the cameras themselves.

In the paper of the already discussed single-shot approach SMOT~\cite{Li20} it is mentioned that replacing the components of the SMOT framework with faster versions can achieve real-time performance on less powerful machines like edge devices.

\subsection{Online and Real-Time}\label{subsec:online_and_real-time}
In addition to subsection~\ref{subsec:single-shot_approaches}, which deals with single-shot approaches and their relevance for real-time applications, this section focuses on online and real-time implementations, mentioning certain methods.

Unlike most of the methods used in MTMCT, the real-time system Uni-ID~\cite{Chen22} follows a distributed concept to ensure that the communication and computing costs of each camera in the network remain almost constant as the number of cameras increases. Therefore, smart stations are installed on the tracked roadside and connected by a wireless multi-hop network. YOLO is used for detection and DeepSORT for tracking. First, intra-camera tracking and feature extraction is performed to assign a local ID to each object. Second, the local ID, features and track information of the target are sent to the adjacent node in the network. Third, the adjacent node performs inter-camera tracking to assign a global ID to the target. The system is tested with three nodes and achieves real-time performance with a relatively low performance GPU for each node.

The work of \textcite{Wang21} focuses on the less attention-grabbing use of fisheye cameras to simulate a checkout-free store, where each person enters or exits the store by scanning a QR code that initializes and terminates the tracking process. Compared to perspective cameras, fisheye cameras are able to cover a larger area with a single camera, reducing the number of cameras needed in the system. In addition, fisheye cameras are less susceptible to occlusion when mounted on a ceiling (top-view). Once a camera is calibrated, the POM of the scene can be created to determine the likelihood of a person being in a particular area and to match the tracks of the same person across different cameras. In a scenario with 5 fisheye cameras and 5 to 10 people in a scene simultaneously, the system achieves real-time performance of about 10 FPS without GPU support.

\citeauthor{Tesfaye19} propose the use of Fast-Constrained Dominant Set Clustering (FCDSC)~\cite{Tesfaye19} to solve both intra- and inter-camera simultaneously. The method is orders of magnitudes faster than existing graph-based methods due to instead of considering the whole graph only a sub-graph is considered. The proposed method follows a three-layer hierarchical approach. The first two layers solve the intra-camera tracking and the third layer the inter-camera tracking while merging the tracks of the same person across camera views. The tracking algorithm runs at 18 FPS and is 2000 times fast than CDSC~\cite{Zemene16} which it is based on.

\subsection{Honorable Mentions}\label{subsec:honorable_mentions}
In the exploration of advanced tracking methodologies, certain studies stand out for their unique and unconventional approaches. This subsection highlights some of these studies, which are not directly related to MTMCT but are still honorable mentions.

One intriguing development in the field of people tracking is \citetitle{Yu13}~\cite{Yu13} from \citeyear{Yu13}. It draws parallels to the fictional Marauder's Map in the Harry Potter series, this research proposes a framework that follows the TbD paradigm and uses nonnegative discretization for robust localization and tracking of persons in complex environments. Their method handles challenges such as occlusions and sparse surveillance camera coverage, employing a semi-supervised learning framework that integrates cues like color, person detection, face recognition, and non-background information. The application in a real-world nursing home setting captured by 15 cameras demonstrates its effectiveness in indoor scenarios.

Equally intriguing is the paper \citetitle{Koehl20}~\cite{Koehl20}, a unique dataset for multi-target multi-camera tracking research, which was captured within the virtual environment of the popular video game Grand Theft Auto 5 (GTA). This creative approach leverages the complex, dynamic world of GTA to provide a rich, diverse dataset for tracking research, highlighting the innovative ways in which simulated environments can contribute to advancements in computer vision without the need of touching someones privacy.

\subsection{State-of-the-Art Approaches}\label{subsec:state-of-the-art_approaches}
This subsection presents state-of-the-art approaches, which were published in \citeyear{Hsu22} and \citeyear{Teepe23}. The approaches achieve state-of-the-art performance on MTMCT datasets but are not real-time capable, due to the use of computationally expensive methods.
\citeauthor{Hsu22} introduce a Self-supervised Camera Link Model (SCLM)~\cite{Hsu22} that extracts both appearance and topological features from a Graph Auto-Encoder (GAE)~\cite{Kipf16} to achieve vehicle tracking in a multi-camera environment. The approach follows the TbD paradigm and advances the Traffic-Aware Single Camera Tracking (TSCT)~\cite{Hsu20} algorithm, which proposes a zone generation algorithm. After common steps of detecting objects and extracting appearance features, these are used as a node for the GAE to establish the camera link model and generate the tracking results. In addition the intra-camera tracking results are used to generate entry and exit points by using the MeanShift~\cite{Comaniciu02} clustering algorithm. The combination of the TSCT and the GAE embeddings with the generation of zones leads to state-of-the-art performance on the CityFlow 2019 and 2020 datasets.

Lifted Multicut Meets Geometry Projections (LMGP)~\cite{Nguyen22a} proposed by \citeauthor{Nguyen22a} follows the traditional TbD paradigm, but with the use of POM for each node in the tracking graph, it integrates concepts from centralized representation methods. A pre-clustering step refines tracklets generated by intra-camera tracking to reduce ID switch errors. For the pre-clustering step the bottom edge center of each bounding box is projected to obtain the 3D coordinates. If the Euclidean distance between two projected ground points is less than a diameter of a person, the two detections may belong to the same person. While solving a global lifted multicut formulation the model takes into account short- and long-range temporal interactions  to perform inter-camera matching. Intra-camera tracking is performed by CenterTrack~\cite{Zhou20} and embedding vectors are extracted by DG-Net~\cite{Zheng19}. LMGP achieves near perfect state-of-the-art performance on the Wildtrack dataset.

EarlyBird~\cite{Teepe23} proposes an early-fusion in the bird's eye view (BEV) that means detections are directly performed in the BEV to solve spatial association of pedestrians across cameras. The approach is built on MVDeTr~\cite{Hou21} and brings the concept of joint detection and re-ID extraction from FairMOT to MTMCT. The input frames are augmented and fed to a encoder network, the image features are projected to the ground plane and aggregated to receive BEV features (in the BEV space). Finally detections and their corresponding re-ID features are fed through a decoder network to association the detections. The proposed approach is similar to ReST in the sense that it associates spatially on the ground plane but it has the advantage of projecting the complete feature space to the ground plane and associating it with the decoder network. EarlyBird shows that early fusion in the BEV space is able to outperform late fusion in the image space. The disadvantage is a higher computational cost due to simultaneously projecting full images of all camera views to the ground plane. Furthermore, high-quality 3D annotations are required which is costly and rare for real-world data.

%TODO: Sadly only tested on own dataset
\citeauthor{Huang23a} propose a method for non-overlapping multi-camera pedestrian tracking that solves the problem of poor long-term feature storage to allow identifying people correctly even though significant appearance changes like different clothes or lighting conditions occur. The proposed method follows the TbD paradigm and combines an state-of-the-art OC-SORT-based~\cite{Cao23} tracker with the person re-ID library Torchreid~\cite{Zhou19} for feature extraction. The feature extraction is performed as an averaging of the features while only taking frames into account where the person is not occluded nor about to leave or enter the scene. Once a new person enters the scene and has accumulated enough features the cosine distance between the features of the new person and all the people in the area is calculated and the ID is restored if the distance is below a certain threshold, this works for both matching people in the same camera and across camera views. Furthermore, a new dataset including 40000 frames recorded by three cameras is proposed to evaluate the performance of their method. Results show that the combination of OC-SORT, the proposed long-term feature extraction and Torchreid outperforms state-of-the-art methods on the new dataset.

\textcite{Huang23b} achieve the first-place ranking in the AI City Challenge 2023 (Track1) with their anchor-guided clustering approach for inter-camera re-ID enabled by self-camera calibrations to improve tracking accuracy of people with similar appearances. Three steps are performed to achieve the final tracking results. First, intra-camera tracking is performed with BoT-SORT~\cite{Aharon22} following a standard TbD scheme. Second, the anchor guided clustering step fixes ID switches and assigns a global ID to each trajectory by hierarchically clustering appearance features from each camera view and obtaining anchors. Each anchor contains features that represent the appearance of the same identity under different conditions. Third, human pose with camera self-calibration is utilized to project the tracked objects on a top-down map.

The \citetitle{Kristan23} presents the performance of the 47 submitted trackers for the challenge. Most trackers apply a uniform dynamic model and utilize transformers.
Multi- as well as single-shot approaches are used, but the top three trackers are single-shot approaches. The top tracker DMAOT is built upon the VOT22~\cite{Kristan22} winner AOT~\cite{Yang21} and its  DeAOT~\cite{Yang22b}. Although a detailed technical documentation on DMAOT is currently not available, it is known that it stores long-term memories object- instead of frame-wise to predict object masks. Overall the challenge reveals a paradigm shift from bounding-box trackers to segmentation-based trackers, which outperform all bounding-box trackers in the challenge.